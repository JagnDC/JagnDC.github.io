<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Spark(七) · SparkStreaming核心</title>
    <url>/2020/03/23/SparkStreaming%E6%A0%B8%E5%BF%83/</url>
    <content><![CDATA[<p>Spark Streaming是<strong>核心Spark API的扩展</strong>，可实现实时数据流的<strong>可伸缩</strong>，<strong>高吞吐量</strong>，<strong>容错流处理</strong>。可以从许多数据源（例如Kafka，Flume，Kinesis或TCP套接字）中提取数据，并可以使用以高级功能（如map，reduce，join和window）表示的复杂算法来处理数据。最后，可以将处理后的数据推送到文件系统，数据库和实时仪表板。实际上，可以在数据流上应用Spark的机器学习和图形处理算法。</p>
<img data-src="/2020/03/23/SparkStreaming%E6%A0%B8%E5%BF%83/streaming-arch.png" class title="streaming-arch">

<p>常用实时流处理框架对比：</p>
<ul>
<li>Storm：真正的实时流处理 Tuple Java</li>
<li>Spark Streaming：并不是真正的实时流处理，而是一个mini batch操作<br>  Scala、Java、Python<br>  Spark一栈式解决问题</li>
<li>Flink：真正的实时流处理</li>
<li>Kafka Stream：用的不多</li>
</ul>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark Streaming receives live input data streams and divides the data into batches, which are then processed by the Spark engine to generate the final stream of results in batches.</p>
<p>Spark Streaming接收实时输入数据流，并将数据分成批次，然后由Spark引擎进行处理，以生成批次的最终结果流。</p>
<img data-src="/2020/03/23/SparkStreaming%E6%A0%B8%E5%BF%83/streaming-flow.png" class title="streaming-flow">

<ul>
<li><p>Spark Core的核心抽象：RDD</p>
</li>
<li><p>Spark Streaming的核心抽象：DStream</p>
</li>
</ul>
<p>A DStream is represented as a sequence of RDDs.</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a local StreamingContext with two working thread and batch interval of 1 second</span></span><br><span class="line">sc = SparkContext(<span class="string">"local[2]"</span>, <span class="string">"NetworkWordCount"</span>)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">"localhost"</span>, <span class="number">9999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Split each line into words</span></span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Count each word in each batch</span></span><br><span class="line">pairs = words.map(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>))</span><br><span class="line">wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x, y: x + y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">wordCounts.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()             <span class="comment"># Start the computation</span></span><br><span class="line">ssc.awaitTermination()  <span class="comment"># Wait for the computation to terminate</span></span><br></pre></td></tr></table></figure>

<p>使用Netcat进行示例数据流</p>
<pre><code>$ nc -lk 9999</code></pre><h2 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h2><p>To initialize a Spark Streaming program, a StreamingContext object has to be created which is the main entry point of all Spark Streaming functionality.</p>
<p>要初始化Spark Streaming程序，必须创建StreamingContext对象，该对象是所有Spark Streaming功能的主要入口点。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line">sc = SparkContext(master, appName)</span><br><span class="line">ssc = StreamingContext(sc, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>After a context is defined, you have to do the following.</p>
<ol>
<li>Define the input sources by creating input DStreams.</li>
<li>Define the streaming computations by applying transformation and output operations to DStreams.</li>
<li>Start receiving data and processing it using streamingContext.start().</li>
<li>Wait for the processing to be stopped (manually or due to any error) using streamingContext.awaitTermination().</li>
<li>The processing can be manually stopped using streamingContext.stop().</li>
</ol>
<img data-src="/2020/03/23/SparkStreaming%E6%A0%B8%E5%BF%83/streaming-dstream.png" class title="streaming-dstream">

<img data-src="/2020/03/23/SparkStreaming%E6%A0%B8%E5%BF%83/streaming-dstream-ops.png" class title="streaming-dstream-ops">

<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>监控文件系统里面的文件流动</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    sc = SparkContext(appName=<span class="string">'spark'</span>)</span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">    lines = ssc.textFileStream(sys.argv[<span class="number">1</span>])</span><br><span class="line">    counts = lines.flatMap(lamba line:line.split(<span class="string">' '</span>))\</span><br><span class="line">        .map(lamba word:(word,<span class="number">1</span>)).reduceByKey(lamba a,b:a+b)</span><br><span class="line"></span><br><span class="line">    counts.pprint()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(六) · SparkSQL核心</title>
    <url>/2020/03/22/SparkSQL%E6%A0%B8%E5%BF%83/</url>
    <content><![CDATA[<p>Spark SQL是用于结构化数据处理的Spark模块。与基本的Spark RDD API不同，Spark SQL提供的接口为Spark提供了有关数据结构和正在执行的计算的更多信息。在内部，Spark SQL使用这些额外的信息来执行额外的优化。与Spark SQL交互的方法有多种，包括SQL和Dataset API。计算结果时，将使用相同的执行引擎，而与要用来表达计算的API/语言无关。这种统一意味着开发人员可以轻松地在不同的API之间来回切换，从而提供最自然的方式来表达给定的转换。</p>
<h2 id="SparkSQL背景"><a href="#SparkSQL背景" class="headerlink" title="SparkSQL背景"></a>SparkSQL背景</h2><p>SQL: MySQL、Oracle、DB2、SQLServer</p>
<p>数据量大 ==&gt; 大数据(Hive、Spark Core)</p>
<p><strong>大家追求的目标是使用SQL语句来对大数据进行分析</strong></p>
<ul>
<li><p>举例：</p>
<p>  person.txt ==&gt; 存放再HDFS</p>
<pre><code>1, zhangsan, 30
2, lisi, 31
3, wangwu, 32</code></pre><p>  hive表：person</p>
<pre><code>id : int 
name : string 
age : int</code></pre><p>  导入数据： loading</p>
<p>  统计分析： select … form person</p>
</li>
</ul>
<ul>
<li><p>这些被统称为SQL on Hadoop</p>
<p>  Hive/Shark/Impala/Presto/Drill</p>
</li>
<li><p>Hive: on MapReduce</p>
<p>  SQL ==&gt; MapReduce ==&gt; Hadoop Cluster</p>
</li>
<li><p>Shark: on Spark</p>
<p>  基于Hive源码进行改造</p>
</li>
<li><p>Spark SQL: on Spark (Spark社区的)</p>
</li>
<li><p>Hive on Spark: (Hive社区的)</p>
</li>
<li><p>共同点：</p>
<p>  metastore  mysql</p>
</li>
</ul>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>Spark SQL is Apache Spark’s module for working with structured data.(官方定义)</p>
<ul>
<li><p>Integrated</p>
<p>  Seamlessly mix SQL queries with Spark programs.</p>
<p>  Spark SQL lets you query structured data inside Spark programs, using either SQL or a familiar DataFrame API. Usable in Java, Scala, Python and R.</p>
<p>  将SQL查询与Spark程序无缝混合。Spark SQL可让您使用SQL或熟悉的DataFrame API在Spark程序中查询结构化数据。可在Java，Scala，Python和R中使用。</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">results = spark.sql(</span><br><span class="line"><span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">names = results.map(lambda p: p.name)</span><br></pre></td></tr></table></figure>



</li>
</ul>
<ul>
<li><p>Uniform Data Access</p>
<p>  Connect to any data source the same way.</p>
<p>  DataFrames and SQL provide a common way to access a variety of data sources, including Hive, Avro, Parquet, ORC, JSON, and JDBC. You can even join data across these sources.</p>
<p>  以相同的方式连接到任何数据源。DataFrame和SQL提供了一种访问各种数据源的通用方法，包括Hive，Avro，Parquet，ORC，JSON和JDBC。甚至可以跨这些源联接数据。</p>
  <figure class="highlight java"><table><tr><td class="code"><pre><span class="line">spark.read.json(<span class="string">"s3n://..."</span>)</span><br><span class="line">.registerTempTable(<span class="string">"json"</span>)</span><br><span class="line">results = spark.sql(</span><br><span class="line"><span class="string">""</span><span class="string">"SELECT *</span></span><br><span class="line"><span class="string">    FROM people</span></span><br><span class="line"><span class="string">    JOIN json ..."</span><span class="string">""</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Hive Integration</p>
<p>  Run SQL or HiveQL queries on existing warehouses.</p>
<p>  Spark SQL supports the HiveQL syntax as well as Hive SerDes and UDFs, allowing you to access existing Hive warehouses.</p>
<p>  在现有仓库上运行SQL或HiveQL查询。Spark SQL支持HiveQL语法以及Hive SerDes和UDF，从而使您可以访问现有的Hive仓库。</p>
  <img data-src="/2020/03/22/SparkSQL%E6%A0%B8%E5%BF%83/sql-hive-arch.png" class title="sql-hive-arch">
</li>
<li><p>Standard Connectivity</p>
<p>  Connect through JDBC or ODBC.</p>
<p>  A server mode provides industry standard JDBC and ODBC connectivity for business intelligence tools.</p>
<p>  通过JDBC或ODBC连接。服务器模式为商业智能工具提供了行业标准的JDBC和ODBC连接。</p>
  <img data-src="/2020/03/22/SparkSQL%E6%A0%B8%E5%BF%83/jdbc.png" class title="jdbc">

</li>
</ul>
<p>Spark SQL提供的操作数据的方式</p>
<ul>
<li>SQL</li>
<li>DataFrame API</li>
<li>Dataset API</li>
</ul>
<p>一个用于处理结构化数据的Spark组件，强调的是“结构化数据”，而非“SQL”</p>
<p>Spark RDD VS MapReduce</p>
<p>Pandas： one machine ==&gt; DataFrame</p>
<p>A Dataset is a distributed collection of data. Dataset is a new interface added in Spark 1.6 that provides the benefits of RDDs (strong typing, ability to use powerful lambda functions) with the benefits of Spark SQL’s optimized execution engine. A Dataset can be constructed from JVM objects and then manipulated using functional transformations (map, flatMap, filter, etc.). </p>
<p>Dataset是数据的分布式集合。Dataset是Spark 1.6中添加的新界面，它具有RDD的优点（强类型输入，使用强大的lambda函数的能力）以及Spark SQL优化的执行引擎的优点。可以从JVM对象构造Dataset，然后使用transformations（map，flatMap，filter等）进行操作。</p>
<p>A DataFrame is a Dataset organized into named columns. </p>
<p>以列(列名、列类型、列值)的形式构成分布式的数据集</p>
<ul>
<li>面试题：</li>
</ul>
<p>RDD与DataFrame的区别？</p>
<h2 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h2><h3 id="Starting-Point-SparkSession"><a href="#Starting-Point-SparkSession" class="headerlink" title="Starting Point: SparkSession"></a>Starting Point: SparkSession</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession \</span><br><span class="line">    .builder \</span><br><span class="line">    .appName(<span class="string">"Python Spark SQL basic example"</span>) \</span><br><span class="line">    .config(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>) \</span><br><span class="line">    .getOrCreate()</span><br></pre></td></tr></table></figure>

<h3 id="Creating-DataFrames"><a href="#Creating-DataFrames" class="headerlink" title="Creating DataFrames"></a>Creating DataFrames</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># spark is an existing SparkSession</span></span><br><span class="line">df = spark.read.json(<span class="string">"examples/src/main/resources/people.json"</span>)</span><br><span class="line"><span class="comment"># Displays the content of the DataFrame to stdout</span></span><br><span class="line">df.show()</span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># | age|   name|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># |null|Michael|</span></span><br><span class="line"><span class="comment"># |  30|   Andy|</span></span><br><span class="line"><span class="comment"># |  19| Justin|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br></pre></td></tr></table></figure>

<h3 id="Untyped-Dataset-Operations-aka-DataFrame-Operations"><a href="#Untyped-Dataset-Operations-aka-DataFrame-Operations" class="headerlink" title="Untyped Dataset Operations (aka DataFrame Operations)"></a>Untyped Dataset Operations (aka DataFrame Operations)</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># spark, df are from the previous example</span></span><br><span class="line"><span class="comment"># Print the schema in a tree format</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment"># root</span></span><br><span class="line"><span class="comment"># |-- age: long (nullable = true)</span></span><br><span class="line"><span class="comment"># |-- name: string (nullable = true)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Select only the "name" column</span></span><br><span class="line">df.select(<span class="string">"name"</span>).show()</span><br><span class="line"><span class="comment"># +-------+</span></span><br><span class="line"><span class="comment"># |   name|</span></span><br><span class="line"><span class="comment"># +-------+</span></span><br><span class="line"><span class="comment"># |Michael|</span></span><br><span class="line"><span class="comment"># |   Andy|</span></span><br><span class="line"><span class="comment"># | Justin|</span></span><br><span class="line"><span class="comment"># +-------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Select everybody, but increment the age by 1</span></span><br><span class="line">df.select(df[<span class="string">'name'</span>], df[<span class="string">'age'</span>] + <span class="number">1</span>).show()</span><br><span class="line"><span class="comment"># +-------+---------+</span></span><br><span class="line"><span class="comment"># |   name|(age + 1)|</span></span><br><span class="line"><span class="comment"># +-------+---------+</span></span><br><span class="line"><span class="comment"># |Michael|     null|</span></span><br><span class="line"><span class="comment"># |   Andy|       31|</span></span><br><span class="line"><span class="comment"># | Justin|       20|</span></span><br><span class="line"><span class="comment"># +-------+---------+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Select people older than 21</span></span><br><span class="line">df.filter(df[<span class="string">'age'</span>] &gt; <span class="number">21</span>).show()</span><br><span class="line"><span class="comment"># +---+----+</span></span><br><span class="line"><span class="comment"># |age|name|</span></span><br><span class="line"><span class="comment"># +---+----+</span></span><br><span class="line"><span class="comment"># | 30|Andy|</span></span><br><span class="line"><span class="comment"># +---+----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Count people by age</span></span><br><span class="line">df.groupBy(<span class="string">"age"</span>).count().show()</span><br><span class="line"><span class="comment"># +----+-----+</span></span><br><span class="line"><span class="comment"># | age|count|</span></span><br><span class="line"><span class="comment"># +----+-----+</span></span><br><span class="line"><span class="comment"># |  19|    1|</span></span><br><span class="line"><span class="comment"># |null|    1|</span></span><br><span class="line"><span class="comment"># |  30|    1|</span></span><br><span class="line"><span class="comment"># +----+-----+</span></span><br></pre></td></tr></table></figure>

<h3 id="Running-SQL-Queries-Programmatically"><a href="#Running-SQL-Queries-Programmatically" class="headerlink" title="Running SQL Queries Programmatically"></a>Running SQL Queries Programmatically</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Register the DataFrame as a SQL temporary view</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line">sqlDF = spark.sql(<span class="string">"SELECT * FROM people"</span>)</span><br><span class="line">sqlDF.show()</span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># | age|   name|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br><span class="line"><span class="comment"># |null|Michael|</span></span><br><span class="line"><span class="comment"># |  30|   Andy|</span></span><br><span class="line"><span class="comment"># |  19| Justin|</span></span><br><span class="line"><span class="comment"># +----+-------+</span></span><br></pre></td></tr></table></figure>

<h3 id="Interoperating-with-RDDs"><a href="#Interoperating-with-RDDs" class="headerlink" title="Interoperating with RDDs"></a>Interoperating with RDDs</h3><p>Spark SQL supports two different methods for converting existing RDDs into Datasets. The first method uses reflection to infer the schema of an RDD that contains specific types of objects. This reflection-based approach leads to more concise code and works well when you already know the schema while writing your Spark application.</p>
<p>The second method for creating Datasets is through a programmatic interface that allows you to construct a schema and then apply it to an existing RDD. While this method is more verbose, it allows you to construct Datasets when the columns and their types are not known until runtime.</p>
<p>Spark SQL支持两种将现有RDD转换为数据集的方法。第一种方法使用反射来推断包含特定对象类型的RDD的架构。这种基于反射的方法可以使代码更简洁，并且当您在编写Spark应用程序时已经了解架构时，可以很好地工作。</p>
<p>第二种方法是通过编程界面，该界面允许您构造模式，然后将其应用于现有的RDD。尽管此方法较为冗长，但可以让您在运行时才知道列及其类型的情况下构造数据集。</p>
<h4 id="Inferring-the-Schema-Using-Reflection"><a href="#Inferring-the-Schema-Using-Reflection" class="headerlink" title="Inferring the Schema Using Reflection"></a>Inferring the Schema Using Reflection</h4><p>Spark SQL can convert an RDD of Row objects to a DataFrame, inferring the datatypes. Rows are constructed by passing a list of key/value pairs as kwargs to the Row class. The keys of this list define the column names of the table, and the types are inferred by sampling the whole dataset, similar to the inference that is performed on JSON files.</p>
<p>Spark SQL可以将Row对象的RDD转换为DataFrame，从而推断数据类型。通过将键/值对的列表作为kwargs传递给Row类来构造行。此列表的键定义表的列名，并且通过对整个数据集进行采样来推断类型，类似于对JSON文件执行的推断。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a text file and convert each line to a Row.</span></span><br><span class="line">lines = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">parts = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</span><br><span class="line">people = parts.map(<span class="keyword">lambda</span> p: Row(name=p[<span class="number">0</span>], age=int(p[<span class="number">1</span>])))</span><br><span class="line"></span><br><span class="line"><span class="comment"># Infer the schema, and register the DataFrame as a table.</span></span><br><span class="line">schemaPeople = spark.createDataFrame(people)</span><br><span class="line">schemaPeople.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL can be run over DataFrames that have been registered as a table.</span></span><br><span class="line">teenagers = spark.sql(<span class="string">"SELECT name FROM people WHERE age &gt;= 13 AND age &lt;= 19"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The results of SQL queries are Dataframe objects.</span></span><br><span class="line"><span class="comment"># rdd returns the content as an :class:`pyspark.RDD` of :class:`Row`.</span></span><br><span class="line">teenNames = teenagers.rdd.map(<span class="keyword">lambda</span> p: <span class="string">"Name: "</span> + p.name).collect()</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> teenNames:</span><br><span class="line">    print(name)</span><br><span class="line"><span class="comment"># Name: Justin</span></span><br></pre></td></tr></table></figure>

<h4 id="Programmatically-Specifying-the-Schema"><a href="#Programmatically-Specifying-the-Schema" class="headerlink" title="Programmatically Specifying the Schema"></a>Programmatically Specifying the Schema</h4><p>When a dictionary of kwargs cannot be defined ahead of time (for example, the structure of records is encoded in a string, or a text dataset will be parsed and fields will be projected differently for different users), a DataFrame can be created programmatically with three steps.</p>
<ol>
<li>Create an RDD of tuples or lists from the original RDD;</li>
<li>Create the schema represented by a StructType matching the structure of tuples or lists in the RDD created in the step 1.</li>
<li>Apply the schema to the RDD via createDataFrame method provided by SparkSession.</li>
</ol>
<p>如果无法提前定义kwarg的字典（例如，记录的结构编码为字符串，或者将解析文本数据集，并且为不同的用户投影不同的字段），则可以使用以下方式以编程方式创建DataFrame：三个步骤。</p>
<ol>
<li>从原始RDD创建元组或列表的RDD；</li>
<li>在第1步中创建的RDD中，创建一个由StructType表示的模式来匹配元组或列表的结构。</li>
<li>通过SparkSession提供的createDataFrame方法将架构应用于RDD。</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Import data types</span></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load a text file and convert each line to a Row.</span></span><br><span class="line">lines = sc.textFile(<span class="string">"examples/src/main/resources/people.txt"</span>)</span><br><span class="line">parts = lines.map(<span class="keyword">lambda</span> l: l.split(<span class="string">","</span>))</span><br><span class="line"><span class="comment"># Each line is converted to a tuple.</span></span><br><span class="line">people = parts.map(<span class="keyword">lambda</span> p: (p[<span class="number">0</span>], p[<span class="number">1</span>].strip()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># The schema is encoded in a string.</span></span><br><span class="line">schemaString = <span class="string">"name age"</span></span><br><span class="line"></span><br><span class="line">fields = [StructField(field_name, StringType(), <span class="literal">True</span>) <span class="keyword">for</span> field_name <span class="keyword">in</span> schemaString.split()]</span><br><span class="line">schema = StructType(fields)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Apply the schema to the RDD.</span></span><br><span class="line">schemaPeople = spark.createDataFrame(people, schema)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Creates a temporary view using the DataFrame</span></span><br><span class="line">schemaPeople.createOrReplaceTempView(<span class="string">"people"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># SQL can be run over DataFrames that have been registered as a table.</span></span><br><span class="line">results = spark.sql(<span class="string">"SELECT name FROM people"</span>)</span><br><span class="line"></span><br><span class="line">results.show()</span><br><span class="line"><span class="comment"># +-------+</span></span><br><span class="line"><span class="comment"># |   name|</span></span><br><span class="line"><span class="comment"># +-------+</span></span><br><span class="line"><span class="comment"># |Michael|</span></span><br><span class="line"><span class="comment"># |   Andy|</span></span><br><span class="line"><span class="comment"># | Justin|</span></span><br><span class="line"><span class="comment"># +-------+</span></span><br></pre></td></tr></table></figure>





]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SparkExecutor内存管理</title>
    <url>/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
    <content><![CDATA[<p>以下文章来源于vivo互联网技术 ，作者郑志彬</p>
<p>我们都知道 Spark 能够有效的利用内存并进行分布式计算，其内存管理模块在整个系统中扮演着非常重要的角色。为了更好地利用 Spark，深入地理解其内存管理模型具有非常重要的意义，这有助于我们对 Spark 进行更好的调优；在出现各种内存问题时，能够摸清头脑，找到哪块内存区域出现问题。</p>
<p>首先我们知道在执行 Spark 的应用程序时，Spark 集群会启动 Driver 和 Executor 两种 JVM 进程，前者为主控进程，负责创建 Spark 上下文，提交 Spark 作业（Job），并将作业转化为计算任务（Task），在各个 Executor 进程间协调任务的调度，后者负责在工作节点上执行具体的计算任务，并将结果返回给 Driver，同时为需要持久化的 RDD 提供存储功能。由于 Driver 的内存管理相对来说较为简单，本文主要对 Executor 的内存管理进行分析，下文中的 Spark 内存均特指 Executor 的内存。</p>
<p>另外，Spark 1.6 之前使用的是静态内存管理 (StaticMemoryManager) 机制，</p>
<p>StaticMemoryManager 也是 Spark 1.6 之前唯一的内存管理器。在 Spark1.6 之后引入了统一内存管理</p>
<p>(UnifiedMemoryManager) 机制，UnifiedMemoryManager 是 Spark 1.6 之后默认的内存管理器，1.6 之前采用的静态管理（StaticMemoryManager）方式仍被保留，可通过配置 spark.memory.useLegacyMode 参数启用。</p>
<p>这里仅对统一内存管理模块 (UnifiedMemoryManager) 机制进行分析。</p>
<h2 id="Executor内存总体布局"><a href="#Executor内存总体布局" class="headerlink" title="Executor内存总体布局"></a>Executor内存总体布局</h2><p>默认情况下，Executor不开启堆外内存，因此整个 Executor 端内存布局如下图所示：</p>


<p>我们可以看到在Yarn集群管理模式中，Spark 以 Executor Container 的形式在 NodeManager 中运行，其可使用的内存上限由</p>
<p>yarn.scheduler.maximum-allocation-mb 指定，我们称之为 MonitorMemory。</p>
<p>整个Executor内存区域分为两块：</p>
<ol>
<li>JVM堆外内存</li>
</ol>
<p>大小由 spark.yarn.executor.memoryOverhead 参数指定。默认大小为 executorMemory * 0.10, with minimum of 384m。</p>
<p>此部分内存主要用于JVM自身，字符串, NIO Buffer（Driect Buffer）等开销。此部分为用户代码及Spark 不可操作的内存，不足时可通过调整参数解决。</p>
<p>The amount of off-heap memory (in megabytes) to be allocated per executor. This is memory that accounts for things like VM overheads, interned strings, other native overheads, etc. This tends to grow with the executor size (typically 6-10%).</p>
<ol start="2">
<li>堆内内存（ExecutorMemory）</li>
</ol>
<p>大小由 Spark 应用程序启动时的 –executor-memory 或 spark.executor.memory 参数配置，即JVM最大分配的堆内存 (-Xmx)。Spark为了更高效的使用这部分内存，对这部分内存进行了逻辑上的划分管理。我们在下面的统一内存管理会详细介绍。</p>
<p>NOTES</p>
<p>对于Yarn集群，存在: ExecutorMemory + MemoryOverhead &lt;= MonitorMemory，若应用提交之时，指定的 ExecutorMemory 与 MemoryOverhead 之和大于 MonitorMemory，则会导致 Executor 申请失败；若运行过程中，实际使用内存超过上限阈值，Executor 进程会被 Yarn 终止掉 (kill)。</p>
<h2 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h2><p>Spark 1.6之后引入了统一内存管理，包括了堆内内存 (On-heap Memory) 和堆外内存 (Off-heap Memory) 两大区域，下面对这两块区域进行详细的说明。</p>
<h3 id="堆内内存-On-heap-Memory"><a href="#堆内内存-On-heap-Memory" class="headerlink" title="堆内内存 (On-heap Memory)"></a>堆内内存 (On-heap Memory)</h3><p>默认情况下，Spark 仅仅使用了堆内内存。Spark 对堆内内存的管理是一种逻辑上的“规划式”的管理，Executor 端的堆内内存区域在逻辑上被划分为以下四个区域：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%A0%86%E5%86%85%E5%86%85%E5%AD%98.png" class title="堆内内存">


<ul>
<li><p>执行内存 (Execution Memory) : 主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据；</p>
</li>
<li><p>存储内存 (Storage Memory) : 主要用于存储 spark 的 cache 数据，例如RDD的缓存、unroll数据；</p>
</li>
<li><p>用户内存（User Memory）: 主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息；</p>
</li>
<li><p>预留内存（Reserved Memory）: 系统预留内存，会用来存储Spark内部对象。</p>
</li>
</ul>
<p>下面的图对这个四个内存区域的分配比例做了详细的描述：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpg" class title="内存管理">

<p>（1）预留内存 (Reserved Memory)</p>
<p>系统预留内存，会用来存储Spark内部对象。其大小在代码中是写死的，其值等于 300MB，这个值是不能修改的（如果在测试环境下，我们可以通过 spark.testing.reservedMemory 参数进行修改）；如果Executor分配的内存小于 1.5 * 300 = 450M 时，Executor将无法执行。</p>
<p>（2）存储内存 (Storage Memory)</p>
<p>主要用于存储 spark 的 cache 数据，例如 RDD 的缓存、广播（Broadcast）数据、和 unroll 数据。内存占比为 UsableMemory * spark.memory.fraction * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * 0.5 = 0.3）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。</p>
<p>（3）执行内存 (Execution Memory)</p>
<p>主要用于存放 Shuffle、Join、Sort、Aggregation 等计算过程中的临时数据。内存占比为 UsableMemory * spark.memory.fraction * (1 - spark.memory.storageFraction)，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的30%（1 * 0.6 * (1 - 0.5) = 0.3）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。</p>
<p>（4）其他/用户内存 (Other/User Memory) </p>
<p>主要用于存储 RDD 转换操作所需要的数据，例如 RDD 依赖等信息。内存占比为 UsableMemory * (1 - spark.memory.fraction)，在Spark2+ 中，默认占可用内存的40%（1 * (1 - 0.6) = 0.4）。</p>
<p>其中，usableMemory = executorMemory - reservedMemory，这个就是 Spark 可用内存。</p>
<blockquote>
<p>NOTES</p>
</blockquote>
<blockquote>
<p>（1）为什么设置300M预留内存</p>
</blockquote>
<blockquote>
<p>统一内存管理最初版本other这部分内存没有固定值 300M 设置，而是和静态内存管理相似，设置的百分比，最初版本占 25%。百分比设置在实际使用中出现了问题，若给定的内存较低时，例如 1G，会导致 OOM，具体讨论参考这里 Make unified memory management work with small heaps。因此，other这部分内存做了修改，先划出 300M 内存。</p>
</blockquote>
<blockquote>
<p>（2）spark.memory.fraction 由 0.75 降至 0.6</p>
</blockquote>
<blockquote>
<p>spark.memory.fraction 最初版本的值是 0.75，很多分析统一内存管理这块的文章也是这么介绍的，同样的，在使用中发现这个值设置的偏高，导致了 gc 时间过长，spark 2.0 版本将其调整为 0.6，详细谈论参见 Reduce spark.memory.fraction default to avoid overrunning old gen in JVM default config。</p>
</blockquote>
<h3 id="堆外内存-Off-heap-Memory"><a href="#堆外内存-Off-heap-Memory" class="headerlink" title="堆外内存 (Off-heap Memory)"></a>堆外内存 (Off-heap Memory)</h3><p>Spark 1.6 开始引入了 Off-heap memory (详见SPARK-11389)。这种模式不在 JVM 内申请内存，而是调用 Java 的 unsafe 相关 API 进行诸如 C 语言里面的 malloc() 直接向操作系统申请内存。这种方式下 Spark 可以直接操作系统堆外内存，减少了不必要的内存开销，以及频繁的 GC 扫描和回收，提升了处理性能。另外，堆外内存可以被精确地申请和释放，而且序列化的数据占用的空间可以被精确计算，所以相比堆内内存来说降低了管理的难度，也降低了误差。，缺点是必须自己编写内存申请和释放的逻辑。</p>
<p>默认情况下Off-heap模式的内存并不启用，我们可以通过 spark.memory.offHeap.enabled 参数开启，并由 spark.memory.offHeap.size 指定堆外内存的大小，单位是字节（占用的空间划归 JVM OffHeap 内存）。</p>
<p>如果堆外内存被启用，那么 Executor 内将同时存在堆内和堆外内存，两者的使用互补影响，这个时候 Executor 中的 Execution 内存是堆内的 Execution 内存和堆外的 Execution 内存之和，同理，Storage 内存也一样。其内存分布如下图所示：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98.png" class title="堆外内存">

<p>相比堆内内存，堆外内存只区分 Execution 内存和 Storage 内存：</p>
<p>（1）存储内存 (Storage Memory)</p>
<p>内存占比为 maxOffHeapMemory * spark.memory.storageFraction，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的50%（1 * 0.5 = 0.5）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。</p>
<p>（2）执行内存 (Execution Memory)</p>
<p>内存占比为 maxOffHeapMemory * (1 - spark.memory.storageFraction)，Spark 2+ 中，默认初始状态下 Storage Memory 和 Execution Memory 均约占系统总内存的50%（1 * (1 - 0.5) = 0.5）。在 UnifiedMemory 管理中，这两部分内存可以相互借用，具体借用机制我们下一小节会详细介绍。</p>
<h3 id="Execution-内存和-Storage-内存动态占用机制"><a href="#Execution-内存和-Storage-内存动态占用机制" class="headerlink" title="Execution 内存和 Storage 内存动态占用机制"></a>Execution 内存和 Storage 内存动态占用机制</h3><p>在 Spark 1.5 之前，Execution 内存和 Storage 内存分配是静态的，换句话说就是如果 Execution 内存不足，即使 Storage 内存有很大空闲程序也是无法利用到的；反之亦然。</p>
<p>静态内存管理机制实现起来较为简单，但如果用户不熟悉 Spark 的存储机制，或没有根据具体的数据规模和计算任务或做相应的配置，很容易造成”一半海水，一半火焰”的局面，即存储内存和执行内存中的一方剩余大量的空间，而另一方却早早被占满，不得不淘汰或移出旧的内容以存储新的内容。</p>
<p>统一内存管理机制，与静态内存管理最大的区别在于存储内存和执行内存共享同一块空间，可以动态占用对方的空闲区域：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E7%BB%9F%E4%B8%80%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpg" class title="统一内存管理">

<p>其中最重要的优化在于动态占用机制，其规则如下：</p>
<ul>
<li>程序提交的时候我们都会设定基本的 Execution 内存和 Storage 内存区域（通过 spark.memory.storageFraction 参数设置）。我们用 onHeapStorageRegionSize 来表示 spark.storage.storageFraction 划分的存储内存区域。这部分内存是不可以被驱逐(Evict)的存储内存（但是如果空闲是可以被占用的）。</li>
</ul>
<ul>
<li>当计算内存不足时，可以借用 onHeapStorageRegionSize 中未使用部分，且 Storage 内存的空间被对方占用后，需要等待执行内存自己释放，不能抢占。</li>
</ul>
<ul>
<li>若实际 StorageMemory 使用量超过 onHeapStorageRegionSize，那么当计算内存不足时，可以驱逐并借用 StorageMemory – onHeapStorageRegionSize 部分，而 onHeapStorageRegionSize 部分不可被抢占。</li>
</ul>
<ul>
<li>反之，当存储内存不足时（存储空间不足是指不足以放下一个完整的 Block），也可以借用计算内存空间；但是 Execution 内存的空间被存储内存占用后，是可让对方将占用的部分转存到硬盘，然后“归还”借用的空间。</li>
</ul>
<ul>
<li>如果双方的空间都不足时，则存储到硬盘；将内存中的块存储到磁盘的策略是按照 LRU 规则进行的。</li>
</ul>
<p><strong>说明</strong></p>
<p>（1）出于兼容旧版本的应用程序的目的，Spark 仍然保留了它的实现。可通过配置spark.memory.useLegacyMode 参数启用。</p>
<p>（2）spark.memory.storageFraction 是不可被驱逐的内存空间。只有空闲的时候能够被执行内存占用，但是不能被驱逐抢占。</p>
<blockquote>
<p>Amount of storage memory immune to eviction, expressed as a fraction of the size of the region set aside by spark.memory.fraction. The higher this is, the less working memory may be available to execution and tasks may spill to disk more often. Leaving this at the default value is recommended. For more detail, see this description.</p>
</blockquote>
<p>（3）Storage 内存的空间被对方占用后，目前的实现是无法让对方”归还”，因为需要考虑 Shuffle 过程中的很多因素，实现起来较为复杂；而且 Shuffle 过程产生的文件在后面一定会被使用到，而 Cache 在内存的数据不一定在后面使用。在 Unified Memory Management in Spark 1.6 中详细讲解了为何选择这种策略，简单总结如下:</p>
<ul>
<li>数据清除的开销 : 驱逐storage内存的开销取决于 storage level，MEMORY_ONLY 可能是最昂贵的，因为需要重新计算，MEMORY_AND_DISK_SER 正好相反，只涉及到磁盘IO。溢写 execution 内存到磁盘的开销并不昂贵，因为 execution 存储的数据格式紧凑(compact format)，序列化开销低。并且，清除的 storage 内存可能不会被用到，但是，可以预见的是，驱逐的 execution 内存是必然会再被读到内存的，频繁的驱除重读 execution 内存将导致昂贵的开销。</li>
</ul>
<ul>
<li>实现的复杂度 : storage 内存的驱逐是容易实现的，只需要使用已有的方法，drop 掉 block。execution 则复杂的多，首先，execution 以 page 为单位管理这部分内存，并且确保相应的操作至少有 one page ，如果把这 one page 内存驱逐了，对应的操作就会处于饥饿状态。此外，还需要考虑 execution 内存被驱逐的情况下，等待 cache 的 block 如何处理。</li>
</ul>
<p>（4）上面说的借用对方的内存需要借用方和被借用方的内存类型都一样，都是堆内内存或者都是堆外内存，不存在堆内i内存不够去借用堆外内存的空间。</p>
<h3 id="任务内存管理（Task-Memory-Manager）"><a href="#任务内存管理（Task-Memory-Manager）" class="headerlink" title="任务内存管理（Task Memory Manager）"></a>任务内存管理（Task Memory Manager）</h3><p>Executor 中任务以线程的方式执行，各线程共享JVM的资源（即 Execution 内存），任务之间的内存资源没有强隔离（任务没有专用的Heap区域）。因此，可能会出现这样的情况：先到达的任务可能占用较大的内存，而后到的任务因得不到足够的内存而挂起。</p>
<p>在 Spark 任务内存管理中，使用 HashMap 存储任务与其消耗内存的映射关系。每个任务可占用的内存大小为潜在可使用计算内存（ 潜在可使用计算内存为: 初始计算内存 + 可抢占存储内存）的 1/2n ~ 1/n，当剩余内存为小于 1/2n 时，任务将被挂起，直至有其他任务释放执行内存，而满足内存下限 1/2n，任务被唤醒。其中 n 为当前 Executor 中活跃的任务树。</p>
<p>比如如果 Execution 内存大小为 10GB，当前 Executor 内正在运行的 Task 个数为5，则该 Task 可以申请的内存范围为 10 / (2 * 5) ~ 10 / 5，也就是 1GB ~ 2GB 的范围。</p>
<p>任务执行过程中，如果需要更多的内存，则会进行申请，如果存在空闲内存，则自动扩容成功，否则，将抛出 OutOffMemroyError。</p>
<p>每个 Executor 中可同时运行的任务数由 Executor 分配的 CPU 的核数 N 和每个任务需要的 CPU 核心数 C 决定。其中：</p>
<pre><code>N = spark.executor.cores
C = spark.task.cpus</code></pre><p>由此每个 Executor 的最大任务并行度可表示为 : TP = N / C 。</p>
<p>其中，C 值与应用类型有关，大部分应用使用默认值 1 即可，因此，影响 Executor 中最大任务并行度（最大活跃task数）的主要因素是 N。</p>
<p>依据 Task 的内存使用特征，前文所述的 Executor 内存模型可以简单抽象为下图所示模型：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B.png" class title="内存模型">

<p>其中，Executor 向 yarn 申请的总内存可表示为 : M = M1 + M2 。</p>
<p>如果考虑堆外内存则大概是如下结构：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/%E8%80%83%E8%99%91%E5%A0%86%E5%A4%96%E5%86%85%E5%AD%98.png" class title="考虑堆外内存">


<h3 id="一个示例"><a href="#一个示例" class="headerlink" title="一个示例"></a>一个示例</h3><p>为了更好的理解上面堆内内存和堆外内存的使用情况，这里给出一个简单的例子。</p>
<p>（1）只用了堆内内存</p>
<p>现在我们提交的 Spark 作业关于内存的配置如下：–executor-memory 18g</p>
<p>由于没有设置spark.memory.fraction 和 spark.memory.storageFraction 参数，我们可以看到 Spark UI 关于 Storage Memory 的显示如下：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/storage1.png" class>

<p>上图很清楚地看到 Storage Memory 的可用内存是 10.1GB，这个数是咋来的呢？根据前面的规则，我们可以得出以下的计算：</p>
<pre><code>systemMemory = spark.executor.memory
reservedMemory = 300MB
usableMemory = systemMemory - reservedMemory
StorageMemory= usableMemory * spark.memory.fraction * spark.memory.storageFraction</code></pre><p>如果我们把数据代进去，得出以下的结果：</p>
<pre><code>systemMemory = 18Gb = 19327352832 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 19327352832 - 314572800 = 19012780032
StorageMemory = usableMemory * spark.memory.fraction * spark.memory.storageFraction
            = 19012780032 * 0.6 * 0.5 = 5703834009.6 = 5.312109375GB</code></pre><p>和上面的 10.1GB 对不上啊。为什么呢？这是因为 Spark UI 上面显示的 Storage Memory 可用内存其实等于 Execution 内存和 Storage 内存之和，也就是 usableMemory * spark.memory.fraction :</p>
<pre><code>StorageMemory = usableMemory * spark.memory.fraction
            = 19012780032 * 0.6 = 11407668019.2 = 10.62421GB</code></pre><p>还是不对，这是因为我们虽然设置了 –executor-memory 18g ，但是 Spark 的 Executor 端通过 Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，所以 systemMemory=17179869184，然后计算的数据如下：</p>
<pre><code>systemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
StorageMemory= usableMemory * spark.memory.fraction
            = 16865296384 * 0.6 = 9.42421875 GB</code></pre><p>我们通过将上面的 16865296384 * 0.6 字节除于 1024 * 1024 * 1024 转换成 9.42421875 GB，和 UI 上显示的还是对不上，这是因为 Spark UI 是通过除于 1000 * 1000 * 1000 将字节转换成 GB，如下：</p>
<pre><code>systemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
StorageMemory = usableMemory * spark.memory.fraction
            = 16865296384 * 0.6 字节 =  16865296384 * 0.6 / (1000 * 1000 * 1000) = 10.1GB</code></pre><p>现在终于对上了。</p>
<p>具体将字节转换成 GB 的计算逻辑如下(core 模块下面的 /core/src/main/resources/org/apache/spark/ui/static/utils.js)：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">functionformatBytes(bytes, type) &#123;</span><br><span class="line">    <span class="keyword">if</span>(type !==<span class="string">'display'</span>)returnbytes;</span><br><span class="line">    <span class="keyword">if</span>(bytes == <span class="number">0</span>)<span class="keyword">return</span><span class="string">'0.0 B'</span>;</span><br><span class="line">    vark = <span class="number">1000</span>;</span><br><span class="line">    vardm = <span class="number">1</span>;</span><br><span class="line">    varsizes = [<span class="string">'B'</span>,<span class="string">'KB'</span>,<span class="string">'MB'</span>,<span class="string">'GB'</span>,<span class="string">'TB'</span>,<span class="string">'PB'</span>,<span class="string">'EB'</span>,<span class="string">'ZB'</span>,<span class="string">'YB'</span>];</span><br><span class="line">    vari = Math.floor(Math.log(bytes) / Math.log(k));</span><br><span class="line">    returnparseFloat((bytes / Math.pow(k, i)).toFixed(dm)) +<span class="string">' '</span>+ sizes[i];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>我们设置了 –executor-memory 18g，但是 Spark 的 Executor 端通过</p>
<p>Runtime.getRuntime.maxMemory 拿到的内存其实没这么大，只有 17179869184 字节，这个数据是怎么计算的？</p>
<p>Runtime.getRuntime.maxMemory 是程序能够使用的最大内存，其值会比实际配置的执行器内存的值小。这是因为内存分配池的堆部分划分为 Eden，Survivor 和 Tenured 三部分空间，而这里面一共包含了两个 Survivor 区域，而这两个 Survivor 区域在任何时候我们只能用到其中一个，所以我们可以使用下面的公式进行描述 :</p>
<pre><code>ExecutorMemory = Eden + 2 * Survivor + Tenured
Runtime.getRuntime.maxMemory = Eden + Survivor + Tenured</code></pre><p>上面的 17179869184 字节可能因为你的 GC 配置不一样得到的数据不一样，但是上面的计算公式是一样的。</p>
<p>（2）用了堆内和堆外内存</p>
<p>现在如果我们启用了堆外内存，情况会怎样呢？我们的内存相关配置如下：</p>
<pre><code>spark.executor.memory           18g
spark.memory.offHeap.enabled   true
spark.memory.offHeap.size       10737418240</code></pre><p>从上面可以看出，堆外内存为 10GB，现在 Spark UI 上面显示的 Storage Memory 可用内存为 20.9GB，如下：</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/storage2.png" class>

<p>其实 Spark UI 上面显示的 Storage Memory 可用内存等于堆内内存和堆外内存之和，计算公式如下：</p>
<p>堆内:</p>
<pre><code>systemMemory = 17179869184 字节
reservedMemory = 300MB = 300 * 1024 * 1024 = 314572800
usableMemory = systemMemory - reservedMemory = 17179869184 - 314572800 = 16865296384
totalOnHeapStorageMemory = usableMemory * spark.memory.fraction
                        = 16865296384 * 0.6 = 10119177830</code></pre><p>堆外:</p>
<pre><code>totalOffHeapStorageMemory = spark.memory.offHeap.size = 10737418240

总 Storage 内存:
StorageMemory = totalOnHeapStorageMemory + totalOffHeapStorageMemory
            = (10119177830 + 10737418240) 字节
            = (20856596070 / (1000 * 1000 * 1000)) GB
            = 20.9 GB</code></pre><h3 id="Executor内存参数调优"><a href="#Executor内存参数调优" class="headerlink" title="Executor内存参数调优"></a>Executor内存参数调优</h3><p>（1） Executor JVM Used Memory Heuristic</p>
<p>现象：配置的executor内存比实际使用的JVM最大使用内存还要大很多。</p>
<p>原因：这意味着 executor 内存申请过多了，实际上并不需要使用这么多内存。</p>
<p>解决方案：将 spark.executor.memory 设置为一个比较小的值。</p>
<p>例如：</p>
<pre><code>spark.executor.memory : 16 GB
Max executor peak JVM used memory : 6.6 GB

Suggested spark.executor.memory : 7 GB</code></pre><img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/memory1.png" class>

<p>（2） Executor Unified Memory Heuristic</p>
<p>现象：分配的统一内存 (Unified Memory = Storage Memory + Execution Memory) 比 executor 实际使用的统一内存大的多。</p>
<p>原因：这意味着不需要这么大的统一内存。</p>
<p>解决方案：降低 spark.memory.fraction 的比例。</p>
<p>例如：</p>
<pre><code>spark.executor.memory : 10 GB
spark.memory.fraction : 0.6
Allocated unified memory : 6 GB
Max peak JVM userd memory : 7.2 GB
Max peak unified memory : 1.2 GB

Suggested spark.memory.fraction : 0.2</code></pre><img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/memory2.png" class>

<p>（3）Executor OOM类错误 （错误代码 137、143等）</p>
<p>该类错误一般是由于 Heap（M2）已达上限，Task 需要更多的内存，而又得不到足够的内存而导致。因此，解决方案要从增加每个 Task 的内存使用量，满足任务需求 或 降低单个 Task 的内存消耗量，从而使现有内存可以满足任务运行需求两个角度出发。因此有如下解决方案：</p>
<p>法一：增加单个task的内存使用量</p>
<p>增加最大 Heap值，即上图中 M2 的值，使每个 Task 可使用内存增加。</p>
<p>降低 Executor 的可用 Core 的数量 N , 使 Executor 中同时运行的任务数减少，在总资源不变的情况下，使每个 Task 获得的内存相对增加。当然，这会使得 Executor 的并行度下降。可以通过调高spark.executor.instances 参数来申请更多的 executor 实例（或者通过 spark.dynamicAllocation.enabled 启动动态分配），提高job的总并行度。</p>
<p>法二：降低单个Task的内存消耗量</p>
<p>降低单个Task的内存消耗量可从配置方式和调整应用逻辑两个层面进行优化：</p>
<p>配置方式</p>
<p>减少每个 Task 处理的数据量，可降低 Task 的内存开销，在 Spark 中，每个 partition 对应一个处理任务 Task，因此，在数据总量一定的前提下，可以通过增加 partition 数量的方式来减少每个 Task 处理的数据量，从而降低 Task 的内存开销。针对不同的 Spark 应用类型，存在不同的 partition 配置参数 :</p>
<pre><code>P = spark.default.parallism (非SQL应用)
P = spark.sql.shuffle.partition (SQL 应用)</code></pre><p>通过增加 P 的值，可在一定程度上使 Task 现有内存满足任务运行。注: 当调整一个参数不能解决问题时，上述方案应进行协同调整。</p>
<p>a.调整应用逻辑</p>
<p>Executor OOM 一般发生 Shuffle 阶段，该阶段需求计算内存较大，且应用逻辑对内存需求有较大影响，下面举例就行说明：</p>
<p>选择合适的算子，如 groupByKey 转换为 reduceByKey。</p>
<p>一般情况下，groupByKey 能实现的功能使用 reduceByKey 均可实现，而 ReduceByKey 存在 Map 端的合并，可以有效减少传输带宽占用及 Reduce 端内存消耗。</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/memory3.png" class>

<p>b.避免数据倾斜 (data skew)</p>
<p>Data Skew 是指任务间处理的数据量存大较大的差异。</p>
<p>如左图所示，key 为 010 的数据较多，当发生 shuffle 时，010 所在分区存在大量数据，不仅拖慢 Job 执行（Job 的执行时间由最后完成的任务决定）。而且导致 010 对应 Task 内存消耗过多，可能导致 OOM。</p>
<p>而右图，经过预处理（加盐，此处仅为举例说明问题，解决方法不限于此）可以有效减少 Data Skew 导致的问题。</p>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/memory4.png" class>

<blockquote>
<p>NOTE</p>
</blockquote>
<blockquote>
<p>上述举例仅为说明调整应用逻辑可以在一定程序上解决OOM问题，解决方法不限于此</p>
</blockquote>
<p>（4）Execution Memory Spill Heuristic</p>
<ul>
<li><p>现象：在 stage 3 发现执行内存溢出。Shuffle read bytes 和 spill 分布均匀。这个 stage 有 200 个 tasks。</p>
</li>
<li><p>原因：执行内存溢出，意味着执行内存不足。跟上面的 OOM 错误一样，只是执行内存不足的情况下不会报 OOM 而是会将数据溢出到磁盘。但是整个性能很难接受。</p>
</li>
<li><p>解决方案：同 3。</p>
</li>
</ul>
<img data-src="/2020/03/21/SparkExecutor%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/memory5.png" class>

<p>（5） Executor GC Heuristic</p>
<ul>
<li><p>现象：Executor 花费很多时间在 GC。</p>
</li>
<li><p>原因：可以通过-verbose:gc</p>
<p>  -XX:+PrintGCDetails</p>
<p>  -XX:+PrintGCTimeStamps 查看 GC 情况</p>
</li>
<li><p>解决方案： Garbage Collection Tuning</p>
</li>
</ul>
<p>（6）Beyond … memory, killed by yarn.</p>
<p>出现该问题原因是由于实际使用内存上限超过申请的内存上限而被 Yarn 终止掉了, 首先说明 Yarn 中 Container 的内存监控机制：</p>
<p>Container 进程的内存使用量 : 以 Container 进程为根的进程树中所有进程的内存使用总量。</p>
<p>Container 被杀死的判断依据 : 进程树总内存（物理内存或虚拟内存）使用量超过向 Yarn 申请的内存上限值，则认为该 Container 使用内存超量，可以被“杀死”。</p>
<p>因此，对该异常的分析要从是否存在子进程两个角度出发。</p>
<p>a. 不存在子进程</p>
<p>Overhead不足，依据 Yarn 内存使用情况有如下两种方案:</p>
<ul>
<li><p>法一：如果，M (spark.executor.memory) 未达到 Yarn 单个 Container 允许的上限时，可仅增加 M1（spark.yarn.executor.memoryOverhead），从而增加 M；如果，M 达到 Yarn 单个 Container 允许的上限时，增加 M1，降低 M2。</p>
<p>  注意二者之各要小于 Container 监控内存量，否则伸请资源将被 yarn 拒绝。</p>
</li>
<li><p>法二：减少可用的 Core 的数量 N，使并行任务数减少，从而减少 Overhead 开销</p>
</li>
</ul>
<p>b. 存在子进程</p>
<p>Spark 应用中 Container 以 Executor（JVM进程）的形式存在，因此根进程为 Executor 对应的进程，而 Spark 应用向Yarn申请的总资源 M = M1 + M2，都是以 Executor(JVM) 进程（非进程树）可用资源的名义申请的。</p>
<p>申请的资源并非一次性全量分配给 JVM 使用，而是先为 JVM 分配初始值，随后内存不足时再按比率不断进行扩容，直致达到 Container 监控的最大内存使用量 M。当 Executor 中启动了子进程（如调用 shell 等）时，子进程占用的内存（记为 S）就被加入 Container 进程树，此时就会影响 Executor 实际可使用内存资源（Executor 进程实际可使用资源变为: M - S），然而启动 JVM 时设置的可用最大资源为 M，且 JVM 进程并不会感知 Container 中留给自己的使用量已被子进程占用。因此，当 JVM 使用量达到 M - S，还会继续开劈内存空间，这就会导致 Executor 进程树使用的总内存量大于 M 而被 Yarn 杀死。</p>
<p><strong>典形场景有:</strong></p>
<p>PySpark（Spark已做内存限制，一般不会占用过大内存）</p>
<p>自定义Shell调用</p>
<p>其解决方案分别为:</p>
<p>a. PySpark场景：</p>
<p>如果，M 未达到 Yarn 单个 Container 允许的上限时，可仅增加 M1 ，从而增加 M；如果，M 达到 Yarn 单个 Container 允许的上限时，增加 M1，降低 M2。</p>
<p>减少可用的 Core 的数量 N，使并行任务数减少，从而减少 Overhead 开销</p>
<p>b. 自定义 Shell 场景:（OverHead 不足为假象）</p>
<p>调整子进程可用内存量 (通过单机测试，内存控制在 Container 监控内存以内，且为 Spark 保留内存等留有空间）。</p>
]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(五) · 优化</title>
    <url>/2020/03/21/Spark%E4%BC%98%E5%8C%96/</url>
    <content><![CDATA[<p>由于大多数Spark计算的内存性质，Spark程序可能会受到群集中任何资源（CPU，网络带宽或内存）的瓶颈。<br>通常，如果数据适合内存，则瓶颈是网络带宽，但是有时，还需要进行一些调整，例如以序列化形式存储RDD，以减少内存使用量。</p>
<h2 id="Monitoring"><a href="#Monitoring" class="headerlink" title="Monitoring"></a>Monitoring</h2><p>localhost:4040</p>
<ul>
<li>A list of scheduler stages and tasks</li>
<li>A summary of RDD sizes and memory usage</li>
<li>Environmental information.</li>
<li>Information about the running executors</li>
</ul>
<p>Note that this information is only available for the duration of the application by default. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.</p>
<p>默认情况下，此信息仅在应用程序期间有效。要在结束后查看Web UI，请在启动应用程序之前将spark.eventLog.enabled设置为true。这会将Spark配置为记录Spark事件，这些事件将UI中显示的信息编码为持久存储。</p>
<p>historyServer：</p>
<pre><code>$SPARK_HOME/conf/spark-default.conf
    spark.eventLog.enabled      true
    spark.eventLog.dir          hdfs://namenode/shared/spark-logs

$SPARK_HOME/conf/spark-env.sh
    SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://namenode/shared/spark-logs&quot;</code></pre><p>It is still possible to construct the UI of an application through Spark’s history server, provided that the application’s event logs exist. You can start the history server by executing:</p>
<pre><code>./sbin/start-history-server.sh</code></pre><p>web UI：<a href="http://localhost.18080" target="_blank" rel="noopener">http://localhost.18080</a></p>
<h2 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h2><ol>
<li><p>序列化的作用</p>
<p> 网络传输<br> 节省内存</p>
</li>
<li><p>如何选择</p>
<p> Serialization plays an important role in the performance of any distributed application. Formats that are slow to serialize objects into, or consume a large number of bytes, will greatly slow down the computation. Often, this will be the first thing you should tune to optimize a Spark application. Spark aims to strike a balance between convenience (allowing you to work with any Java type in your operations) and performance. It provides two serialization libraries:</p>
<ul>
<li><p>Java serialization: By default, Spark serializes objects using Java’s ObjectOutputStream framework, and can work with any class you create that implements java.io.Serializable. You can also control the performance of your serialization more closely by extending java.io.Externalizable. Java serialization is flexible but often quite slow, and leads to large serialized formats for many classes.</p>
<p>  兼容性较好，但是性能较弱</p>
</li>
<li><p>Kryo serialization: Spark can also use the Kryo library (version 4) to serialize objects more quickly. Kryo is significantly faster and more compact than Java serialization (often as much as 10x), but does not support all Serializable types and requires you to register the classes you’ll use in the program in advance for best performance.</p>
<p>  kryo性能更好，但是需要自己去注册</p>
</li>
</ul>
</li>
</ol>
<h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><ul>
<li><p>execution</p>
<p>  shuffles,joins,sorts,aggregations</p>
</li>
<li><p>storage</p>
<p>  cache,propagate</p>
</li>
</ul>
<img data-src="/2020/03/21/Spark%E4%BC%98%E5%8C%96/%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86.jpg" class title="内存管理">

<h2 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h2><p>Using the broadcast functionality available in SparkContext can greatly reduce the size of each serialized task, and the cost of launching a job over a cluster. If your tasks use any large object from the driver program inside of them (e.g. a static lookup table), consider turning it into a broadcast variable. Spark prints the serialized size of each task on the master, so you can look at that to decide whether your tasks are too large; in general tasks larger than about 20 KB are probably worth optimizing.</p>
<p>大幅降低task的大小，每个机器一个副本。通常数据大于20KB就值得进行优化</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>broadcastVar = sc.broadcast([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">&lt;pyspark.broadcast.Broadcast object at <span class="number">0x102789f10</span>&gt;</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>broadcastVar.value</span><br><span class="line">[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<h2 id="数据本地性"><a href="#数据本地性" class="headerlink" title="数据本地性"></a>数据本地性</h2><p>Data locality can have a major impact on the performance of Spark jobs. If data and the code that operates on it are together then computation tends to be fast. But if code and data are separated, one must move to the other. Typically it is faster to ship serialized code from place to place than a chunk of data because code size is much smaller than data. Spark builds its scheduling around this general principle of data locality.</p>
<p>代码距离数据有多近</p>
<ul>
<li>PROCESS_LOCAL data is in the same JVM as the running code. This is the best locality possible</li>
<li>NODE_LOCAL data is on the same node. Examples might be in HDFS on the same node, or in another executor on the same node. This is a little slower than PROCESS_LOCAL because the data has to travel between processes</li>
<li>NO_PREF data is accessed equally quickly from anywhere and has no locality preference</li>
<li>RACK_LOCAL data is on the same rack of servers. Data is on a different server on the same rack so needs to be sent over the network, typically through a single switch</li>
<li>ANY data is elsewhere on the network and not in the same rack</li>
</ul>
]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(四) · 核心进阶</title>
    <url>/2020/03/17/Spark%E6%A0%B8%E5%BF%83%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<h2 id="Spark-核心概念"><a href="#Spark-核心概念" class="headerlink" title="Spark 核心概念"></a>Spark 核心概念</h2><img data-src="/2020/03/17/Spark%E6%A0%B8%E5%BF%83%E8%BF%9B%E9%98%B6/%E6%A0%B8%E5%BF%83.png" class title="核心概念from官网">

<ol>
<li><p>Application</p>
<p> User program built on Spark. Consists of a driver program and executors on the cluster.</p>
<p> 基于Spark的应用程序 = 1 driver + executors</p>
</li>
<li><p>Driver program</p>
<p> The process running the main() function of the application and creating the SparkContext</p>
<p> 运行主函数，创建SparkContext</p>
</li>
<li><p>Cluster manager</p>
<p> An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)<br> spark-submit –master local[*]/spark://localhost:7077/yarn</p>
</li>
<li><p>Deploy mode</p>
<p> Distinguishes where the driver process runs. </p>
<p> In “cluster” mode, the framework launches the driver inside of the cluster. </p>
<p> In “client” mode, the submitter launches the driver outside of the cluster.</p>
</li>
<li><p>Worker node</p>
<p> Any node that can run application code in the cluster</p>
<ul>
<li><p>standalone</p>
<p>  slave节点 slaves配置文件</p>
</li>
<li><p>yarn</p>
<p>  nodemanager</p>
</li>
</ul>
</li>
<li><p>Executor</p>
<p> A process launched for an application on a worker node</p>
<p> runs tasks</p>
<p> keeps data in memory or disk storage across them</p>
<p> Each application has its own executors.</p>
</li>
<li><p>task</p>
<p> A unit of work that will be sent to one executor</p>
<p> 最小工作单元</p>
</li>
<li><p>Job</p>
<p> A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.</p>
<p> 一个action对应一个Job</p>
</li>
<li><p>Stage</p>
<p> Each job gets divided into smaller sets of tasks called stages that depend on each other </p>
<p> (similar to the map and reduce stages in MapReduce);</p>
<p> you’ll see this term used in the driver’s logs.</p>
<p> 一个stage的边界往往是从某个地方取数据开始，到shuffle的地方结束</p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>一个application是由driver和executors构成的</p>
<p>一个action对应一个job，一个job对应一到多个stage</p>
<p>一个stage对应一到多个task</p>
<p>task运行在executor里面</p>
<p>executors跑在worker上面的</p>
<h2 id="Spark-运行架构"><a href="#Spark-运行架构" class="headerlink" title="Spark 运行架构"></a>Spark 运行架构</h2><img data-src="/2020/03/17/Spark%E6%A0%B8%E5%BF%83%E8%BF%9B%E9%98%B6/components.png" class title="components">

<p>Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).</p>
<p>Spark应用程序在群集上作为独立的进程集运行，由主程序（称为驱动程序）中的SparkContext对象协调。</p>
<p>Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.</p>
<p>具体来说，要在集群上运行，SparkContext可以连接到几种类型的集群管理器（Spark自己的独立集群管理器Mesos或YARN），它们可以在应用程序之间分配资源。连接后，Spark会在集群中的节点上获取执行程序，这些节点是运行计算并为您的应用程序存储数据的进程。接下来，它将应用程序代码（由传递给SparkContext的JAR或Python文件定义）发送给执行者。最后，SparkContext将任务发送给执行程序以运行。</p>
<ol>
<li><p>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.</p>
<p> 线程是独立的，应用程序之间数据不共享，除非写入外部存储管理系统中</p>
</li>
<li><p>Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).</p>
<p> 写spark应用程序的时候不需要关注程序运行在哪里，只需要和executor进行通讯</p>
</li>
<li><p>The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes.</p>
<p> driver必须要和executor进行通讯，连接是双向的，发送作业，接受心跳信息</p>
</li>
<li><p>Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.</p>
<p> driver要尽可能近的靠近worker nodes，最好是local，远程的话最好使用RPC通讯</p>
</li>
</ol>
<h2 id="Spark和Hadoop的概念区分"><a href="#Spark和Hadoop的概念区分" class="headerlink" title="Spark和Hadoop的概念区分"></a>Spark和Hadoop的概念区分</h2><p>Hadoop</p>
<ol>
<li>一个MR程序 = 一个Job</li>
<li>一个Job = 1-n个task(Map/Reduce)</li>
<li>一个task对应于一个进程</li>
<li>task运行时相当于开启了进程，执行完毕后销毁进程，对于多个task来说，开销是比较大的(即使能够JVM共享)</li>
</ol>
<p>Spark</p>
<ol>
<li>Application = Driver (main方法中创建SparkContext + Executors)</li>
<li>一个Application = 0-n个Job</li>
<li>一个Job = 一个Action</li>
<li>一个Job = 1-n个Stage</li>
<li>一个Stage = 1-n个Task</li>
<li>一个Task对应一个线程，多个Task可以以并行的方式运行在一个Executor进程中</li>
</ol>
<h2 id="Spark-Cache详解"><a href="#Spark-Cache详解" class="headerlink" title="Spark Cache详解"></a>Spark Cache详解</h2><p>rdd.cache() : StorageLevel</p>
<p>cache:lazy 只有遇到action才会提交运行</p>
<p>不缓存的场景中，有多少次action就会读取多少次disk</p>
<p>如果一个RDD在后续的计算中会使用到，建议使用cache缓存</p>
<p>cache底层调用persist方法，传入的参数是：StorageLevel.MEMORY_ONLY</p>
<p>cache = persist</p>
<p>unpersist：立即执行</p>
<p><strong>如何选择</strong></p>
<p>Which Storage Level to Choose?<br>Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:</p>
<ol>
<li><p>If your RDDs fit comfortably with the default storage level (MEMORY_ONLY), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.</p>
</li>
<li><p>If not, try using MEMORY_ONLY_SER and selecting a fast serialization library to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)</p>
</li>
<li><p>Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.</p>
</li>
<li><p>Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.</p>
</li>
</ol>
<p>Spark的存储级别旨在在内存使用量和CPU效率之间提供不同的权衡。我们建议通过以下过程选择一个：如果您的RDD适合默认存储级别</p>
<ol>
<li><p>（MEMORY_ONLY），请保持这种状态。这是CPU效率最高的选项，允许RDD上的操作尽可能快地运行。</p>
</li>
<li><p>如果不是，请尝试使用MEMORY_ONLY_SER并选择一个快速的序列化库，以使对象的空间效率更高，但访问速度仍然相当快。（Java和Scala）</p>
</li>
<li><p>除非用于计算数据集的函数很昂贵，否则它们会过滤掉大量数据。否则，重新计算分区可能与从磁盘读取分区一样快。</p>
</li>
<li><p>如果要快速恢复故障（例如，如果使用Spark来处理来自Web应用程序的请求），请使用副本存储。所有存储级别都通过重新计算丢失的数据来提供完全的容错能力，但是副本存储使您可以继续在RDD上运行任务，而不必等待重新计算丢失的分区。</p>
</li>
</ol>
<h2 id="Spark-Lineage详解"><a href="#Spark-Lineage详解" class="headerlink" title="Spark Lineage详解"></a>Spark Lineage详解</h2><p>RDD之间的依赖关系就是Lineage</p>
<p>当计算过程中RDD的其中某个partition出现问题就可以根据Lineage从上个RDD中计算出来</p>
<h2 id="Spark-Dependency详解"><a href="#Spark-Dependency详解" class="headerlink" title="Spark Dependency详解"></a>Spark Dependency详解</h2><ol>
<li><p>窄依赖 Narrow (pipeline-able)</p>
<p> 一个父RDD的partition最多被子RDD的某个partition使用一次</p>
</li>
<li><p>宽依赖 Wide (shuffle)</p>
<p> 一个父RDD的partition会被子RDD的partition使用多次，有shuffle</p>
</li>
</ol>
<p>n个shuffle会对应n+1个stage</p>
]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(三) · 运行模式</title>
    <url>/2020/03/13/Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<h2 id="Local模式："><a href="#Local模式：" class="headerlink" title="Local模式："></a>Local模式：</h2><pre><code>适用于开发和本地测试环境

pyspark/spark-submit

--master

--name

--py-files

    ./spark-submit --master local[*] --name spark-local /xxx/xx.py (argv)</code></pre><h2 id="standalone"><a href="#standalone" class="headerlink" title="standalone"></a>standalone</h2><pre><code>spark自带的部署方法

hdfs : NameNode DataNode
yarn : ResourceManager NodeManager

master, worker

    $SPARK_HOME/conf/slaves
    dxy409
    ...
    如果有多台机器则在后面添加名字

启动spark集群

    $SPARK_HOME/sbin/start-all.sh
    ps:在spark-env.sh中添加JAVA_HOME

检测：

jps:Master和Worker进程

webui:8080

    ./spark-submit --master spark://192.168.0.7:7077 --name spark-standalone /xxx/xx.py (argv)

如果使用standalone模式，且节点数大于1，如果使用本地文件，必须保证每个节点都有文件</code></pre><h2 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h2><pre><code>生产环境中70%使用的方法

spark作业客户端，提交作业到yarn上执行

yarn vs standalone

yarn:只需要一个节点即可提交作业，不需要spark集群，不需要启动master和worker

standalone:spark集群的每个节点都需要部署spark，需要启动spark集群，需要master和worker

    ./spark-submit --master yarn --deploy mode --name spark-yarn /xxx/xx.py (argv)

When running with master &apos;yarn&apos; either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.

- 在spark-env.sh添加 HADOOP_CONF_DIR={hadoop的位置}

yarn支持client和cluster模式：区别在于driver运行在哪里

- cluster：提交完作业就可以断开了，因为driver运行在application master上的

- client：提交作业的进程不能禁止，否则driver就挂了

pyspark/spark-shell/spark-sql：交互式运行程序 client

查看已经完成运行的yarn的日志：

    yarn logs -applicationId &lt;applicationId&gt;</code></pre>]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL (一) · 基础学习</title>
    <url>/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在我们的日常工作中，使用的是类似 MySQL、Oracle 这种的数据库管理系统，实际上这些数据库管理系统都遵循 SQL 语言，这就意味着，我们在使用这些数据库的时候，都是通过 SQL 语言与它们打交道。所以对于从事编程或者互联网行业的人来说，最具有中台能力的语言便是 SQL 语言。自从 SQL 加入了 TIOBE 编程语言排行榜，就一直保持在 Top 10。</p>
<h3 id="SQL-语言按照功能划分成以下的-4-个部分："><a href="#SQL-语言按照功能划分成以下的-4-个部分：" class="headerlink" title="SQL 语言按照功能划分成以下的 4 个部分："></a>SQL 语言按照功能划分成以下的 4 个部分：</h3><ul>
<li><p>DDL，英文叫做 Data Definition Language，也就是数据定义语言，它用来定义我们的数据库对象，包括数据库、数据表和列。通过使用 DDL，我们可以创建，删除和修改数据库和表结构。</p>
</li>
<li><p>DML，英文叫做 Data Manipulation Language，数据操作语言，我们用它操作和数据库相关的记录，比如增加、删除、修改数据表中的记录。</p>
</li>
<li><p>DCL，英文叫做 Data Control Language，数据控制语言，我们用它来定义访问权限和安全级别。</p>
</li>
<li><p>DQL，英文叫做 Data Query Language，数据查询语言，我们用它查询想要的记录，它是 SQL 语言的重中之重。在实际的业务中，我们绝大多数情况下都是在和查询打交道，因此学会编写正确且高效的查询语句，是学习的重点。</p>
<h2 id="初步使用"><a href="#初步使用" class="headerlink" title="初步使用"></a>初步使用</h2><h3 id="大小写问题"><a href="#大小写问题" class="headerlink" title="大小写问题"></a>大小写问题</h3><ul>
<li>表名、表别名、字段名、字段别名等都小写；</li>
<li>SQL 保留字、函数名、绑定变量等都大写;</li>
<li>此外在数据表的字段名推荐采用下划线命名。</li>
</ul>
<h3 id="DB、DBS-和-DBMS-的区别是什么"><a href="#DB、DBS-和-DBMS-的区别是什么" class="headerlink" title="DB、DBS 和 DBMS 的区别是什么"></a>DB、DBS 和 DBMS 的区别是什么</h3><p>说到 DBMS，有一些概念你需要了解。</p>
</li>
<li><p>DBMS 的英文全称是 DataBase Management System，数据库管理系统，实际上它可以对多个数据库进行管理，所以你可以理解为 DBMS = 多个数据库（DB） + 管理程序。</p>
</li>
<li><p>DB 的英文是 DataBase，也就是数据库。数据库是存储数据的集合，你可以把它理解为多个数据表。</p>
</li>
<li><p>DBS 的英文是 DataBase System，数据库系统。它是更大的概念，包括了数据库、数据库管理系统以及数据库管理人员 DBA。</p>
</li>
</ul>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/DBMS.png" class title="DBMS">


<h3 id="MySQL-中的-SQL-是如何执行的"><a href="#MySQL-中的-SQL-是如何执行的" class="headerlink" title="MySQL 中的 SQL 是如何执行的"></a>MySQL 中的 SQL 是如何执行的</h3><p> 首先 MySQL 是典型的 C/S 架构，即 Client/Server 架构，服务器端程序使用的 mysqld。整体的 MySQL 流程如下图所示：</p>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/MySQL.png" class title="MySQL">


<p> <strong>MySQL 由三层组成：</strong></p>
<ul>
<li>连接层：客户端和服务器端建立连接，客户端发送 SQL 至服务器端；</li>
<li>SQL 层：对 SQL 语句进行查询处理；</li>
<li>存储引擎层：与数据库文件打交道，负责数据的存储和读取。</li>
</ul>
<p> <strong>SQL层的结构：</strong></p>
<ul>
<li>查询缓存：Server 如果在查询缓存中发现了这条 SQL 语句，就会直接将结果返回给客户端；如果没有，就进入到解析器阶段。需要说明的是，因为查询缓存往往效率不高，所以在 MySQL8.0 之后就抛弃了这个功能。</li>
<li>解析器：在解析器中对 SQL 语句进行语法分析、语义分析。</li>
<li>优化器：在优化器中会确定 SQL 语句的执行路径，比如是根据全表检索，还是根据索引来检索等。</li>
<li>执行器：在执行之前需要判断该用户是否具备权限，如果具备权限就执行 SQL 查询并返回结果。在 MySQL8.0 以下的版本，如果设置了查询缓存，这时会将查询结果进行缓存。</li>
</ul>
<p> <strong>SQL 语句在 MySQL 中的流程是：</strong></p>
<p> SQL 语句→缓存查询→解析器→优化器→执行器</p>
<p>MySQL 的存储引擎采用了插件的形式，每个存储引擎都面向一种特定的数据库应用环境。同时开源的 MySQL 还允许开发人员设置自己的存储引擎，下面是一些常见的存储引擎：</p>
<ul>
<li>InnoDB 存储引擎：它是 MySQL 5.5 版本之后默认的存储引擎，最大的特点是支持事务、行级锁定、外键约束等。</li>
<li>MyISAM 存储引擎：在 MySQL 5.5 版本之前是默认的存储引擎，不支持事务，也不支持外键，最大的特点是速度快，占用资源少。</li>
<li>Memory 存储引擎：使用系统内存作为存储介质，以便得到更快的响应速度。不过如果 mysqld 进程崩溃，则会导致所有的数据丢失，因此我们只有当数据是临时的情况下才使用 Memory 存储引擎。</li>
<li>NDB 存储引擎：也叫做 NDB Cluster 存储引擎，主要用于 MySQL Cluster 分布式集群环境，类似于 Oracle 的 RAC 集群。</li>
<li>Archive 存储引擎：它有很好的压缩机制，用于文件归档，在请求写入时会进行压缩，所以也经常用来做仓库。</li>
</ul>
<p> 数据库的设计在于表的设计，而在 MySQL 中每个表的设计都可以采用不同的存储引擎，我们可以根据实际的数据处理需要来选择存储引擎，这也是 MySQL 的强大之处。</p>
<p>WHERE 子句中同时出现 AND 和 OR 操作符的时候，需要考虑到执行的先后顺序，也就是两个操作符执行的优先级。一般来说 () 优先级最高，其次优先级是 AND，然后是 OR。</p>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/WHERE.png" class title="WHERE">


<h2 id="常见SQL函数"><a href="#常见SQL函数" class="headerlink" title="常见SQL函数"></a>常见SQL函数</h2><ol>
<li>算术函数<ul>
<li>ABS() 绝对值</li>
<li>MOD() 取余</li>
<li>ROUND() 四舍五入</li>
</ul>
</li>
<li>字符串函数<ul>
<li>CONCAT() 拼接</li>
<li>LENGTH() 字段长度 汉字算三个 数字字母为一</li>
<li>CHAR_LENGTH() 字段长度 汉字数字字母都为一个</li>
<li>LOWER() 转小写</li>
<li>UPPER() 转大写</li>
<li>REPLACE() 替换</li>
<li>SUBSTRING() 截取</li>
</ul>
</li>
<li>日期函数<ul>
<li>CURRENT_DATE() 当前日期</li>
<li>CURRENT_TIME() 当前时间无日期</li>
<li>CURRENT_TIMESTAMP() 日期加时间</li>
<li>EXTRACT() 抽取年月日</li>
<li>DATE() YEAR() MONTH() DAY() HOUR() MINUTE() SECOND() 时间的各个部分 </li>
</ul>
</li>
<li>转换函数<ul>
<li>CAST() 数据类型转换</li>
<li>COALESCE 返回第一个非空值</li>
</ul>
</li>
<li>聚集函数<ul>
<li>COUNT() 总行数</li>
<li>MAX() MIN() 最大最小值</li>
<li>SUM() 求和</li>
<li>AVG() 平均值</li>
<li>DISTINCT() 取唯一</li>
</ul>
</li>
</ol>
<p>在 SQL 中，你还是要确定大小写的规范，因为在 Linux 和 Windows 环境下，可能会遇到不同的大小写问题。</p>
<p>比如 MySQL 在 Linux 的环境下，数据库名、表名、变量名是严格区分大小写的，而字段名是忽略大小写的。</p>
<p>而 MySQL 在 Windows 的环境下全部不区分大小写。</p>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/SQL.png" class title="SQL">

<h2 id="HAVING"><a href="#HAVING" class="headerlink" title="HAVING"></a>HAVING</h2><p>对于分组的筛选，我们一定要用 HAVING，而不是 WHERE。另外，HAVING 支持所有 WHERE 的操作，因此所有需要 WHERE 子句实现的功能，都可以使用 HAVING 对分组进行筛选。</p>
<p>用 WHERE 进行数据量的过滤，用 GROUP BY 进行分组，用 HAVING 进行分组过滤，用 ORDER BY 进行排序。</p>
<p> <strong>要记住，在 SELECT 查询中，关键字的顺序是不能颠倒的，它们的顺序是：</strong></p>
<pre><code>SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ...</code></pre><h2 id="关联子查询，非关联子查询"><a href="#关联子查询，非关联子查询" class="headerlink" title="关联子查询，非关联子查询"></a>关联子查询，非关联子查询</h2><p>子查询虽然是一种嵌套查询的形式，不过我们依然可以依据子查询是否执行多次，从而将子查询划分为关联子查询和非关联子查询。</p>
<ul>
<li><p>子查询从数据表中查询了数据结果，如果这个数据结果只执行一次，然后这个数据结果作为主查询的条件进行执行，那么这样的子查询叫做非关联子查询。</p>
</li>
<li><p>同样，如果子查询需要执行多次，即采用循环的方式，先从外部查询开始，每次都传入子查询进行查询，然后再将结果反馈给外部，这种嵌套的执行方式就称为关联子查询。</p>
</li>
</ul>
<p> <strong>IN 和 EXIST</strong></p>
<pre><code>SELECT * FROM A WHERE cc IN (SELECT cc FROM B)

SELECT * FROM A WHERE EXIST (SELECT cc FROM B WHERE B.cc=A.cc)</code></pre><p>如果表 A 比表 B 大，那么 IN 子查询的效率要比 EXIST 子查询效率高，因为这时 B 表中如果对 cc 列进行了索引，那么 IN 子查询的效率就会比较高。</p>
<p>同样，如果表 A 比表 B 小，那么使用 EXISTS 子查询效率会更高，因为我们可以使用到 A 表中对 cc 列的索引，而不用从 B 中进行 cc 列的查询。</p>
<p> <strong>ANY、ALL 关键字必须与一个比较操作符一起使用</strong></p>
<p> SQL 中，子查询的使用大大增强了 SELECT 查询的能力，因为很多时候查询需要从结果集中获取数据，或者需要从同一个表中先计算得出一个数据结果，然后与这个数据结果（可能是某个标量，也可能是某个集合）进行比较。</p>
 <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%AD%90%E6%9F%A5%E8%AF%A2.png" class title="子查询">

<h2 id="SQL标准"><a href="#SQL标准" class="headerlink" title="SQL标准"></a>SQL标准</h2><p>  SQL 的英文全称叫做 Structured Query Language，它有一个很强大的功能，就是能在各个数据表之间进行连接查询（Query）。这是因为 SQL 是建立在关系型数据库基础上的一种语言。关系型数据库的典型数据结构就是数据表，这些数据表的组成都是结构化的（Structured）。你可以把关系模型理解成一个二维表格模型，这个二维表格是由行（row）和列（column）组成的。每一个行（row）就是一条数据，每一列（column）就是数据在某一维度的属性。</p>
<p>  正是因为在数据库中，表的组成是基于关系模型的，所以一个表就是一个关系。一个数据库中可以包括多个表，也就是存在多种数据之间的关系。而我们之所以能使用 SQL 语言对各个数据表进行复杂查询，核心就在于连接，它可以用一条 SELECT 语句在多张表之间进行查询。你也可以理解为，关系型数据库的核心之一就是连接。</p>
<p>  <strong>SQL 有两个主要的标准，分别是 SQL92 和 SQL99</strong></p>
<h3 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h3><p>  笛卡尔乘积是一个数学运算。假设我有两个集合 X 和 Y，那么 X 和 Y 的笛卡尔积就是 X 和 Y 的所有可能组合，也就是第一个对象来自于 X，第二个对象来自于 Y 的所有可能。</p>
<p>  笛卡尔积也称为交叉连接，英文是 CROSS JOIN，它的作用就是可以把任意表进行连接，即使这两张表不相关。但我们通常进行连接还是需要筛选的，因此你需要在连接后面加上 WHERE 子句，也就是作为过滤条件对连接数据进行筛选。比如后面要讲到的等值连接。</p>
<h3 id="等值连接"><a href="#等值连接" class="headerlink" title="等值连接"></a>等值连接</h3><p>  两张表的等值连接就是用两张表中都存在的列进行连接。我们也可以对多张表进行等值连接。</p>
<h3 id="非等值连接"><a href="#非等值连接" class="headerlink" title="非等值连接"></a>非等值连接</h3><p>  当我们进行多表查询的时候，如果连接多个表的条件是等号时，就是等值连接，其他的运算符连接就是非等值查询。</p>
<h3 id="外连接"><a href="#外连接" class="headerlink" title="外连接"></a>外连接</h3><p>  除了查询满足条件的记录以外，外连接还可以查询某一方不满足条件的记录。两张表的外连接，会有一张是主表，另一张是从表。如果是多张表的外连接，那么第一张表是主表，即显示全部的行，而第剩下的表则显示对应连接的信息。在 SQL92 中采用（+）代表从表所在的位置，而且在 SQL92 中，只有左外连接和右外连接，没有全外连接。</p>
<p>  <strong>左右外连接</strong></p>
<p>  就是指左边的表是主表，需要显示左边表的全部行，而右侧的表是从表，（+）表示哪个是从表。</p>
<pre><code>SQL：SELECT * FROM player, team where player.team_id = team.team_id(+)</code></pre><p>  相当于 SQL99 中的：</p>
<pre><code>SQL：SELECT * FROM player LEFT JOIN team on player.team_id = team.team_id</code></pre><p>  右外连接同理</p>
<h3 id="自连接"><a href="#自连接" class="headerlink" title="自连接"></a>自连接</h3><p>  自连接可以对多个表进行操作，也可以对同一个表进行操作。也就是说查询条件使用了当前表的字段。</p>
  <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/SQL%E6%A0%87%E5%87%86.png" class title="SQL标准">

<h2 id="SQL99与SQL92在连接上的区别"><a href="#SQL99与SQL92在连接上的区别" class="headerlink" title="SQL99与SQL92在连接上的区别"></a>SQL99与SQL92在连接上的区别</h2><h3 id="交叉连接"><a href="#交叉连接" class="headerlink" title="交叉连接"></a>交叉连接</h3><p>  交叉连接实际上就是 SQL92 中的笛卡尔乘积，只是这里我们采用的是 CROSS JOIN。</p>
<pre><code>SQL: SELECT * FROM player CROSS JOIN team</code></pre><h3 id="自然连接"><a href="#自然连接" class="headerlink" title="自然连接"></a>自然连接</h3><p>  可以把自然连接理解为 SQL92 中的等值连接。它会帮你自动查询两张连接表中所有相同的字段，然后进行等值连接。</p>
<p>  SQL92：<br>    SELECT player_id, a.team_id, player_name, height, team_name FROM player as a, team as b WHERE a.team_id = b.team_id</p>
<p>  SQL99：<br>    SELECT player_id, team_id, player_name, height, team_name FROM player NATURAL JOIN team </p>
<h3 id="ON-连接"><a href="#ON-连接" class="headerlink" title="ON 连接"></a>ON 连接</h3><p>  ON 连接用来指定我们想要的连接条件，针对上面的例子，它同样可以帮助我们实现自然连接的功能：</p>
<pre><code>SELECT player_id, player.team_id, player_name, height, team_name FROM player JOIN team ON player.team_id = team.team_id</code></pre><p>  相当于是用 ON 进行了 team_id 字段的等值连接。</p>
<p>  一般来说在 SQL99 中，我们需要连接的表会采用 JOIN 进行连接，ON 指定了连接条件，后面可以是等值连接，也可以采用非等值连接。</p>
<h3 id="USING-连接"><a href="#USING-连接" class="headerlink" title="USING 连接"></a>USING 连接</h3><p>  当我们进行连接的时候，可以用 USING 指定数据表里的同名字段进行等值连接。比如：</p>
<pre><code>SELECT player_id, team_id, player_name, height, team_name FROM player JOIN team USING(team_id)</code></pre><p>  与自然连接 NATURAL JOIN 不同的是，USING 指定了具体的相同的字段名称，你需要在 USING 的括号 () 中填入要指定的同名字段。</p>
<h3 id="外连接-1"><a href="#外连接-1" class="headerlink" title="外连接"></a>外连接</h3><p>  SQL99 的外连接包括了三种形式：</p>
<ul>
<li><p>左外连接：LEFT JOIN 或 LEFT OUTER JOIN</p>
</li>
<li><p>右外连接：RIGHT JOIN 或 RIGHT OUTER JOIN</p>
</li>
<li><p>全外连接：FULL JOIN 或 FULL OUTER JOIN</p>
<p>全外连接实际上就是左外连接和右外连接的结合。在这三种外连接中，我们一般省略 OUTER 不写。</p>
<p>MySQL 不支持全外连接，否则的话全外连接会返回左表和右表中的所有行。当表之间有匹配的行，会显示内连接的结果。当某行在另一个表中没有匹配时，那么会把另一个表中选择的列显示为空值。</p>
<p>全外连接的结果 = 左右表匹配的数据 + 左表没有匹配到的数据 + 右表没有匹配到的数据。</p>
<h3 id="SQL99-和-SQL92-的区别"><a href="#SQL99-和-SQL92-的区别" class="headerlink" title="SQL99 和 SQL92 的区别"></a>SQL99 和 SQL92 的区别</h3><p>在 SQL92 中进行查询时，会把所有需要连接的表都放到 FROM 之后，然后在 WHERE 中写明连接的条件。而 SQL99 在这方面更灵活，它不需要一次性把所有需要连接的表都放到 FROM 之后，而是采用 JOIN 的方式，每次连接一张表，可以多次使用 JOIN 进行连接。</p>
<p>建议多表连接使用 SQL99 标准，因为层次性更强，可读性更强，比如：</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ...</span><br><span class="line"><span class="keyword">FROM</span> table1</span><br><span class="line">    <span class="keyword">JOIN</span> table2 <span class="keyword">ON</span> table1 和 table2 的连接条件</span><br><span class="line">        <span class="keyword">JOIN</span> table3 <span class="keyword">ON</span> table2 和 table3 的连接条件</span><br></pre></td></tr></table></figure>

<p>  SQL99 采用的这种嵌套结构非常清爽，即使再多的表进行连接也都清晰可见。如果你采用 SQL92，可读性就会大打折扣。</p>
<p>  最后一点就是，SQL99 在 SQL92 的基础上提供了一些特殊语法，比如 NATURAL JOIN 和 JOIN USING。它们在实际中是比较常用的，省略了 ON 后面的等值条件判断，让 SQL 语句更加简洁。</p>
  <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/SQL99.png" class title="SQL99">

<h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><p>  视图，也就是虚拟表</p>
  <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%BA%94%E7%94%A8%E6%9F%A5%E8%AF%A2.jpg" class title="应用查询">

<h3 id="创建视图-CREATE-VIEW"><a href="#创建视图-CREATE-VIEW" class="headerlink" title="创建视图 CREATE VIEW"></a>创建视图 CREATE VIEW</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_name <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> column1, column2</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">table</span></span><br><span class="line"><span class="keyword">WHERE</span> condition</span><br></pre></td></tr></table></figure>

<p>  实际上就是我们在 SQL 查询语句的基础上封装了视图 VIEW，这样就会基于 SQL 语句的结果集形成一张虚拟表。其中 view_name 为视图名称，column1、column2 代表列名，condition 代表查询过滤条件。</p>
<h3 id="修改视图-ALTER-VIEW"><a href="#修改视图-ALTER-VIEW" class="headerlink" title="修改视图 ALTER VIEW"></a>修改视图 ALTER VIEW</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> view_name <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> column1, column2</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">table</span></span><br><span class="line"><span class="keyword">WHERE</span> condition</span><br></pre></td></tr></table></figure>

<h3 id="删除视图-DROP-VIEW"><a href="#删除视图-DROP-VIEW" class="headerlink" title="删除视图 DROP VIEW"></a>删除视图 DROP VIEW</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> view_name</span><br></pre></td></tr></table></figure>

<h3 id="视图简化SQL操作"><a href="#视图简化SQL操作" class="headerlink" title="视图简化SQL操作"></a>视图简化SQL操作</h3><ul>
<li><p>完成复杂的连接</p>
</li>
<li><p>对数据进行格式化</p>
</li>
<li><p>使用视图计算字段</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3></li>
<li><p>安全性：虚拟表是基于底层数据表的，我们在使用视图时，一般不会轻易通过视图对底层数据进行修改。</p>
</li>
<li><p>简单清晰：视图是对 SQL 查询的封装，它可以将原本复杂的 SQL 查询简化，在编写好查询之后，我们就可以直接重用它而不必要知道基本的查询细节。</p>
</li>
<li><p>临时表是真实存在的数据表，不过它不用于长期存放数据，只为当前连接存在，关闭连接后，临时表就会自动释放。</p>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E8%A7%86%E5%9B%BE.png" class title="视图">

<h2 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h2><p>SQL 的存储过程是 SQL 中另一个重要应用，和视图一样，都是对 SQL 代码进行封装，可以反复利用。它和视图有着同样的优点，清晰、安全，还可以减少网络传输量。</p>
<p>存储过程的英文是 Stored Procedure。它的思想很简单，就是 SQL 语句的封装。一旦存储过程被创建出来，使用它就像使用函数一样简单，我们直接通过调用存储过程名即可。</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>使用 CREATE PROCEDURE 创建一个存储过程，后面是存储过程的名称，以及过程所带的参数，可以包括输入参数和输出参数。最后由 BEGIN 和 END 来定义我们所要执行的语句块。</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> 存储过程名称 ([参数列表])</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">    需要执行的语句</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  删除已经创建的存储过程，使用的是 DROP PROCEDURE。如果要更新存储过程，我们需要使用 ALTER PROCEDURE。</p>
<p>  例子：<br>  累加运算，计算 1+2+…+n 等于多少</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`add_num`</span>(<span class="keyword">IN</span> n <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">       <span class="keyword">DECLARE</span> i <span class="built_in">INT</span>;</span><br><span class="line">       <span class="keyword">DECLARE</span> <span class="keyword">sum</span> <span class="built_in">INT</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">SET</span> i = <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="number">0</span>;</span><br><span class="line">       WHILE i &lt;= n DO</span><br><span class="line">              <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="keyword">sum</span> + i;</span><br><span class="line">              <span class="keyword">SET</span> i = i +<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">END</span> <span class="keyword">WHILE</span>;</span><br><span class="line">       <span class="keyword">SELECT</span> <span class="keyword">sum</span>;</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  如果你使用 Navicat 这个工具来管理 MySQL 执行存储过程，那么直接执行上面这段代码就可以了。如果用的是 MySQL，你还需要用 DELIMITER 来临时定义新的结束符。因为默认情况下 SQL 采用（；）作为结束符，这样当存储过程中的每一句 SQL 结束之后，采用（；）作为结束符，就相当于告诉 SQL 可以执行这一句了。但是存储过程是一个整体，我们不希望 SQL 逐条执行，而是采用存储过程整段执行的方式，因此我们就需要临时定义新的 DELIMITER，新的结束符可以用（//）或者（$$）。如果你用的是 MySQL，那么上面这段代码，应该写成下面这样：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">DELIMITER //</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`add_num`</span>(<span class="keyword">IN</span> n <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">       <span class="keyword">DECLARE</span> i <span class="built_in">INT</span>;</span><br><span class="line">       <span class="keyword">DECLARE</span> <span class="keyword">sum</span> <span class="built_in">INT</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">SET</span> i = <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="number">0</span>;</span><br><span class="line">       WHILE i &lt;= n DO</span><br><span class="line">              <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="keyword">sum</span> + i;</span><br><span class="line">              <span class="keyword">SET</span> i = i +<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">END</span> <span class="keyword">WHILE</span>;</span><br><span class="line">       <span class="keyword">SELECT</span> <span class="keyword">sum</span>;</span><br><span class="line"><span class="keyword">END</span> //</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>

<h3 id="存储过程的三种参数类型"><a href="#存储过程的三种参数类型" class="headerlink" title="存储过程的三种参数类型"></a>存储过程的三种参数类型</h3>  <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E7%B1%BB%E5%9E%8B.png" class title="存储过程类型">

<p>  例子：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`get_hero_scores`</span>(</span><br><span class="line">       <span class="keyword">OUT</span> max_max_hp <span class="built_in">FLOAT</span>,</span><br><span class="line">       <span class="keyword">OUT</span> min_max_mp <span class="built_in">FLOAT</span>,</span><br><span class="line">       <span class="keyword">OUT</span> avg_max_attack <span class="built_in">FLOAT</span>,  </span><br><span class="line">       s <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</span><br><span class="line">       )</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">       <span class="keyword">SELECT</span> <span class="keyword">MAX</span>(hp_max), <span class="keyword">MIN</span>(mp_max), <span class="keyword">AVG</span>(attack_max) <span class="keyword">FROM</span> heros <span class="keyword">WHERE</span> role_main = s <span class="keyword">INTO</span> max_max_hp, min_max_mp, avg_max_attack;</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  调用存储过程：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CALL</span> get_hero_scores(@max_max_hp, @min_max_mp, @avg_max_attack, <span class="string">'战士'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> @max_max_hp, @min_max_mp, @avg_max_attack;</span><br></pre></td></tr></table></figure>

<h3 id="流控制语句"><a href="#流控制语句" class="headerlink" title="流控制语句"></a>流控制语句</h3><ol>
<li>BEGIN…END：BEGIN…END 中间包含了多个语句，每个语句都以（;）号为结束符。</li>
<li>DECLARE：DECLARE 用来声明变量，使用的位置在于 BEGIN…END 语句中间，而且需要在其他语句使用之前进行变量的声明。</li>
<li>SET：赋值语句，用于对变量进行赋值。</li>
<li>SELECT…INTO：把从数据表中查询的结果存放到变量中，也就是为变量赋值。</li>
<li>IF…THEN…ENDIF：条件判断语句，我们还可以在 IF…THEN…ENDIF 中使用 ELSE 和 ELSEIF 来进行条件判断。</li>
<li>CASE：CASE 语句用于多条件的分支判断，使用的语法是下面这样的。</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CASE </span><br><span class="line">	WHEN expression1 THEN ...</span><br><span class="line">	WHEN expression2 THEN ...</span><br><span class="line">	...</span><br><span class="line">    ELSE </span><br><span class="line">    <span class="comment">--ELSE 语句可以加，也可以不加。加的话代表的所有条件都不满足时采用的方式。</span></span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>
<ol start="7">
<li><p>LOOP、LEAVE 和 ITERATE：LOOP 是循环语句，使用 LEAVE 可以跳出循环，使用 ITERATE 则可以进入下一次循环。如果你有面向过程的编程语言的使用经验，你可以把 LEAVE 理解为 BREAK，把 ITERATE 理解为 CONTINUE。</p>
</li>
<li><p>REPEAT…UNTIL…END REPEAT：这是一个循环语句，首先会执行一次循环，然后在 UNTIL 中进行表达式的判断，如果满足条件就退出，即 END REPEAT；如果条件不满足，则会就继续执行循环，直到满足退出条件为止。</p>
</li>
<li><p>WHILE…DO…END WHILE：这也是循环语句，和 REPEAT 循环不同的是，这个语句需要先进行条件判断，如果满足条件就进行循环，如果不满足条件就退出循环。</p>
<h3 id="存储过程的争议"><a href="#存储过程的争议" class="headerlink" title="存储过程的争议"></a>存储过程的争议</h3></li>
</ol>
<ul>
<li><p>优点</p>
<ul>
<li>多次使用</li>
<li>可以封装，减少工作量，代码结构清晰</li>
<li>安全性强</li>
<li>减少网络传输</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>可移植性差</li>
<li>调试困难</li>
<li>版本管理困难</li>
<li>不适合高并发场景</li>
</ul>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B.png" class title="存储过程">

<h2 id="事务处理"><a href="#事务处理" class="headerlink" title="事务处理"></a>事务处理</h2><h3 id="事务的特性"><a href="#事务的特性" class="headerlink" title="事务的特性"></a>事务的特性</h3><p>要么完全执行，要么都不执行。不过要对事务进行更深一步的理解，还要从事务的 4 个特性说起，这 4 个特性用英文字母来表达就是 ACID。</p>
</li>
<li><p>A，也就是原子性（Atomicity）。原子的概念就是不可分割，你可以把它理解为组成物质的基本单位，也是我们进行数据处理操作的基本单位。</p>
</li>
<li><p>C，就是一致性（Consistency）。一致性指的就是数据库在进行事务操作后，会由原来的一致状态，变成另一种一致的状态。也就是说当事务提交后，或者当事务发生回滚后，数据库的完整性约束不能被破坏。</p>
</li>
<li><p>I，就是隔离性（Isolation）。它指的是每个事务都是彼此独立的，不会受到其他事务的执行影响。也就是说一个事务在提交之前，对其他事务都是不可见的。</p>
</li>
<li><p>D，指的是持久性（Durability）。事务提交之后对数据的修改是持久性的，即使在系统出故障的情况下，比如系统崩溃或者存储介质发生故障，数据的修改依然是有效的。因为当事务完成，数据库的日志就会被更新，这时可以通过日志，让系统恢复到最后一次成功的更新状态。</p>
<p>ACID 可以说是事务的四大特性，在这四个特性中，原子性是基础，隔离性是手段，一致性是约束条件，而持久性是我们的目的。</p>
<p>任何写入数据库中的数据都需要满足我们事先定义的约束规则。事务操作会让数据表的状态变成另一种一致的状态，如果事务中的某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作之前的状态。</p>
<p>事务的另一个特点就是持久性，持久性是通过事务日志来保证的。日志包括了回滚日志和重做日志。当我们通过事务对数据进行修改的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。</p>
<h3 id="事务的控制"><a href="#事务的控制" class="headerlink" title="事务的控制"></a>事务的控制</h3><p>MySQL，可以通过 SHOW ENGINES 命令来查看当前 MySQL 支持的存储引擎都有哪些，以及这些存储引擎是否支持事务。InnoDB 是支持事务的，而 MyISAM 存储引擎不支持事务。</p>
</li>
</ul>
<ol>
<li><p>START TRANSACTION 或者 BEGIN，作用是显式开启一个事务。</p>
</li>
<li><p>COMMIT：提交事务。当提交事务后，对数据库的修改是永久性的。</p>
</li>
<li><p>ROLLBACK 或者 ROLLBACK TO [SAVEPOINT]，意为回滚事务。意思是撤销正在进行的所有没有提交的修改，或者将事务回滚到某个保存点。</p>
</li>
<li><p>SAVEPOINT：在事务中创建保存点，方便后续针对保存点进行回滚。一个事务中可以存在多个保存点。</p>
</li>
<li><p>RELEASE SAVEPOINT：删除某个保存点。</p>
</li>
<li><p>SET TRANSACTION，设置事务的隔离级别。</p>
<p>需要说明的是，使用事务有两种方式，分别为隐式事务和显式事务。隐式事务实际上就是自动提交，Oracle 默认不自动提交，需要手写 COMMIT 命令，而 MySQL 默认自动提交，当然我们可以配置 MySQL 的参数：</p>
<p>mysql&gt; set autocommit =0;  // 关闭自动提交<br>mysql&gt; set autocommit =1;  // 开启自动提交</p>
<p>MySQL 中 completion_type 参数的作用，实际上这个参数有 3 种可能：</p>
</li>
<li><p>completion=0，这是默认情况。也就是说当我们执行 COMMIT 的时候会提交事务，在执行下一个事务时，还需要我们使用 START TRANSACTION 或者 BEGIN 来开启。</p>
</li>
<li><p>completion=1，这种情况下，当我们提交事务后，相当于执行了 COMMIT AND CHAIN，也就是开启一个链式事务，即当我们提交事务之后会开启一个相同隔离级别的事务（隔离级别会在下一节中进行介绍）。</p>
</li>
<li><p>completion=2，这种情况下 COMMIT=COMMIT AND RELEASE，也就是当我们提交后，会自动与服务器断开连接。</p>
<p>当我们设置 autocommit=0 时，不论是否采用 START TRANSACTION 或者 BEGIN 的方式来开启事务，都需要用 COMMIT 进行提交，让事务生效，使用 ROLLBACK 对事务进行回滚。</p>
<p>当我们设置 autocommit=1 时，每条 SQL 语句都会自动进行提交。不过这时，如果你采用 START TRANSACTION 或者 BEGIN 的方式来显式地开启事务，那么这个事务只有在 COMMIT 时才会生效，在 ROLLBACK 时才会回滚。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>正是因为有事务的存在，即使在数据库操作失败的情况下，也能保证数据的一致性。同样，多个应用程序访问数据库的时候，事务可以提供隔离，保证事务之间不被干扰。最后，事务一旦提交，结果就会是永久性的，这就意味着，即使系统崩溃了，数据库也可以对数据进行恢复。</p>
<p>事务是数据库区别于文件系统的重要特性之一，当我们有了事务就会让数据库始终保持一致性，同时我们还能通过事务的机制恢复到某个时间点，这样可以保证已提交到数据库的修改不会因为系统崩溃而丢失。</p>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86.png" class title="事务处理">

<h2 id="事务隔离"><a href="#事务隔离" class="headerlink" title="事务隔离"></a>事务隔离</h2><h3 id="事务并发的可能异常"><a href="#事务并发的可能异常" class="headerlink" title="事务并发的可能异常"></a>事务并发的可能异常</h3><p>SQL-92 标准中已经对 3 种异常情况进行了定义，这些异常情况级别分别为脏读（Dirty Read）、不可重复读（Nnrepeatable Read）和幻读（Phantom Read）。</p>
</li>
<li><p>脏读：读到了其他事务还没有提交的数据。</p>
</li>
<li><p>不可重复读：对某数据进行读取，发现两次读取的结果不同，也就是说没有读到相同的内容。这是因为有其他事务对这个数据同时进行了修改或删除。</p>
</li>
<li><p>幻读：事务 A 根据条件查询得到了 N 条数据，但此时事务 B 更改或者增加了 M 条符合事务 A 查询条件的数据，这样当事务 A 再次进行查询的时候发现会有 N+M 条数据，产生了幻读。</p>
</li>
</ol>
<p>  <strong>不可重复读 VS 幻读的区别：</strong></p>
<ul>
<li><p>不可重复读是同一条记录的内容被修改了，重点在于UPDATE或DELETE</p>
</li>
<li><p>幻读是查询某一个范围的数据行变多了或者少了，重点在于INSERT</p>
<p>SQL-92 标准还定义了 4 种隔离级别来解决这些异常情况。</p>
<p>解决异常数量从少到多的顺序（比如读未提交可能存在 3 种异常，可串行化则不会存在这些异常）决定了隔离级别的高低，这四种隔离级别从低到高分别是：读未提交（READ UNCOMMITTED ）、读已提交（READ COMMITTED）、可重复读（REPEATABLE READ）和可串行化（SERIALIZABLE）。这些隔离级别能解决的异常情况如下表所示：</p>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E5%86%B3%E5%BC%82%E5%B8%B8.png" class title="解决异常">
</li>
<li><p>读未提交，也就是允许读到未提交的数据，这种情况下查询是不会使用锁的，可能会产生脏读、不可重复读、幻读等情况。</p>
</li>
<li><p>读已提交就是只能读到已经提交的内容，可以避免脏读的产生，属于 RDBMS 中常见的默认隔离级别（比如说 Oracle 和 SQL Server），但如果想要避免不可重复读或者幻读，就需要我们在 SQL 查询的时候编写带加锁的 SQL 语句（我会在进阶篇里讲加锁）。</p>
</li>
<li><p>可重复读，保证一个事务在相同查询条件下两次查询得到的数据结果是一致的，可以避免不可重复读和脏读，但无法避免幻读。MySQL 默认的隔离级别就是可重复读。</p>
</li>
<li><p>可串行化，将事务进行串行化，也就是在一个队列中按照顺序执行，可串行化是最高级别的隔离等级，可以解决事务读取中所有可能出现的异常情况，但是它牺牲了系统的并发性。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>隔离级别的实现满足了下面的两个条件：</p>
</li>
<li><p>正确性：只要能满足某一个隔离级别，一定能解决这个隔离级别对应的异常问题。</p>
</li>
<li><p>与实现无关：实际上 RDBMS 种类很多，这就意味着有多少种 RDBMS，就有多少种锁的实现方式，因此它们实现隔离级别的原理可能不同，然而一个好的标准不应该限制其实现的方式。</p>
<p>隔离级别越低，意味着系统吞吐量（并发程度）越大，但同时也意味着出现异常问题的可能性会更大。在实际使用过程中我们往往需要在性能和正确性上进行权衡和取舍，没有完美的解决方案，只有适合与否。</p>
<img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB.png" class title="事务隔离">

<h2 id="游标"><a href="#游标" class="headerlink" title="游标"></a>游标</h2><p>在数据库中，游标是个重要的概念，它提供了一种灵活的操作方式，可以让我们从数据结果集中每次提取一条数据记录进行操作。游标让 SQL 这种面向集合的语言有了面向过程开发的能力。可以说，游标是面向过程的编程方式，这与面向集合的编程方式有所不同。</p>
<p>在 SQL 中，游标是一种临时的数据库对象，可以指向存储在数据库表中的数据行指针。这里游标充当了指针的作用，我们可以通过操作游标来对数据行进行操作。</p>
<h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>游标实际上是一种控制数据集的更加灵活的处理方式。</p>
<p>如果我们想要使用游标，一般需要经历五个步骤。不同 DBMS 中，使用游标的语法可能略有不同。</p>
</li>
</ul>
<ol>
<li>定义游标。</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DECLARE</span> cursor_name <span class="keyword">CURSOR</span> <span class="keyword">FOR</span> select_statement</span><br></pre></td></tr></table></figure>

<p>  要使用 SELECT 语句来获取数据结果集，而此时还没有开始遍历数据，这里 select_statement 代表的是 SELECT 语句。</p>
<ol start="2">
<li>打开游标</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">OPEN cursor_name</span><br></pre></td></tr></table></figure>

<p>  当我们定义好游标之后，如果想要使用游标，必须先打开游标。打开游标的时候 SELECT 语句的查询结果集就会送到游标工作区。</p>
<ol start="3">
<li>从游标中取得数据</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FETCH cursor_name INTO var_name ...</span><br></pre></td></tr></table></figure>

<p>  这句的作用是使用 cursor_name 这个游标来读取当前行，并且将数据保存到 var_name 这个变量中，游标指针指到下一行。如果游标读取的数据行有多个列名，则在 INTO 关键字后面赋值给多个变量名即可。</p>
<ol start="4">
<li>关闭游标</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CLOSE cursor_name</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>释放游标</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DEALLOCATE</span> <span class="keyword">PREPARE</span></span><br></pre></td></tr></table></figure>

<p>  我们一定要养成释放游标的习惯，否则游标会一直存在于内存中，直到进程结束后才会自动释放。当你不需要使用游标的时候，释放游标可以减少资源浪费。</p>
<p>  例子：<br>  先创建一个存储过程 calc_hp_max，然后在存储过程中定义游标 cur_hero，使用 FETCH 获取每一行的具体数值，然后赋值给变量 hp，再用变量 hp_sum 做累加求和，最后再输出 hp_sum，代码如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`calc_hp_max`</span>()</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">        <span class="comment">-- 创建接收游标的变量</span></span><br><span class="line">        <span class="keyword">DECLARE</span> hp <span class="built_in">INT</span>;  </span><br><span class="line">        <span class="comment">-- 创建总数变量 </span></span><br><span class="line">        <span class="keyword">DECLARE</span> hp_sum <span class="built_in">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">-- 创建结束标志变量  </span></span><br><span class="line">        <span class="keyword">DECLARE</span> done <span class="built_in">INT</span> <span class="keyword">DEFAULT</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="comment">-- 定义游标     </span></span><br><span class="line">        <span class="keyword">DECLARE</span> cur_hero <span class="keyword">CURSOR</span> <span class="keyword">FOR</span> <span class="keyword">SELECT</span> hp_max <span class="keyword">FROM</span> heros;</span><br><span class="line">        <span class="comment">-- 指定游标循环结束时的返回值  </span></span><br><span class="line">        <span class="keyword">DECLARE</span> CONTINUE <span class="keyword">HANDLER</span> <span class="keyword">FOR</span> <span class="keyword">NOT</span> <span class="keyword">FOUND</span> <span class="keyword">SET</span> done = <span class="literal">true</span>;  </span><br><span class="line">        OPEN cur_hero;</span><br><span class="line">        read_loop:LOOP </span><br><span class="line">        FETCH cur_hero INTO hp;</span><br><span class="line">        <span class="keyword">SET</span> hp_sum = hp_sum + hp;</span><br><span class="line">        <span class="keyword">END</span> <span class="keyword">LOOP</span>;</span><br><span class="line">        CLOSE cur_hero;</span><br><span class="line">        <span class="keyword">SELECT</span> hp_sum;</span><br><span class="line">        <span class="keyword">DEALLOCATE</span> <span class="keyword">PREPARE</span> cur_hero;</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  当游标溢出时（也就是当游标指向到最后一行数据后继续执行会报的错误），我们可以定义一个 continue 的事件，指定这个事件发生时修改变量 done 的值，以此来判断游标是否已经溢出，即：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DECLARE</span> CONTINUE <span class="keyword">HANDLER</span> <span class="keyword">FOR</span> <span class="keyword">NOT</span> <span class="keyword">FOUND</span> <span class="keyword">SET</span> done = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>  在游标中的循环中，除了使用 LOOP 循环以外，你还可以使用 REPEAT… UNTIL…以及 WHILE 循环。它们同样需要设置 CONTINUE 事件来处理游标溢出的情况。</p>
<p>  所以你能看出，使用游标可以让我们对 SELECT 结果集中的每一行数据进行相同或者不同的操作，从而很精细化地管理结果集中的每一条数据。</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p>  游标实际上是面向过程的思维方式，与面向集合的思维方式不同的地方在于，游标更加关注“如何执行”。我们可以通过游标更加精细、灵活地查询和管理想要的数据行。</p>
<p>  有的时候，我们需要找特定数据，用 SQL 查询写起来会比较困难，比如两表或多表之间的嵌套循环查找，如果用 JOIN 会非常消耗资源，效率也可能不高，而用游标则会比较高效。</p>
<p>  虽然在处理某些复杂的数据情况下，使用游标可以更灵活，但同时也会带来一些性能问题，比如在使用游标的过程中，会对数据行进行加锁，这样在业务并发量大的时候，不仅会影响业务之间的效率，还会消耗系统资源，造成内存不足，这是因为游标是在内存中进行的处理。如果有游标的替代方案，我们可以采用替代方案。</p>
  <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E6%B8%B8%E6%A0%87.png" class title="游标">

<h2 id="Python-DB-API"><a href="#Python-DB-API" class="headerlink" title="Python DB API"></a>Python DB API</h2><p>  Python 可以支持非常多的数据库管理系统，比如 MySQL、Oracle、SQL Server 和 PostgreSQL 等。为了实现对这些 DBMS 的统一访问，Python 需要遵守一个规范，这就是 DB API 规范。</p>
  <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/pythonAPI.png" class title="pythonAPI">

<p>  使用 Python 对 DBMS 进行操作的时候，需要经过下面的几个步骤：</p>
<ul>
<li><p>引入 API 模块；</p>
</li>
<li><p>与数据库建立连接；</p>
</li>
<li><p>执行 SQL 语句；</p>
</li>
<li><p>关闭数据库连接。</p>
<h3 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h3><p>例子：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> mysql.connector</span><br><span class="line"><span class="comment"># 打开数据库连接</span></span><br><span class="line">db = mysql.connector.connect(</span><br><span class="line">       host=<span class="string">"localhost"</span>,</span><br><span class="line">       user=<span class="string">"root"</span>,</span><br><span class="line">       passwd=<span class="string">"XXX"</span>, <span class="comment"># 写上你的数据库密码</span></span><br><span class="line">       database=<span class="string">'wucai'</span>, </span><br><span class="line">       auth_plugin=<span class="string">'mysql_native_password'</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 获取操作游标 </span></span><br><span class="line">cursor = db.cursor()</span><br><span class="line"><span class="comment"># 执行 SQL 语句</span></span><br><span class="line">cursor.execute(<span class="string">"SELECT VERSION()"</span>)</span><br><span class="line"><span class="comment"># 获取一条数据</span></span><br><span class="line">data = cursor.fetchone()</span><br><span class="line">print(<span class="string">"MySQL 版本: %s "</span> % data)</span><br><span class="line"><span class="comment"># 关闭游标 &amp; 数据库连接</span></span><br><span class="line">cursor.close()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>

<p>  以上代码有两个重要部分Connection 和 Cursor</p>
<p>  Connection 就是对数据库的当前连接进行管理，我们可以通过它来进行以下操作：</p>
<ol>
<li><p>通过指定 host、user、passwd 和 port 等参数来创建数据库连接，这些参数分别对应着数据库 IP 地址、用户名、密码和端口号；</p>
</li>
<li><p>使用 db.close() 关闭数据库连接；</p>
</li>
<li><p>使用 db.cursor() 创建游标，操作数据库中的数据；</p>
</li>
<li><p>使用 db.begin() 开启事务；</p>
</li>
<li><p>使用 db.commit() 和 db.rollback()，对事务进行提交以及回滚。</p>
<p>当我们通过cursor = db.cursor()创建游标后，就可以通过面向过程的编程方式对数据库中的数据进行操作：</p>
</li>
<li><p>使用cursor.execute(query_sql)，执行数据库查询；</p>
</li>
<li><p>使用cursor.fetchone()，读取数据集中的一条数据；</p>
</li>
<li><p>使用cursor.fetchall()，取出数据集中的所有行，返回一个元组 tuples 类型；</p>
</li>
<li><p>使用cursor.fetchmany(n)，取出数据集中的多条数据，同样返回一个元组 tuples；</p>
</li>
<li><p>使用cursor.rowcount，返回查询结果集中的行数。如果没有查询到数据或者还没有查询，则结果为 -1，否则会返回查询得到的数据行数；</p>
</li>
<li><p>使用cursor.close()，关闭游标。</p>
<p>增删改查的时候可以使用捕获异常</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  sql = <span class="string">"INSERT INTO player (team_id, player_name, height) VALUES (%s, %s, %s)"</span></span><br><span class="line">  val = (<span class="number">1003</span>, <span class="string">" 约翰 - 科林斯 "</span>, <span class="number">2.08</span>)</span><br><span class="line">  cursor.execute(sql, val)</span><br><span class="line">  db.commit()</span><br><span class="line">  print(cursor.rowcount, <span class="string">" 记录插入成功。"</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">  <span class="comment"># 打印异常信息</span></span><br><span class="line">  traceback.print_exc()</span><br><span class="line">  <span class="comment"># 回滚  </span></span><br><span class="line">  db.rollback()</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">  <span class="comment"># 关闭数据库连接</span></span><br><span class="line">  db.close()</span><br></pre></td></tr></table></figure>

<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><p>  在使用基于 DB API 规范的协议时，重点需要掌握 Connection 和 Cursor 这两个对象，Connection 就是对数据库的连接进行管理，而 Cursor 是对数据库的游标进行管理，通过它们，我们可以执行具体的 SQL 语句，以及处理复杂的数据。</p>
<p>  用 Python 操作 MySQL，还有很多种姿势，mysql-connector 只是其中一种，实际上还有另外一种方式，就是采用 ORM 框架。ORM 的英文是 Object Relational Mapping，也就是采用对象关系映射的模式，使用这种模式可以将数据库中各种数据表之间的关系映射到程序中的对象。这种模式可以屏蔽底层的数据库的细节，不需要我们与复杂的 SQL 语句打交道，直接采用操作对象的形式操作就可以。</p>
  <img data-src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/pythonMySQL.png" class title="pythonMySQL">

<h2 id="ORM框架"><a href="#ORM框架" class="headerlink" title="ORM框架"></a>ORM框架</h2><p>  持久化层在业务逻辑层和数据库层起到了衔接的作用，它可以将内存中的数据模型转化为存储模型，或者将存储模型转化为内存中的数据模型。</p>
  

<p>  在程序的层面操作数据，其实都是把数据放到内存中进行处理，如果需要数据就会通过持久化层，从数据库中取数据；如果需要保存数据，就是将对象数据通过持久化层存储到数据库中。</p>
<p>   ORM 提供了一种持久化模式，可以高效地对数据库进行访问。ORM 的英文是 Object Relation Mapping，中文叫对象关系映射。它是 RDBMS 和业务实体对象之间的一个映射，从图中你也能看到，它可以把底层的 RDBMS 封装成业务实体对象，提供给业务逻辑层使用。程序员往往关注业务逻辑层面，而不是底层数据库该如何访问，以及如何编写 SQL 语句获取数据等等。采用 ORM，就可以从数据库的设计层面转化成面向对象的思维。</p>
<p>   没有一种模式是完美的，采用 ORM 当然也会付出一些代价，比如性能上的一些损失。面对一些复杂的数据查询，ORM 会显得力不从心。虽然可以实现功能，但相比于直接编写 SQL 查询语句来说，ORM 需要编写的代码量和花费的时间会比较多，这种情况下，直接编写 SQL 反而会更简单有效。</p>
<h3 id="python中的三种主流ORM框架"><a href="#python中的三种主流ORM框架" class="headerlink" title="python中的三种主流ORM框架"></a>python中的三种主流ORM框架</h3><ol>
<li><p>Django，它是 Python 的 WEB 应用开发框架，本身走大而全的方式。Django 采用了 MTV 的框架模式，包括了 Model（模型），View（视图）和 Template（模版）。Model 模型只是 Django 的一部分功能，我们可以通过它来实现数据库的增删改查操作。</p>

</li>
<li><p>SQLALchemy，它也是 Python 中常用的 ORM 框架之一。它提供了 SQL 工具包及 ORM 工具，如果你想用支持 ORM 和支持原生 SQL 两种方式的工具，那么 SQLALchemy 是很好的选择。另外 SQLALchemy 的社区更加活跃，这对项目实施会很有帮助。</p>
</li>
<li><p>peewee，这是一个轻量级的 ORM 框架，简单易用。peewee 采用了 Model 类、Field 实例和 Model 实例来与数据库建立映射关系，从而完成面向对象的管理方式。使用起来方便，学习成本也低。</p>
<h3 id="SQLAlchemy-操作-MySQL"><a href="#SQLAlchemy-操作-MySQL" class="headerlink" title="SQLAlchemy 操作 MySQL"></a>SQLAlchemy 操作 MySQL</h3></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install sqlalchemy</span><br><span class="line">初始化数据库连接</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="comment"># 初始化数据库连接，修改为你的数据库用户名和密码</span></span><br><span class="line">engine = create_engine(<span class="string">'mysql+mysqlconnector://root:password@localhost:3306/wucai'</span>)</span><br><span class="line"><span class="comment"># mysql+mysqlconnector，后面的是用户名:密码@IP地址:端口号/数据库名称</span></span><br></pre></td></tr></table></figure>

<h4 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> Column, String, Integer, Float</span><br><span class="line"><span class="comment"># 定义 Player 对象:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Player</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="comment"># 表的名字:</span></span><br><span class="line">    __tablename__ = <span class="string">'player'</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 表的结构:</span></span><br><span class="line">    player_id = Column(Integer, primary_key=<span class="literal">True</span>, autoincrement=<span class="literal">True</span>)</span><br><span class="line">    team_id = Column(Integer)</span><br><span class="line">    player_name = Column(String(<span class="number">255</span>))</span><br><span class="line">    height = Column(Float(<span class="number">3</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>  <strong>tablename</strong> 指明了模型对应的数据表名称，即 player 数据表。同时我们在 Player 模型中对采用的变量名进行定义，变量名需要和数据表中的字段名称保持一致，否则会找不到数据表中的字段。</p>
<h4 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 DBSession 类型:</span></span><br><span class="line">DBSession = sessionmaker(bind=engine)</span><br><span class="line"><span class="comment"># 创建 session 对象:</span></span><br><span class="line">session = DBSession()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建 Player 对象:</span></span><br><span class="line">new_player = Player(team_id = <span class="number">1003</span>, player_name = <span class="string">" 约翰 - 科林斯 "</span>, height = <span class="number">2.08</span>)</span><br><span class="line"><span class="comment"># 添加到 session:</span></span><br><span class="line">session.add(new_player)</span><br><span class="line"><span class="comment"># 提交即保存到数据库:</span></span><br><span class="line">session.commit()</span><br><span class="line"><span class="comment"># 关闭 session:</span></span><br><span class="line">session.close()</span><br></pre></td></tr></table></figure>

<p>  查询数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 增加 to_dict() 方法到 Base 类中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;c.name: getattr(self, c.name, <span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> self.__table__.columns&#125;</span><br><span class="line"><span class="comment"># 将对象可以转化为 dict 类型</span></span><br><span class="line">Base.to_dict = to_dict</span><br><span class="line"><span class="comment"># 查询身高 &gt;=2.08 的球员有哪些</span></span><br><span class="line">rows = session.query(Player).filter(Player.height &gt;= <span class="number">2.08</span>).all()</span><br><span class="line">print([row.to_dict() <span class="keyword">for</span> row <span class="keyword">in</span> rows])</span><br></pre></td></tr></table></figure>

<p>  删除数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">row = session.query(Player).filter(Player.player_name==<span class="string">'约翰 - 科林斯'</span>).first()</span><br><span class="line">session.delete(row)</span><br><span class="line">session.commit()</span><br><span class="line">session.close()</span><br></pre></td></tr></table></figure>

<h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><p>  如果项目本身不大，那么自己动手写 SQL 语句会比较简单，可以不使用 ORM 工具，而是直接使用 mysql-connector。但是随着项目代码量的增加，为了在业务逻辑层与数据库底层进行松耦合，采用 ORM 框架是更加适合的。</p>
  


<h2 id="基础总结"><a href="#基础总结" class="headerlink" title="基础总结"></a>基础总结</h2><ul>
<li><p>列式数据库是将数据按照列存储到数据库中，这样做的好处是可以大量降低系统的 I/O</p>
<p>行式存储是把一行的数据都串起来进行存储，然后再存储下一行。同样，列式存储是把一列的数据都串起来进行存储，然后再存储下一列。这样做的话，相邻数据的数据类型都是一样的，更容易压缩，压缩之后就自然降低了 I/O。</p>
<p>数据处理可以分为 OLTP（联机事务处理）和 OLAP（联机分析处理）两大类。</p>
<p>OLTP 一般用于处理客户的事务和进行查询，需要随时对数据表中的记录进行增删改查，对实时性要求高。</p>
<p>OLAP 一般用于市场的数据分析，通常数据量大，需要进行复杂的分析操作，可以对大量历史数据进行汇总和分析，对实时性要求不高。</p>
<p>那么对于 OLTP 来说，由于随时需要对数据记录进行增删改查，更适合采用行式存储，因为一行数据的写入会同时修改多个列。传统的 RDBMS 都属于行式存储，比如 Oracle、SQL Server 和 MySQL 等。</p>
<p>对于 OLAP 来说，由于需要对大量历史数据进行汇总和分析，则适合采用列式存储，这样的话汇总数据会非常快，但是对于插入（INSERT）和更新（UPDATE）会比较麻烦，相比于行式存储性能会差不少。</p>
<p>所以说列式存储适合大批量数据查询，可以降低 I/O，但如果对实时性要求高，则更适合行式存储。</p>
</li>
<li><p>在 MySQL InnoDB 存储引擎中，COUNT(*)和COUNT(1)都是对所有结果进行COUNT。如果有 WHERE 子句，则是对所有符合筛选条件的数据行进行统计；如果没有 WHERE 子句，则是对数据表的数据行数进行统计。</p>
<p>一般情况下，三者执行的效率为 COUNT(*)= COUNT(1)&gt; COUNT(字段)。我们尽量使用COUNT(*)，当然如果你要统计的是某个字段的非空数据行数，则另当别论，毕竟比较执行效率的前提是结果一样才可以。<br>如果要统计COUNT(*),尽量在数据表上建立二级索引，系统会自动采用key_len小的二级索引进行扫描，这样当我们使用SELECT COUNT(*)的时候效率就会提升，有时候可以提升几倍甚至更高。</p>
</li>
<li><p>一条完整的 SELECT 语句内部的执行顺序是这样的：</p>
</li>
</ul>
<ol>
<li>FROM 子句组装数据（包括通过 ON 进行连接）；</li>
<li>WHERE 子句进行条件筛选；</li>
<li>GROUP BY 分组 ；</li>
<li>使用聚集函数进行计算；</li>
<li>HAVING 筛选分组；</li>
<li>计算所有的表达式；</li>
<li>SELECT 的字段；</li>
<li>ORDER BY 排序；</li>
<li>LIMIT 筛选。</li>
</ol>
<ul>
<li><p>varchar和nvarchar区别</p>
<ul>
<li><p>相同点：可变长度，字符类型数据</p>
</li>
<li><p>不同点：</p>
<p>  varchar(n)是n个字节，非Unicode字符。（英文字母占1个字节，中文占2个字节）</p>
<p>  而nvarchar(n)是n个字符，Unicode字符。（英文字母或者中文都是占用2个字节）</p>
</li>
</ul>
<p>举个例子，varchar(10)代表10个字节，所以可以是10个英文字母，也可以是5个汉字。<br>而nvarchar(10)代表10个字符，这10个字符可以是10个字母，也可以是10个汉字（英文字母或者中文 都是占用2个字节）</p>
</li>
</ul>
]]></content>
      <categories>
        <category>SQL系列</category>
      </categories>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(二) · 核心编程</title>
    <url>/2020/02/26/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="RDD常用操作"><a href="#RDD常用操作" class="headerlink" title="RDD常用操作"></a>RDD常用操作</h2><h3 id="RDD-Operation"><a href="#RDD-Operation" class="headerlink" title="RDD Operation"></a>RDD Operation</h3><ol>
<li><p>transformations: which create a new dataset from an existing one</p>
<p> lazy()<br> map/filter/group by/distinct/…</p>
</li>
<li><p>actions: which return a value to the driver program after running a computation on the dataset</p>
<p> count/reduce/collect/….</p>
</li>
</ol>
<p>transformations are lazy. The transformations are only computed when an action requires a result to be returned to the driver program. </p>
<p>action triggers the computation, returns values to driver or writes data to external storage.</p>
<h4 id="transformations"><a href="#transformations" class="headerlink" title="transformations"></a>transformations</h4><ol>
<li><p>map:</p>
<p> map(func) </p>
<p> 将func函数作用到数据集的每一个元素上，生成一个新的分布式数据集返回</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_map</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    rdd1 = sc.parallelize(data)</span><br><span class="line">    rdd2 = rdd1.map(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line">    print(rdd2.collect())</span><br></pre></td></tr></table></figure>


<ol start="2">
<li><p>filter：</p>
<p> filter(func)</p>
<p> 选出所有func返回值为true的元素，生成一个新的分布式数据集返回</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_filter</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    rdd1 = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd1.map(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line">    filterRdd = mapRdd.filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>)</span><br><span class="line">    print(filterRdd.collect())</span><br><span class="line"></span><br><span class="line">    print(sc.parallelize(data).map(</span><br><span class="line">        <span class="keyword">lambda</span> x: x*<span class="number">2</span>).filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>).collect())</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>flatMap：</p>
<p> faltMap(func)</p>
<p> 输入的item能够被map到0或者多个items输出，返回值是一个Sequence</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_flatMap</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    print(sc.parallelize(data).flatMap(</span><br><span class="line">        <span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).collect())</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>groupByKey：</p>
<p> groupByKey(func)</p>
<p> 把相同的key的数据分发到一起</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_groupBy</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)</span><br><span class="line">                            ).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    groupByRdd = mapRdd.groupByKey()</span><br><span class="line">    print(groupByRdd.collect())</span><br><span class="line">    print(groupByRdd.map(<span class="keyword">lambda</span> x: &#123;x[<span class="number">0</span>]: list(x[<span class="number">1</span>])&#125;).collect())</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p>reduceByKey：</p>
<p> reduceByKey(func)</p>
<p> 把相同的key的数据分发到一起，并进行相应的计算</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_reduceByKey</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)</span><br><span class="line">                            ).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    reduceByKeyRdd = mapRdd.reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    print(reduceByKeyRdd.collect())</span><br></pre></td></tr></table></figure>

<ol start="6">
<li><p>sortByKey：</p>
<p> 顺序排列，按照key的顺序，缺省True为升序，False为降序</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_sortByKey</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)</span><br><span class="line">                            ).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    reduceByKeyRdd = mapRdd.reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    print(reduceByKeyRdd.collect())</span><br><span class="line">    print(reduceByKeyRdd.map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>])).sortByKey(<span class="literal">False</span>).map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>])).collect())</span><br></pre></td></tr></table></figure>

<ol start="7">
<li><p>union：</p>
<p> 数据集并集</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_union</span><span class="params">()</span>:</span></span><br><span class="line">    a = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    b = sc.parallelize([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    print(a.union(b).collect())</span><br></pre></td></tr></table></figure>

<ol start="8">
<li><p>distinct：</p>
<p> 取唯一</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_distinct</span><span class="params">()</span>:</span></span><br><span class="line">    a = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    b = sc.parallelize([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    print(a.union(b).collect())</span><br><span class="line">    print(a.union(b).distinct().collect())</span><br></pre></td></tr></table></figure>

<ol start="9">
<li><p>join：</p>
<p> 连接：左连接/右连接/全连接</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_join</span><span class="params">()</span>:</span></span><br><span class="line">    a = sc.parallelize([(<span class="string">'A'</span>, <span class="string">'a1'</span>), (<span class="string">'C'</span>, <span class="string">'c1'</span>), (<span class="string">'D'</span>, <span class="string">'d1'</span>), (<span class="string">'F'</span>, <span class="string">'f1'</span>), (<span class="string">'F'</span>, <span class="string">'f2'</span>)])</span><br><span class="line">    b = sc.parallelize([(<span class="string">'A'</span>, <span class="string">'a2'</span>), (<span class="string">'C'</span>, <span class="string">'c2'</span>), (<span class="string">'C'</span>, <span class="string">'c3'</span>), (<span class="string">'E'</span>, <span class="string">'e1'</span>)])</span><br><span class="line">    print(a.join(b).collect())</span><br><span class="line">    print(a.leftOuterJoin(b).collect())</span><br><span class="line">    print(a.rightOuterJoin(b).collect())</span><br><span class="line">    print(a.fullOuterJoin(b).collect())</span><br></pre></td></tr></table></figure>

<h4 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h4><p>collect/count/take/max/min/sum/reduce/foreach/…</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_action</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    print(rdd.collect())</span><br><span class="line">    print(rdd.count())</span><br><span class="line">    print(rdd.take(<span class="number">3</span>))</span><br><span class="line">    print(rdd.max())</span><br><span class="line">    print(rdd.min())</span><br><span class="line">    print(rdd.sum())</span><br><span class="line">    print(rdd.reduce(<span class="keyword">lambda</span> x, y: x+y))</span><br><span class="line">    rdd.foreach(<span class="keyword">lambda</span> x: print(x))</span><br></pre></td></tr></table></figure>

<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><h4 id="词频统计"><a href="#词频统计" class="headerlink" title="词频统计"></a>词频统计</h4><ol>
<li><p>input：</p>
<p> 文件 文件夹 后缀名</p>
</li>
<li><p>步骤分析：</p>
<p> 文本内容转成一个个单词 ：flatMap</p>
<p> 单词 ==&gt; (单词， 1) ：map</p>
<p> 统计相加 ：reduceByKey</p>
</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line">findspark.init()</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">        print(<span class="string">'Usage: wordcount &lt;input&gt;&lt;output&gt;'</span>, file=sys.stderr)</span><br><span class="line">        sys.exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    sc = SparkContext(conf = conf)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printResult</span><span class="params">()</span>:</span></span><br><span class="line">        counts = sc.textFile(sys.argv[<span class="number">1</span>])\</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>))\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))\</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line"></span><br><span class="line">        output = counts.collect()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(word, count) <span class="keyword">in</span> output:</span><br><span class="line">            print(<span class="string">'%s : %i'</span>%(word, count))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">saveFile</span><span class="params">()</span>:</span></span><br><span class="line">        counts = sc.textFile(sys.argv[<span class="number">1</span>])\</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>))\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x:(x, <span class="number">1</span>))\</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b:a+b)\</span><br><span class="line">            .saveAsTextFile(sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        output = counts.collect()</span><br><span class="line">        print(output)</span><br><span class="line"></span><br><span class="line">    printResult()</span><br><span class="line">    saveFile()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br></pre></td></tr></table></figure>

<h4 id="TopN"><a href="#TopN" class="headerlink" title="TopN"></a>TopN</h4><ol>
<li><p>input：</p>
<p> 文件 文件夹 后缀名</p>
</li>
<li><p>求某个维度的topn</p>
</li>
<li><p>步骤分析：</p>
<p> 文本内容的每一行根据需求提取所需要的字段：map</p>
<p> 单词 ==&gt; (单词, 1)：map</p>
<p> 所有相同的相加：reduceByKey</p>
<p> 取最多出现的次数降序：sortByKey</p>
</li>
</ol>
<h4 id="平均数"><a href="#平均数" class="headerlink" title="平均数"></a>平均数</h4><ol>
<li><p>input:</p>
<p> id age</p>
</li>
<li><p>步骤分析：</p>
<p> 读取数据取出年龄：map</p>
<p> 计算总和：reduce</p>
<p> 计算个数：count</p>
<p> 求平均</p>
</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line">findspark.init()</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        print(<span class="string">'Usage: wordcount &lt;input&gt;&lt;output&gt;'</span>, file=sys.stderr)</span><br><span class="line">        sys.exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    sc = SparkContext(conf = conf)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">()</span>:</span></span><br><span class="line">        counts = sc.textFile(sys.argv[<span class="number">1</span>])\</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>))\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))\</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>]))\</span><br><span class="line">            .sortByKey(<span class="literal">False</span>)\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>])).take(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(word, count) <span class="keyword">in</span> counts:</span><br><span class="line">            print(<span class="string">'%s : %i'</span>%(word, count))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">average</span><span class="params">()</span>:</span></span><br><span class="line">        agedata = sc.textFile(sys.argv[<span class="number">1</span>]).map(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>)[<span class="number">1</span>])</span><br><span class="line">        reducedata = agedata.map(<span class="keyword">lambda</span> age:int(age)).reduce(<span class="keyword">lambda</span> a,b:a+b)</span><br><span class="line">        countdata = agedata.count()</span><br><span class="line">        avgdata = reducedata/countdata</span><br><span class="line"></span><br><span class="line">        print(agedata.collect())</span><br><span class="line">        print(reducedata)</span><br><span class="line">        print(countdata)</span><br><span class="line">        print(avgdata)</span><br><span class="line"></span><br><span class="line">    average()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(一) · 初识</title>
    <url>/2020/02/25/Spark%E5%88%9D%E8%AF%86/</url>
    <content><![CDATA[<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>需要的环境</p>
<ul>
<li>Linux</li>
<li>Python : 3.6</li>
<li>Hadoop : 5.7以上</li>
<li>Spark : 2.3.0以上</li>
<li>ElasticSearch : 6.3.0</li>
<li>Kibana : 6.3.0</li>
<li>JDK : 1.8</li>
<li>Azkaban : 3.x</li>
</ul>
<p>掌握技巧</p>
<ul>
<li>tar解压命令</li>
<li>环境的配置</li>
</ul>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><h4 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h4><p>源码： <a href="https://github.com/apache/spark" target="_blank" rel="noopener">https://github.com/apache/spark</a></p>
<p>1) RDD是一个抽象类<br>2) 带泛型，可以支持多种类型</p>
<p>RDD: Resilient Distributed Dataset 弹性 分布式 数据集</p>
<p>Represents an<br>immutable 不可变<br>partitioned collection of elements 分区<br>that can be operated on in parallel. 并行计算</p>
<p>单机存储/计算 ==&gt; 分布式存储/计算</p>
<p>1) 数据的存储: 切割    HDFS的Block<br>2) 数据的计算: 切割(分布式并行计算)    MapReduce/Spark<br>3) 存储+计算: HDFS/S3+MapReduce/Spark</p>
<h4 id="RDD的特性"><a href="#RDD的特性" class="headerlink" title="RDD的特性"></a>RDD的特性</h4><p>Internally, each RDD is characterized by five main properties:</p>
<ul>
<li>A list of partitions</li>
<li>系列的分区/分片</li>
<li>A function for computing each split</li>
<li>操作是对每个分区的</li>
<li>A list of dependencies on other RDDs</li>
<li>RDD存在依赖关系(核心,非常重要)</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>分区策略</li>
<li>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</li>
<li>数据在哪优先把作业调度到数据所在的节点进行计算</li>
</ul>
<p><strong>五大特性</strong></p>
<ul>
<li>def compute(split: Partition, context: TaskContext): Iterator[T] 特性二</li>
<li>def getPartitions: Array[Partition] 特性一</li>
<li>def getDependencies: Seq[Dependency[_]] = deps 特性三</li>
<li>def getPreferredLocations(split: Partition): Seq[String] = Nil 特性五</li>
<li>val partitioner: Option[Partitioner] = None 特性四</li>
</ul>
<p>有两种方式创建RDD </p>
<ul>
<li><p>把一个集合转成RDD Parallelized Collections</p>
</li>
<li><p>把外部数据集Hadoop的兼容转换成RDD External Datasets</p>
</li>
</ul>
<h3 id="SparkContext-amp-SparkConf"><a href="#SparkContext-amp-SparkConf" class="headerlink" title="SparkContext&amp;SparkConf"></a>SparkContext&amp;SparkConf</h3><p>第一要务:创建SparkContext</p>
<ul>
<li><p>连接到Spark集群:local,standalone,yarn,mesos</p>
</li>
<li><p>通过SparkContext来创建RDD，广播变量到集群</p>
</li>
</ul>
<p>在创建SparkContext之前还需要创建SparkConf(优先级高于系统)</p>
<h3 id="PySpark脚本"><a href="#PySpark脚本" class="headerlink" title="PySpark脚本"></a>PySpark脚本</h3><h3 id="Spark应用程序及开发"><a href="#Spark应用程序及开发" class="headerlink" title="Spark应用程序及开发"></a>Spark应用程序及开发</h3><p>1) IDE: pycharm</p>
<p>2) 设置基本参数: python interceptor  PYTHONPATH SPARK_HOME 2zip包</p>
<p>3) 开发</p>
<p>4) 使用local进行本地测试</p>
<p>提交pyspark应用程序</p>
<pre><code>spark-submit --master local[2] --name sparktest /home/***/*.py</code></pre>]]></content>
      <categories>
        <category>Spark系列</category>
      </categories>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>初识Redis</title>
    <url>/2020/02/24/Redis%E5%88%9D%E8%AF%86/</url>
    <content><![CDATA[<p>Redis是一个开源的、基于内存的数据结构存储器，可以用作数据库、缓存和消息中间件。</p>
<h3 id="我们可以从缓存开始熟悉"><a href="#我们可以从缓存开始熟悉" class="headerlink" title="我们可以从缓存开始熟悉"></a>我们可以从缓存开始熟悉</h3><p>实现一个缓存</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// get value from cache</span></span><br><span class="line"><span class="keyword">String</span> value = <span class="built_in">map</span>.<span class="built_in">get</span>(<span class="string">"someKey"</span>);</span><br><span class="line"><span class="keyword">if</span>(null == value) &#123;</span><br><span class="line"> <span class="comment">// get value from DataBase</span></span><br><span class="line"> value = queryValueFromDB(<span class="string">"someKey"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HashMap、TreeMap这些都线程不安全，可以用HashTable或者ConcurrentHashMap。</p>
<p>不管你用什么样的Map，它的背后都是key-value的Hash表结构，目的就是为了实现O(1)复杂度的查找算法，Redis也是这样实现的，另一个常用的缓存框架Memcached也是。</p>
<p>Hash表的数据结构是怎样的呢？<br><img data-src="https://pic1.zhimg.com/80/v2-84e2b36df3700de8417fa052b3ac19b8_hd.jpg" alt="hash"></p>
<p>简单说，Hash表就是一个数组，而这个数组的元素，是一个链表。</p>
<p>为什么元素是链表？理论上，如果我们的数组可以做成无限大，那么每来一个key，我们都可以把它放到一个新的位置。但是这样很明显不可行，数组越大，占用的内存就越大。</p>
<p>所以我们需要限制数组的大小，假设是16，那么计算出key的hash值后，对16取模，得出一个0~15的数，然后放到数组对应的位置上去。</p>
<p>好，现在key1放到index为2的位置，突然又来了一个key9，刚好他也要放到index为2的位置，那咋办，总不能把人家key1给踢掉吧？所以key1的信息必须存储在一个链表结构里面，这样key9来了之后，只需要把key1所在的链表节点的next，指向key9的链表节点即可。</p>
<p>很明显，链表越长，Hash表的查询、插入、删除等操作的性能都会下降，极端情况下，如果全部元素都放到了一个链表里头，复杂度就会降为O(n)，也就和顺序查找算法无异了。（正因如此，Java8里头的HashMap在元素增长到一定程度时会从链表转成一颗红黑树，来减缓查找性能的下降）</p>
<p>怎么解决？rehash。关于rehash，这里就不细讲了，大家可以先了解一下Java HashMap的resize函数，然后再通过这篇文章：<a href="https://medium.com/@kousiknath/a-little-internal-on-redis-key-value-storage-implementation-fdf96bac7453" target="_blank" rel="noopener">A little internal on redis key value storage implementation</a> 去了解Redis的rehash算法，你会惊讶的发现Redis里头居然是两个HashTable。</p>
<h3 id="C-S架构"><a href="#C-S架构" class="headerlink" title="C/S架构"></a>C/S架构</h3><p>作为Redis用户，我们要怎样把数据放到上面提到的Hash表里呢？</p>
<p>我们可以通过Redis的命令行，当然也可以通过各种语言的Redis API，在代码里面对Hash表进行操作，所以我们可以将Redis看作是一个C/S架构，客户端是各种操作，Hash表是服务端。</p>
<p>显然，Client和Server可以是在一台机器上的，也可以不在：</p>
<p><img data-src="https://pic2.zhimg.com/80/v2-d67c538b3a759800eb8102b5eeefee01_hd.jpg" alt="client"></p>
<p><a href="http://try.redis.io/" target="_blank" rel="noopener">try redis</a>这个网站可以用来熟悉Reids的操作</p>
<p>值得一提的是，Redis的Server是单线程服务器，基于Event-Loop模式来处理Client的请求，这一点和NodeJS很相似。使用单线程的好处包括：</p>
<ul>
<li><p>不必考虑线程安全问题。很多操作都不必加锁，既简化了开发，又提高了性能；</p>
</li>
<li><p>减少线程切换损耗的时间。线程一多，CPU在线程之间切来切去是非常耗时的，单线程服务器则没有了这个烦恼；</p>
</li>
</ul>
<p>当然，单线程服务器最大的问题自然是无法充分利用多处理器。</p>
<h3 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h3><p>搭起这样一个框架，一台客户端，一台Redis缓存服务器</p>
<p>随着系统中使用Redis的客户端越来越多，会带来两个问题：</p>
<ul>
<li>Redis内存不足：随着使用Redis的客户端越来越多，Redis上的缓存数据也越来越大，而一台机器的内存毕竟是有限的，放不了那么多数据；</li>
<li>Redis吞吐量低：客户端变多了，可Redis还是只有一台，而且我们已经知道，Redis是单线程的！一台机器的带宽和处理器都是有限的，Redis自然会忙不过来，吞吐量已经不足以支撑我们越来越庞大的系统。</li>
</ul>
<p>可以通过集群的方式解决问题</p>
<p><img data-src="https://pic1.zhimg.com/80/v2-3ea442fd9cfba7ae0569f40e764dd8f0_hd.jpg" alt="集群"></p>
<p>客户端的请求会通过负载均衡算法（通常是一致性Hash），分散到各个Redis服务器上。</p>
<p>通过集群，我们实现了两个特性：</p>
<ul>
<li>扩大缓存容量；</li>
<li>提升吞吐量；</li>
</ul>
<p>解决了上面提到的两个问题。</p>
<h3 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h3><p>现在我们已经把Redis升级到了集群，真可谓效果杠杠的，可运行了一段时间后，运维又过来反馈了两个问题：</p>
<ul>
<li>数据可用性差：如果其中一台Redis挂了，那么上面全部的缓存数据都会丢失，导致原来可以从缓存中获取的请求，都去访问数据库了，数据库压力陡增。</li>
<li>数据查询缓慢：监测发现，每天有一段时间，Redis 1的访问量非常高，而且大多数请求都是去查一个相同的缓存数据，导致Redis 1非常忙碌，吞吐量不足以支撑这个高的查询负载。</li>
</ul>
<p>问题分析完，要想解决可用性问题，我们第一个想到的，就是数据库里头经常用到的Master-Slave模式，于是，我们给每一台Redis都加上了一台Slave：</p>
<p><img data-src="https://pic4.zhimg.com/80/v2-b971a5e0d88583cdb8c5c550b5e5b2ab_hd.jpg" alt="slave"></p>
<p>通过Master-Slave模式，我们又实现了两个特性：</p>
<ul>
<li>数据高可用：Master负责接收客户端的写入请求，将数据写到Master后，同步给Slave，实现数据备份。一旦Master挂了，可以将Slave提拔为Master；</li>
<li>提高查询效率：一旦Master发现自己忙不过来了，可以把一些查询请求，转发给Slave去处理，也就是Master负责读写或者只负责写，Slave负责读；</li>
</ul>
<p>为了让Master-Slave模式发挥更大的威力，我们当然可以放更多的Slave，就像这样：</p>
<p><img data-src="https://pic4.zhimg.com/80/v2-76238e772c8bb5feaa5bb20e4207cfcf_hd.jpg" alt="更多slave"></p>
<p>可这样又引发了另一个问题，那就是Master进行数据备份的工作量变大了，Slava每增加一个，Master就要多备份一次，于是又有了Master/slave chains的架构：</p>
<p><img data-src="https://pic1.zhimg.com/80/v2-eb813169598035287738730a5f53c2cc_hd.jpg" alt="chains"></p>
<p>这样最顶层的Master的备份压力就没那么大了，它只需要备份两次，然后让那它底下的那两台Slave再去和他们的Slave备份。</p>
<p>事实上，Redis内部要处理的问题还有很多：</p>
<ul>
<li>数据结构。文章一开头提到了，Redis不仅仅是数据存储器，而是数据结构存储器。那是因为Redis支持客户端直接往里面塞各种类型的数据结构，比如String、List、Set、SortedSet、Map等等。你或许会问，这很了不起吗？我自己在Java里写一个HashTable不也可以放各种数据结构？呵呵，要知道你的HashTable只能放Java对象，人家那可是支持多语言的，不管你的客户端是Java还是Python还是别的，都可以往Redis塞数据结构。这一点也是Redis和Memcached相比，非常不同的一点。当然Redis要支持数据结构存储，是以牺牲更多内存为代价的，正所谓有利必有弊。关于Redis里头的数据结构，大家可以参考：<a href="https://redis.io/topics/data-types-intro" target="_blank" rel="noopener">Redis Data Types</a></li>
<li>剔除策略。缓存数据总不能无限增长吧，总得剔除掉一些数据，好让新的缓存数据放进来吧？这就需要LRU算法了，大家可以参考：<a href="https://redis.io/topics/lru-cache" target="_blank" rel="noopener">Redis Lru Cache</a></li>
<li>负载均衡。用到了集群，就免不了需要用到负载均衡，用什么负载均衡算法？在哪里使用负载均衡？这点大家可以参考：<a href="https://redis.io/topics/partitioning" target="_blank" rel="noopener">Redis Partitioning</a></li>
<li>Presharding。如果一开始只有三台Redis服务器，后来发现需要加多一台才能满足业务需要，要怎么办？Redis提供了一种策略，叫：<a href="https://redis.io/topics/partitioning#presharding" target="_blank" rel="noopener">Presharding</a></li>
<li>数据持久化。如果我的机器突然全部断电了，我的缓存数据还能恢复吗？Redis说，相信我，可以的，不然我怎么用作数据库？去看看这个：<a href="https://redis.io/topics/persistence" target="_blank" rel="noopener">Redis Persistence</a>]</li>
<li>数据同步。这篇文章里提到了主从复制，那么Redis是怎么进行主从复制的呢？根据CAP理论，既然我们已经选择了集群，也就是P，分区容忍性，那么剩下那两个，Consistency和Availability只能选择一个了，那么Redis到底是支持最终一致性还是强一致性呢？可以参考：<a href="https://redis.io/topics/replication" target="_blank" rel="noopener">Redis Replication</a></li>
</ul>
<blockquote>
<p>参考知乎<a href="https://zhuanlan.zhihu.com/p/37055648" target="_blank" rel="noopener">Redis简明教程</a></p>
</blockquote>
]]></content>
      <categories>
        <category>Redis系列</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark安装</title>
    <url>/2020/02/10/Spark%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="下载Spark"><a href="#下载Spark" class="headerlink" title="下载Spark"></a>下载Spark</h2><p>根据地址选择下载版本</p>
<p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p>
<img data-src="/2020/02/10/Spark%E5%AE%89%E8%A3%85/sparkdownload.png" class title="spark下载版本">

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tar -zvxf spark-3.0.0-bin-hadoop2.7.tgz</span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">$ sudo gedit ~/.bashrc</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_102</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/spark-3.0.0-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> PYSPARK_DRIVER_PYTHON=/home/dc/anaconda3/bin/python</span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/home/dc/anaconda3/bin/python</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>

<h2 id="安装pyspark"><a href="#安装pyspark" class="headerlink" title="安装pyspark"></a>安装pyspark</h2><h3 id="本地安装"><a href="#本地安装" class="headerlink" title="本地安装"></a>本地安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/ubuntu/spark-2.4.0-bin-hadoop2.7/python</span><br><span class="line">$ python3 setup.py install</span><br></pre></td></tr></table></figure>

<h3 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pip3 install pyspark</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    sc = SparkContext(conf = conf)</span><br><span class="line">    </span><br><span class="line">    data = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line">    data = sc.parallelize(data)</span><br><span class="line">    rdd = data.map(<span class="keyword">lambda</span> x: x+<span class="number">1</span>)</span><br><span class="line">    print(rdd.collect())</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2, 3, 4, 5, 6, 7, 8]</span><br></pre></td></tr></table></figure>

<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="PYTHONPATH问题"><a href="#PYTHONPATH问题" class="headerlink" title="PYTHONPATH问题"></a>PYTHONPATH问题</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">py4j.protocol.Py4JError: org.apache.spark.api.python.PythonUtils.getEncryptionEnabled does not exist in the JVM</span><br></pre></td></tr></table></figure>

<p>解决方法</p>
<pre><code>sudo gedit .bashrc</code></pre><p>Please add $SPARK_HOME/python/build to PYTHONPATH:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/spark-3.0.0-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$SPARK_HOME</span>/python:<span class="variable">$SPARK_HOME</span>/python/build:<span class="variable">$PYTHONPATH</span></span><br></pre></td></tr></table></figure>

<pre><code>source .bashrc</code></pre><p>或者在 import pyspark 之前：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line">findspark.init()</span><br></pre></td></tr></table></figure>

<h3 id="java-net-ConnectException"><a href="#java-net-ConnectException" class="headerlink" title="java.net.ConnectException"></a>java.net.ConnectException</h3><p>用spark提交任务时报错java.net.ConnectException，出现这个问题可以从以下几个方面排查：</p>
<ol>
<li><p>防火墙问题</p>
<p> 防火墙问题可以禁用防火墙或者开放相应端口</p>
</li>
<li><p>端口占用问题</p>
<p> 端口占用问题的话可以netstat -nltp查看，结束占用端口的应用</p>
</li>
<li><p>namenode未正常启动</p>
<p> namenode问题的话重新启动namenode</p>
</li>
</ol>
]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>Spark</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop安装</title>
    <url>/2020/02/09/Hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>安装过java，确认可用</p>
<pre><code>java -version</code></pre><h2 id="创建haddop用户组和用户"><a href="#创建haddop用户组和用户" class="headerlink" title="创建haddop用户组和用户"></a>创建haddop用户组和用户</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo addgroup hadoop</span><br><span class="line">sudo adduser --ingroup hadoop hduser</span><br><span class="line">sudo adduser hduser sudo </span><br><span class="line">su hduser</span><br></pre></td></tr></table></figure>

<h2 id="安装和配置ssh"><a href="#安装和配置ssh" class="headerlink" title="安装和配置ssh"></a>安装和配置ssh</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>

<p>切换到hduser用户，执行如下命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -P <span class="string">''</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<h2 id="下载hadoop"><a href="#下载hadoop" class="headerlink" title="下载hadoop"></a>下载hadoop</h2><p><a href="http://mirror.bit.edu.cn/apache/hadoop/common" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/hadoop/common</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo tar -zxvf hadoop-2.7.7.tar.gz -C /opt</span><br><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">sudo mv hadoop-2.7.7 hadoop</span><br><span class="line">sudo chown -R hduser:hadoop hadoop</span><br></pre></td></tr></table></figure>

<h2 id="配置hadoop环境"><a href="#配置hadoop环境" class="headerlink" title="配置hadoop环境"></a>配置hadoop环境</h2><p>编辑以下文件</p>
<ul>
<li><p>.bashrc</p>
<p>  sudo gedit ~/.bashrc</p>
</li>
</ul>
<p>将下面的内容复制到.bashrc中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Hadoop variables</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> HADOOP_INSTALL=/opt/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_INSTALL</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_INSTALL</span>/sbin</span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br></pre></td></tr></table></figure>

<ul>
<li>hadoop-env.sh</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/hadoop/etc/hadoop &amp;&amp; sudo gedit hadoop-env.sh</span><br></pre></td></tr></table></figure>

<p>将下面的三行加入到hadoop-env.sh中，注释原来的 “export JAVA_HOME”那行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="string">"/opt/hadoop/lib/native/"</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -Djava.library.path=/opt/hadoop/lib/"</span></span><br></pre></td></tr></table></figure>

<h2 id="配置hadoop"><a href="#配置hadoop" class="headerlink" title="配置hadoop"></a>配置hadoop</h2><pre><code>cd /usr/local/hadoop/etc/hadoop</code></pre><p>编辑以下文件</p>
<ol>
<li><p>core-site.xml</p>
<p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>yarn-site.xml</p>
<p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>mapred-site.xml</p>
<pre><code>mv mapred-site.xml.template mapred-site.xml</code></pre><p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>hdfs-site.xml</p>
<pre><code>mkdir -p ~/mydata/hdfs/namenode &amp;&amp; mkdir -p ~/mydata/hdfs/datanode</code></pre><p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hduser/mydata/hdfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hduser/mydata/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="启动并查看服务"><a href="#启动并查看服务" class="headerlink" title="启动并查看服务"></a>启动并查看服务</h2><ol>
<li><p>格式化 namenode</p>
<p> 第一次启动hadoop服务之前，必须执行格式化namenode</p>
<pre><code>hdfs namenode -format</code></pre></li>
<li><p>启动服务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-dfs.sh &amp;&amp; start-yarn.sh</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">start-all.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>jps查看服务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jps</span><br><span class="line"><span class="comment"># 最好用sudo</span></span><br><span class="line">sudo jps</span><br></pre></td></tr></table></figure>

<p> 顺利的话可以看的：</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">23313 DataNode</span><br><span class="line">10421 Master</span><br><span class="line">23749 ResourceManager</span><br><span class="line">23559 SecondaryNameNode</span><br><span class="line">23112 NameNode</span><br><span class="line">24104 NodeManager</span><br><span class="line">10605 Worker</span><br><span class="line">6942 Jps</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="namenode不启动"><a href="#namenode不启动" class="headerlink" title="namenode不启动"></a>namenode不启动</h3><ol>
<li><p>先运行 </p>
<pre><code>stop-all.sh</code></pre></li>
<li><p>格式化namdenode</p>
<p> 不过在这之前要先删除原目录，即core-site.xml下配置的<name>hadoop.tmp.dir</name>所指向的目录，删除后切记要重新建立配置的空目录，然后运行</p>
<pre><code>hadoop namenode -format</code></pre><p> 请务必检查文件夹位置是否正确</p>
</li>
<li><p>运行 </p>
<pre><code>start-all.sh</code></pre></li>
</ol>
<h3 id="datanode不启动"><a href="#datanode不启动" class="headerlink" title="datanode不启动"></a>datanode不启动</h3><p>当我们使用hadoop namenode -format格式化namenode时，会在namenode数据文件夹（这个文件夹为自己配置文件中dfs.name.dir的路径）中保存一个current/VERSION文件，记录clusterID，datanode中保存的current/VERSION文件中的clustreID的值是上一次格式化保存的clusterID，这样，datanode和namenode之间的ID不一致。</p>
<ol>
<li><p>如果dfs文件夹中没有重要的数据，那么删除dfs文件夹，再重新运行下列指令： （删除节点下的dfs文件夹，为自己配置文件中dfs.name.dir的路径）</p>
</li>
<li><p>如果dfs文件中有重要的数据，那么在dfs/name目录下找到一个current/VERSION文件，记录clusterID并复制。然后dfs/data目录下找到一个current/VERSION文件，将其中clustreID的值替换成刚刚复制的clusterID的值即可；</p>
</li>
</ol>
<p><strong>注意</strong></p>
<p>每次运行结束Hadoop后，都应该关闭Hadoop</p>
<pre><code>stop-dfs.sh</code></pre><p>下次想重新运行Hadoop，不用再格式化namenode,直接启动Hadoop即可</p>
<pre><code>start-dfs.sh</code></pre>]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Java安装</title>
    <url>/2020/02/08/Java%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="安装jdk"><a href="#安装jdk" class="headerlink" title="安装jdk"></a>安装jdk</h2><h3 id="orcal-Java-Jdk"><a href="#orcal-Java-Jdk" class="headerlink" title="orcal Java Jdk"></a>orcal Java Jdk</h3><h4 id="官网下载"><a href="#官网下载" class="headerlink" title="官网下载"></a>官网下载</h4><ol>
<li><p>官网下载orcal Java Jdk</p>
<p> 下载链接  </p>
<p> <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<p> 可以根据自己的系统进行下载</p>
</li>
<li><p>进行解压</p>
<pre><code>sudo tar -zxvf jdk-linux-x64.tar.gz </code></pre><p> 解压到当前目录下，解压后可以把解压文件移动到自己想要放的目录下，使用mv命令 sudo mv jdk1.8.0_171 /usr/lib/jvm</p>
</li>
<li><p>进行配置</p>
<p> 使用全局设置方法，它是所有用户的共用的环境变量</p>
<p> 命令如下：</p>
<pre><code>$sudo gedit ~/.bashrc</code></pre><p> 然后把如下命令复制到最底部</p>
<pre><code>export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_25  
export JRE_HOME=${JAVA_HOME}/jre  
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib  
export PATH=${JAVA_HOME}/bin:$PATH
export JAVA_HOME=后面要填写自己解压后的jdk的路径</code></pre></li>
<li><p>生效~/.bashrc文件</p>
<p> 命令如下：</p>
<pre><code>$sudo source ~/.bashrc</code></pre></li>
<li><p>测试是否安装成功</p>
<p> 查看版本号是否改变</p>
<pre><code>java -version </code></pre></li>
</ol>
<h4 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h4><ol>
<li><p>安装依赖包：</p>
<pre><code>sudo apt-get install python-software-properties</code></pre></li>
<li><p>添加仓库源：</p>
<pre><code>sudo add-apt-repository ppa:webupd8team/java</code></pre></li>
<li><p>更新软件包列表：</p>
<pre><code>sudo apt-get update</code></pre></li>
<li><p>安装java JDK：</p>
<pre><code>sudo apt-get install oracle-java8-installer</code></pre></li>
</ol>
<h3 id="openjdk"><a href="#openjdk" class="headerlink" title="openjdk"></a>openjdk</h3><ol>
<li><p>更新软件包列表：</p>
<pre><code>sudo apt-get update</code></pre></li>
<li><p>安装openjdk-8-jdk：</p>
<pre><code>sudo apt-get install openjdk-8-jdk</code></pre></li>
<li><p>查看java版本，看看是否安装成功：</p>
<pre><code>java -version</code></pre></li>
</ol>
]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>教程</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/02/01/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br><span class="line">$ hexo n <span class="string">"New Post"</span> <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line">$ hexo s <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo g <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line">$ hexo d <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
<h3 id="Multi-config"><a href="#Multi-config" class="headerlink" title="Multi config"></a>Multi config</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo avgr --config .\coding_config.yml</span><br></pre></td></tr></table></figure>

<p>根据不同的config文件生成不同的网页</p>
]]></content>
      <categories>
        <category>安装教程</category>
      </categories>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
