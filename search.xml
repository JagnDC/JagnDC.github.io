<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Spark(四) · 核心进阶</title>
    <url>/2020/03/17/Spark%E6%A0%B8%E5%BF%83%E8%BF%9B%E9%98%B6/</url>
    <content><![CDATA[<h2 id="Spark-核心概念"><a href="#Spark-核心概念" class="headerlink" title="Spark 核心概念"></a>Spark 核心概念</h2><img src="/2020/03/17/Spark%E6%A0%B8%E5%BF%83%E8%BF%9B%E9%98%B6/%E6%A0%B8%E5%BF%83.png" class title="核心概念from官网">

<ol>
<li><p>Application</p>
<p> User program built on Spark. Consists of a driver program and executors on the cluster.</p>
<p> 基于Spark的应用程序 = 1 driver + executors</p>
</li>
<li><p>Driver program</p>
<p> The process running the main() function of the application and creating the SparkContext</p>
<p> 运行主函数，创建SparkContext</p>
</li>
<li><p>Cluster manager</p>
<p> An external service for acquiring resources on the cluster (e.g. standalone manager, Mesos, YARN)<br> spark-submit –master local[*]/spark://localhost:7077/yarn</p>
</li>
<li><p>Deploy mode</p>
<p> Distinguishes where the driver process runs. </p>
<p> In “cluster” mode, the framework launches the driver inside of the cluster. </p>
<p> In “client” mode, the submitter launches the driver outside of the cluster.</p>
</li>
<li><p>Worker node</p>
<p> Any node that can run application code in the cluster</p>
<ul>
<li><p>standalone</p>
<p>  slave节点 slaves配置文件</p>
</li>
<li><p>yarn</p>
<p>  nodemanager</p>
</li>
</ul>
</li>
<li><p>Executor</p>
<p> A process launched for an application on a worker node</p>
<p> runs tasks</p>
<p> keeps data in memory or disk storage across them</p>
<p> Each application has its own executors.</p>
</li>
<li><p>task</p>
<p> A unit of work that will be sent to one executor</p>
<p> 最小工作单元</p>
</li>
<li><p>Job</p>
<p> A parallel computation consisting of multiple tasks that gets spawned in response to a Spark action (e.g. save, collect); you’ll see this term used in the driver’s logs.</p>
<p> 一个action对应一个Job</p>
</li>
<li><p>Stage</p>
<p> Each job gets divided into smaller sets of tasks called stages that depend on each other </p>
<p> (similar to the map and reduce stages in MapReduce);</p>
<p> you’ll see this term used in the driver’s logs.</p>
<p> 一个stage的边界往往是从某个地方取数据开始，到shuffle的地方结束</p>
</li>
</ol>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>一个application是由driver和executors构成的</p>
<p>一个action对应一个job，一个job对应一到多个stage</p>
<p>一个stage对应一到多个task</p>
<p>task运行在executor里面</p>
<p>executors跑在worker上面的</p>
<h2 id="Spark-运行架构"><a href="#Spark-运行架构" class="headerlink" title="Spark 运行架构"></a>Spark 运行架构</h2><img src="/2020/03/17/Spark%E6%A0%B8%E5%BF%83%E8%BF%9B%E9%98%B6/components.png" class title="components">

<p>Spark applications run as independent sets of processes on a cluster, coordinated by the SparkContext object in your main program (called the driver program).</p>
<p>Spark应用程序在群集上作为独立的进程集运行，由主程序（称为驱动程序）中的SparkContext对象协调。</p>
<p>Specifically, to run on a cluster, the SparkContext can connect to several types of cluster managers (either Spark’s own standalone cluster manager, Mesos or YARN), which allocate resources across applications. Once connected, Spark acquires executors on nodes in the cluster, which are processes that run computations and store data for your application. Next, it sends your application code (defined by JAR or Python files passed to SparkContext) to the executors. Finally, SparkContext sends tasks to the executors to run.</p>
<p>具体来说，要在集群上运行，SparkContext可以连接到几种类型的集群管理器（Spark自己的独立集群管理器Mesos或YARN），它们可以在应用程序之间分配资源。连接后，Spark会在集群中的节点上获取执行程序，这些节点是运行计算并为您的应用程序存储数据的进程。接下来，它将应用程序代码（由传递给SparkContext的JAR或Python文件定义）发送给执行者。最后，SparkContext将任务发送给执行程序以运行。</p>
<ol>
<li><p>Each application gets its own executor processes, which stay up for the duration of the whole application and run tasks in multiple threads. This has the benefit of isolating applications from each other, on both the scheduling side (each driver schedules its own tasks) and executor side (tasks from different applications run in different JVMs). However, it also means that data cannot be shared across different Spark applications (instances of SparkContext) without writing it to an external storage system.</p>
<p> 线程是独立的，应用程序之间数据不共享，除非写入外部存储管理系统中</p>
</li>
<li><p>Spark is agnostic to the underlying cluster manager. As long as it can acquire executor processes, and these communicate with each other, it is relatively easy to run it even on a cluster manager that also supports other applications (e.g. Mesos/YARN).</p>
<p> 写spark应用程序的时候不需要关注程序运行在哪里，只需要和executor进行通讯</p>
</li>
<li><p>The driver program must listen for and accept incoming connections from its executors throughout its lifetime (e.g., see spark.driver.port in the network config section). As such, the driver program must be network addressable from the worker nodes.</p>
<p> driver必须要和executor进行通讯，连接是双向的，发送作业，接受心跳信息</p>
</li>
<li><p>Because the driver schedules tasks on the cluster, it should be run close to the worker nodes, preferably on the same local area network. If you’d like to send requests to the cluster remotely, it’s better to open an RPC to the driver and have it submit operations from nearby than to run a driver far away from the worker nodes.</p>
<p> driver要尽可能近的靠近worker nodes，最好是local，远程的话最好使用RPC通讯</p>
</li>
</ol>
<h2 id="Spark和Hadoop的概念区分"><a href="#Spark和Hadoop的概念区分" class="headerlink" title="Spark和Hadoop的概念区分"></a>Spark和Hadoop的概念区分</h2><p>Hadoop</p>
<ol>
<li>一个MR程序 = 一个Job</li>
<li>一个Job = 1-n个task(Map/Reduce)</li>
<li>一个task对应于一个进程</li>
<li>task运行时相当于开启了进程，执行完毕后销毁进程，对于多个task来说，开销是比较大的(即使能够JVM共享)</li>
</ol>
<p>Spark</p>
<ol>
<li>Application = Driver (main方法中创建SparkContext + Executors)</li>
<li>一个Application = 0-n个Job</li>
<li>一个Job = 一个Action</li>
<li>一个Job = 1-n个Stage</li>
<li>一个Stage = 1-n个Task</li>
<li>一个Task对应一个线程，多个Task可以以并行的方式运行在一个Executor进程中</li>
</ol>
<h2 id="Spark-Cache详解"><a href="#Spark-Cache详解" class="headerlink" title="Spark Cache详解"></a>Spark Cache详解</h2><p>rdd.cache() : StorageLevel</p>
<p>cache:lazy 只有遇到action才会提交运行</p>
<p>不缓存的场景中，有多少次action就会读取多少次disk</p>
<p>如果一个RDD在后续的计算中会使用到，建议使用cache缓存</p>
<p>cache底层调用persist方法，传入的参数是：StorageLevel.MEMORY_ONLY</p>
<p>cache = persist</p>
<p>unpersist：立即执行</p>
<p><strong>如何选择</strong></p>
<p>Which Storage Level to Choose?<br>Spark’s storage levels are meant to provide different trade-offs between memory usage and CPU efficiency. We recommend going through the following process to select one:</p>
<ol>
<li><p>If your RDDs fit comfortably with the default storage level (MEMORY_ONLY), leave them that way. This is the most CPU-efficient option, allowing operations on the RDDs to run as fast as possible.</p>
</li>
<li><p>If not, try using MEMORY_ONLY_SER and selecting a fast serialization library to make the objects much more space-efficient, but still reasonably fast to access. (Java and Scala)</p>
</li>
<li><p>Don’t spill to disk unless the functions that computed your datasets are expensive, or they filter a large amount of the data. Otherwise, recomputing a partition may be as fast as reading it from disk.</p>
</li>
<li><p>Use the replicated storage levels if you want fast fault recovery (e.g. if using Spark to serve requests from a web application). All the storage levels provide full fault tolerance by recomputing lost data, but the replicated ones let you continue running tasks on the RDD without waiting to recompute a lost partition.</p>
</li>
</ol>
<p>Spark的存储级别旨在在内存使用量和CPU效率之间提供不同的权衡。我们建议通过以下过程选择一个：如果您的RDD适合默认存储级别</p>
<ol>
<li><p>（MEMORY_ONLY），请保持这种状态。这是CPU效率最高的选项，允许RDD上的操作尽可能快地运行。</p>
</li>
<li><p>如果不是，请尝试使用MEMORY_ONLY_SER并选择一个快速的序列化库，以使对象的空间效率更高，但访问速度仍然相当快。（Java和Scala）</p>
</li>
<li><p>除非用于计算数据集的函数很昂贵，否则它们会过滤掉大量数据。否则，重新计算分区可能与从磁盘读取分区一样快。</p>
</li>
<li><p>如果要快速恢复故障（例如，如果使用Spark来处理来自Web应用程序的请求），请使用副本存储。所有存储级别都通过重新计算丢失的数据来提供完全的容错能力，但是副本存储使您可以继续在RDD上运行任务，而不必等待重新计算丢失的分区。</p>
</li>
</ol>
<h2 id="Spark-Lineage详解"><a href="#Spark-Lineage详解" class="headerlink" title="Spark Lineage详解"></a>Spark Lineage详解</h2><p>RDD之间的依赖关系就是Lineage</p>
<p>当计算过程中RDD的其中某个partition出现问题就可以根据Lineage从上个RDD中计算出来</p>
<h2 id="Spark-Dependency详解"><a href="#Spark-Dependency详解" class="headerlink" title="Spark Dependency详解"></a>Spark Dependency详解</h2><ol>
<li><p>窄依赖 Narrow (pipeline-able)</p>
<p> 一个父RDD的partition最多被子RDD的某个partition使用一次</p>
</li>
<li><p>宽依赖 Wide (shuffle)</p>
<p> 一个父RDD的partition会被子RDD的partition使用多次，有shuffle</p>
</li>
</ol>
<p>n个shuffle会对应n+1个stage</p>
]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(三) · 运行模式</title>
    <url>/2020/03/13/Spark%E8%BF%90%E8%A1%8C%E6%A8%A1%E5%BC%8F/</url>
    <content><![CDATA[<ol>
<li><p>Local模式：</p>
<p> 适用于开发和本地测试环境</p>
<p> pyspark/spark-submit</p>
<p> –master</p>
<p> –name</p>
<p> –py-files</p>
<pre><code>./spark-submit --master local[*] --name spark-local /xxx/xx.py (argv)</code></pre></li>
<li><p>standalone</p>
<p> spark自带的部署方法</p>
<p> hdfs : NameNode DataNode<br> yarn : ResourceManager NodeManager</p>
<p> master, worker</p>
<pre><code>$SPARK_HOME/conf/slaves
dxy409
...
如果有多台机器则在后面添加名字</code></pre><p> 启动spark集群</p>
<pre><code>$SPARK_HOME/sbin/start-all.sh
ps:在spark-env.sh中添加JAVA_HOME</code></pre><p> 检测：</p>
<p> jps:Master和Worker进程</p>
<p> webui:8080</p>
<pre><code>./spark-submit --master spark://192.168.0.7:7077 --name spark-standalone /xxx/xx.py (argv)</code></pre><p> 如果使用standalone模式，且节点数大于1，如果使用本地文件，必须保证每个节点都有文件</p>
</li>
</ol>
<ol start="3">
<li><p>yarn</p>
<p> 生产环境中70%使用的方法</p>
<p> spark作业客户端，提交作业到yarn上执行</p>
<p> yarn vs standalone</p>
<p> yarn:只需要一个节点即可提交作业，不需要spark集群，不需要启动master和worker</p>
<p> standalone:spark集群的每个节点都需要部署spark，需要启动spark集群，需要master和worker</p>
<pre><code>./spark-submit --master yarn --deploy mode --name spark-yarn /xxx/xx.py (argv)</code></pre><p> When running with master ‘yarn’ either HADOOP_CONF_DIR or YARN_CONF_DIR must be set in the environment.</p>
<ul>
<li><p>在spark-env.sh添加 HADOOP_CONF_DIR={hadoop的位置}</p>
<p>yarn支持client和cluster模式：区别在于driver运行在哪里</p>
</li>
<li><p>cluster：提交完作业就可以断开了，因为driver运行在application master上的</p>
</li>
<li><p>client：提交作业的进程不能禁止，否则driver就挂了</p>
<p>pyspark/spark-shell/spark-sql：交互式运行程序 client</p>
<p>查看已经完成运行的yarn的日志：</p>
<p>  yarn logs -applicationId <applicationId></applicationId></p>
</li>
</ul>
</li>
</ol>
]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>SQL (一) · 基础学习</title>
    <url>/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>在我们的日常工作中，使用的是类似 MySQL、Oracle 这种的数据库管理系统，实际上这些数据库管理系统都遵循 SQL 语言，这就意味着，我们在使用这些数据库的时候，都是通过 SQL 语言与它们打交道。所以对于从事编程或者互联网行业的人来说，最具有中台能力的语言便是 SQL 语言。自从 SQL 加入了 TIOBE 编程语言排行榜，就一直保持在 Top 10。</p>
<h3 id="SQL-语言按照功能划分成以下的-4-个部分："><a href="#SQL-语言按照功能划分成以下的-4-个部分：" class="headerlink" title="SQL 语言按照功能划分成以下的 4 个部分："></a>SQL 语言按照功能划分成以下的 4 个部分：</h3><ul>
<li><p>DDL，英文叫做 Data Definition Language，也就是数据定义语言，它用来定义我们的数据库对象，包括数据库、数据表和列。通过使用 DDL，我们可以创建，删除和修改数据库和表结构。</p>
</li>
<li><p>DML，英文叫做 Data Manipulation Language，数据操作语言，我们用它操作和数据库相关的记录，比如增加、删除、修改数据表中的记录。</p>
</li>
<li><p>DCL，英文叫做 Data Control Language，数据控制语言，我们用它来定义访问权限和安全级别。</p>
</li>
<li><p>DQL，英文叫做 Data Query Language，数据查询语言，我们用它查询想要的记录，它是 SQL 语言的重中之重。在实际的业务中，我们绝大多数情况下都是在和查询打交道，因此学会编写正确且高效的查询语句，是学习的重点。</p>
<h2 id="初步使用"><a href="#初步使用" class="headerlink" title="初步使用"></a>初步使用</h2><h3 id="大小写问题"><a href="#大小写问题" class="headerlink" title="大小写问题"></a>大小写问题</h3><ul>
<li>表名、表别名、字段名、字段别名等都小写；</li>
<li>SQL 保留字、函数名、绑定变量等都大写;</li>
<li>此外在数据表的字段名推荐采用下划线命名。</li>
</ul>
<h3 id="DB、DBS-和-DBMS-的区别是什么"><a href="#DB、DBS-和-DBMS-的区别是什么" class="headerlink" title="DB、DBS 和 DBMS 的区别是什么"></a>DB、DBS 和 DBMS 的区别是什么</h3><p>说到 DBMS，有一些概念你需要了解。</p>
</li>
<li><p>DBMS 的英文全称是 DataBase Management System，数据库管理系统，实际上它可以对多个数据库进行管理，所以你可以理解为 DBMS = 多个数据库（DB） + 管理程序。</p>
</li>
<li><p>DB 的英文是 DataBase，也就是数据库。数据库是存储数据的集合，你可以把它理解为多个数据表。</p>
</li>
<li><p>DBS 的英文是 DataBase System，数据库系统。它是更大的概念，包括了数据库、数据库管理系统以及数据库管理人员 DBA。</p>
</li>
</ul>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/DBMS.png" class title="DBMS">


<h3 id="MySQL-中的-SQL-是如何执行的"><a href="#MySQL-中的-SQL-是如何执行的" class="headerlink" title="MySQL 中的 SQL 是如何执行的"></a>MySQL 中的 SQL 是如何执行的</h3><p> 首先 MySQL 是典型的 C/S 架构，即 Client/Server 架构，服务器端程序使用的 mysqld。整体的 MySQL 流程如下图所示：</p>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/MySQL.png" class title="MySQL">


<p> <strong>MySQL 由三层组成：</strong></p>
<ul>
<li>连接层：客户端和服务器端建立连接，客户端发送 SQL 至服务器端；</li>
<li>SQL 层：对 SQL 语句进行查询处理；</li>
<li>存储引擎层：与数据库文件打交道，负责数据的存储和读取。</li>
</ul>
<p> <strong>SQL层的结构：</strong></p>
<ul>
<li>查询缓存：Server 如果在查询缓存中发现了这条 SQL 语句，就会直接将结果返回给客户端；如果没有，就进入到解析器阶段。需要说明的是，因为查询缓存往往效率不高，所以在 MySQL8.0 之后就抛弃了这个功能。</li>
<li>解析器：在解析器中对 SQL 语句进行语法分析、语义分析。</li>
<li>优化器：在优化器中会确定 SQL 语句的执行路径，比如是根据全表检索，还是根据索引来检索等。</li>
<li>执行器：在执行之前需要判断该用户是否具备权限，如果具备权限就执行 SQL 查询并返回结果。在 MySQL8.0 以下的版本，如果设置了查询缓存，这时会将查询结果进行缓存。</li>
</ul>
<p> <strong>SQL 语句在 MySQL 中的流程是：</strong></p>
<p> SQL 语句→缓存查询→解析器→优化器→执行器</p>
<p>MySQL 的存储引擎采用了插件的形式，每个存储引擎都面向一种特定的数据库应用环境。同时开源的 MySQL 还允许开发人员设置自己的存储引擎，下面是一些常见的存储引擎：</p>
<ul>
<li>InnoDB 存储引擎：它是 MySQL 5.5 版本之后默认的存储引擎，最大的特点是支持事务、行级锁定、外键约束等。</li>
<li>MyISAM 存储引擎：在 MySQL 5.5 版本之前是默认的存储引擎，不支持事务，也不支持外键，最大的特点是速度快，占用资源少。</li>
<li>Memory 存储引擎：使用系统内存作为存储介质，以便得到更快的响应速度。不过如果 mysqld 进程崩溃，则会导致所有的数据丢失，因此我们只有当数据是临时的情况下才使用 Memory 存储引擎。</li>
<li>NDB 存储引擎：也叫做 NDB Cluster 存储引擎，主要用于 MySQL Cluster 分布式集群环境，类似于 Oracle 的 RAC 集群。</li>
<li>Archive 存储引擎：它有很好的压缩机制，用于文件归档，在请求写入时会进行压缩，所以也经常用来做仓库。</li>
</ul>
<p> 数据库的设计在于表的设计，而在 MySQL 中每个表的设计都可以采用不同的存储引擎，我们可以根据实际的数据处理需要来选择存储引擎，这也是 MySQL 的强大之处。</p>
<p>WHERE 子句中同时出现 AND 和 OR 操作符的时候，需要考虑到执行的先后顺序，也就是两个操作符执行的优先级。一般来说 () 优先级最高，其次优先级是 AND，然后是 OR。</p>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/WHERE.png" class title="WHERE">


<h2 id="常见SQL函数"><a href="#常见SQL函数" class="headerlink" title="常见SQL函数"></a>常见SQL函数</h2><ol>
<li>算术函数<ul>
<li>ABS() 绝对值</li>
<li>MOD() 取余</li>
<li>ROUND() 四舍五入</li>
</ul>
</li>
<li>字符串函数<ul>
<li>CONCAT() 拼接</li>
<li>LENGTH() 字段长度 汉字算三个 数字字母为一</li>
<li>CHAR_LENGTH() 字段长度 汉字数字字母都为一个</li>
<li>LOWER() 转小写</li>
<li>UPPER() 转大写</li>
<li>REPLACE() 替换</li>
<li>SUBSTRING() 截取</li>
</ul>
</li>
<li>日期函数<ul>
<li>CURRENT_DATE() 当前日期</li>
<li>CURRENT_TIME() 当前时间无日期</li>
<li>CURRENT_TIMESTAMP() 日期加时间</li>
<li>EXTRACT() 抽取年月日</li>
<li>DATE() YEAR() MONTH() DAY() HOUR() MINUTE() SECOND() 时间的各个部分 </li>
</ul>
</li>
<li>转换函数<ul>
<li>CAST() 数据类型转换</li>
<li>COALESCE 返回第一个非空值</li>
</ul>
</li>
<li>聚集函数<ul>
<li>COUNT() 总行数</li>
<li>MAX() MIN() 最大最小值</li>
<li>SUM() 求和</li>
<li>AVG() 平均值</li>
<li>DISTINCT() 取唯一</li>
</ul>
</li>
</ol>
<p>在 SQL 中，你还是要确定大小写的规范，因为在 Linux 和 Windows 环境下，可能会遇到不同的大小写问题。</p>
<p>比如 MySQL 在 Linux 的环境下，数据库名、表名、变量名是严格区分大小写的，而字段名是忽略大小写的。</p>
<p>而 MySQL 在 Windows 的环境下全部不区分大小写。</p>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/SQL.png" class title="SQL">

<h2 id="HAVING"><a href="#HAVING" class="headerlink" title="HAVING"></a>HAVING</h2><p>对于分组的筛选，我们一定要用 HAVING，而不是 WHERE。另外，HAVING 支持所有 WHERE 的操作，因此所有需要 WHERE 子句实现的功能，都可以使用 HAVING 对分组进行筛选。</p>
<p>用 WHERE 进行数据量的过滤，用 GROUP BY 进行分组，用 HAVING 进行分组过滤，用 ORDER BY 进行排序。</p>
<p> <strong>要记住，在 SELECT 查询中，关键字的顺序是不能颠倒的，它们的顺序是：</strong></p>
<pre><code>SELECT ... FROM ... WHERE ... GROUP BY ... HAVING ... ORDER BY ...</code></pre><h2 id="关联子查询，非关联子查询"><a href="#关联子查询，非关联子查询" class="headerlink" title="关联子查询，非关联子查询"></a>关联子查询，非关联子查询</h2><p>子查询虽然是一种嵌套查询的形式，不过我们依然可以依据子查询是否执行多次，从而将子查询划分为关联子查询和非关联子查询。</p>
<ul>
<li><p>子查询从数据表中查询了数据结果，如果这个数据结果只执行一次，然后这个数据结果作为主查询的条件进行执行，那么这样的子查询叫做非关联子查询。</p>
</li>
<li><p>同样，如果子查询需要执行多次，即采用循环的方式，先从外部查询开始，每次都传入子查询进行查询，然后再将结果反馈给外部，这种嵌套的执行方式就称为关联子查询。</p>
</li>
</ul>
<p> <strong>IN 和 EXIST</strong></p>
<pre><code>SELECT * FROM A WHERE cc IN (SELECT cc FROM B)

SELECT * FROM A WHERE EXIST (SELECT cc FROM B WHERE B.cc=A.cc)</code></pre><p>如果表 A 比表 B 大，那么 IN 子查询的效率要比 EXIST 子查询效率高，因为这时 B 表中如果对 cc 列进行了索引，那么 IN 子查询的效率就会比较高。</p>
<p>同样，如果表 A 比表 B 小，那么使用 EXISTS 子查询效率会更高，因为我们可以使用到 A 表中对 cc 列的索引，而不用从 B 中进行 cc 列的查询。</p>
<p> <strong>ANY、ALL 关键字必须与一个比较操作符一起使用</strong></p>
<p> SQL 中，子查询的使用大大增强了 SELECT 查询的能力，因为很多时候查询需要从结果集中获取数据，或者需要从同一个表中先计算得出一个数据结果，然后与这个数据结果（可能是某个标量，也可能是某个集合）进行比较。</p>
 <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%AD%90%E6%9F%A5%E8%AF%A2.png" class title="子查询">

<h2 id="SQL标准"><a href="#SQL标准" class="headerlink" title="SQL标准"></a>SQL标准</h2><p>  SQL 的英文全称叫做 Structured Query Language，它有一个很强大的功能，就是能在各个数据表之间进行连接查询（Query）。这是因为 SQL 是建立在关系型数据库基础上的一种语言。关系型数据库的典型数据结构就是数据表，这些数据表的组成都是结构化的（Structured）。你可以把关系模型理解成一个二维表格模型，这个二维表格是由行（row）和列（column）组成的。每一个行（row）就是一条数据，每一列（column）就是数据在某一维度的属性。</p>
<p>  正是因为在数据库中，表的组成是基于关系模型的，所以一个表就是一个关系。一个数据库中可以包括多个表，也就是存在多种数据之间的关系。而我们之所以能使用 SQL 语言对各个数据表进行复杂查询，核心就在于连接，它可以用一条 SELECT 语句在多张表之间进行查询。你也可以理解为，关系型数据库的核心之一就是连接。</p>
<p>  <strong>SQL 有两个主要的标准，分别是 SQL92 和 SQL99</strong></p>
<h3 id="笛卡尔积"><a href="#笛卡尔积" class="headerlink" title="笛卡尔积"></a>笛卡尔积</h3><p>  笛卡尔乘积是一个数学运算。假设我有两个集合 X 和 Y，那么 X 和 Y 的笛卡尔积就是 X 和 Y 的所有可能组合，也就是第一个对象来自于 X，第二个对象来自于 Y 的所有可能。</p>
<p>  笛卡尔积也称为交叉连接，英文是 CROSS JOIN，它的作用就是可以把任意表进行连接，即使这两张表不相关。但我们通常进行连接还是需要筛选的，因此你需要在连接后面加上 WHERE 子句，也就是作为过滤条件对连接数据进行筛选。比如后面要讲到的等值连接。</p>
<h3 id="等值连接"><a href="#等值连接" class="headerlink" title="等值连接"></a>等值连接</h3><p>  两张表的等值连接就是用两张表中都存在的列进行连接。我们也可以对多张表进行等值连接。</p>
<h3 id="非等值连接"><a href="#非等值连接" class="headerlink" title="非等值连接"></a>非等值连接</h3><p>  当我们进行多表查询的时候，如果连接多个表的条件是等号时，就是等值连接，其他的运算符连接就是非等值查询。</p>
<h3 id="外连接"><a href="#外连接" class="headerlink" title="外连接"></a>外连接</h3><p>  除了查询满足条件的记录以外，外连接还可以查询某一方不满足条件的记录。两张表的外连接，会有一张是主表，另一张是从表。如果是多张表的外连接，那么第一张表是主表，即显示全部的行，而第剩下的表则显示对应连接的信息。在 SQL92 中采用（+）代表从表所在的位置，而且在 SQL92 中，只有左外连接和右外连接，没有全外连接。</p>
<p>  <strong>左右外连接</strong></p>
<p>  就是指左边的表是主表，需要显示左边表的全部行，而右侧的表是从表，（+）表示哪个是从表。</p>
<pre><code>SQL：SELECT * FROM player, team where player.team_id = team.team_id(+)</code></pre><p>  相当于 SQL99 中的：</p>
<pre><code>SQL：SELECT * FROM player LEFT JOIN team on player.team_id = team.team_id</code></pre><p>  右外连接同理</p>
<h3 id="自连接"><a href="#自连接" class="headerlink" title="自连接"></a>自连接</h3><p>  自连接可以对多个表进行操作，也可以对同一个表进行操作。也就是说查询条件使用了当前表的字段。</p>
  <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/SQL%E6%A0%87%E5%87%86.png" class title="SQL标准">

<h2 id="SQL99与SQL92在连接上的区别"><a href="#SQL99与SQL92在连接上的区别" class="headerlink" title="SQL99与SQL92在连接上的区别"></a>SQL99与SQL92在连接上的区别</h2><h3 id="交叉连接"><a href="#交叉连接" class="headerlink" title="交叉连接"></a>交叉连接</h3><p>  交叉连接实际上就是 SQL92 中的笛卡尔乘积，只是这里我们采用的是 CROSS JOIN。</p>
<pre><code>SQL: SELECT * FROM player CROSS JOIN team</code></pre><h3 id="自然连接"><a href="#自然连接" class="headerlink" title="自然连接"></a>自然连接</h3><p>  可以把自然连接理解为 SQL92 中的等值连接。它会帮你自动查询两张连接表中所有相同的字段，然后进行等值连接。</p>
<p>  SQL92：<br>    SELECT player_id, a.team_id, player_name, height, team_name FROM player as a, team as b WHERE a.team_id = b.team_id</p>
<p>  SQL99：<br>    SELECT player_id, team_id, player_name, height, team_name FROM player NATURAL JOIN team </p>
<h3 id="ON-连接"><a href="#ON-连接" class="headerlink" title="ON 连接"></a>ON 连接</h3><p>  ON 连接用来指定我们想要的连接条件，针对上面的例子，它同样可以帮助我们实现自然连接的功能：</p>
<pre><code>SELECT player_id, player.team_id, player_name, height, team_name FROM player JOIN team ON player.team_id = team.team_id</code></pre><p>  相当于是用 ON 进行了 team_id 字段的等值连接。</p>
<p>  一般来说在 SQL99 中，我们需要连接的表会采用 JOIN 进行连接，ON 指定了连接条件，后面可以是等值连接，也可以采用非等值连接。</p>
<h3 id="USING-连接"><a href="#USING-连接" class="headerlink" title="USING 连接"></a>USING 连接</h3><p>  当我们进行连接的时候，可以用 USING 指定数据表里的同名字段进行等值连接。比如：</p>
<pre><code>SELECT player_id, team_id, player_name, height, team_name FROM player JOIN team USING(team_id)</code></pre><p>  与自然连接 NATURAL JOIN 不同的是，USING 指定了具体的相同的字段名称，你需要在 USING 的括号 () 中填入要指定的同名字段。</p>
<h3 id="外连接-1"><a href="#外连接-1" class="headerlink" title="外连接"></a>外连接</h3><p>  SQL99 的外连接包括了三种形式：</p>
<ul>
<li><p>左外连接：LEFT JOIN 或 LEFT OUTER JOIN</p>
</li>
<li><p>右外连接：RIGHT JOIN 或 RIGHT OUTER JOIN</p>
</li>
<li><p>全外连接：FULL JOIN 或 FULL OUTER JOIN</p>
<p>全外连接实际上就是左外连接和右外连接的结合。在这三种外连接中，我们一般省略 OUTER 不写。</p>
<p>MySQL 不支持全外连接，否则的话全外连接会返回左表和右表中的所有行。当表之间有匹配的行，会显示内连接的结果。当某行在另一个表中没有匹配时，那么会把另一个表中选择的列显示为空值。</p>
<p>全外连接的结果 = 左右表匹配的数据 + 左表没有匹配到的数据 + 右表没有匹配到的数据。</p>
<h3 id="SQL99-和-SQL92-的区别"><a href="#SQL99-和-SQL92-的区别" class="headerlink" title="SQL99 和 SQL92 的区别"></a>SQL99 和 SQL92 的区别</h3><p>在 SQL92 中进行查询时，会把所有需要连接的表都放到 FROM 之后，然后在 WHERE 中写明连接的条件。而 SQL99 在这方面更灵活，它不需要一次性把所有需要连接的表都放到 FROM 之后，而是采用 JOIN 的方式，每次连接一张表，可以多次使用 JOIN 进行连接。</p>
<p>建议多表连接使用 SQL99 标准，因为层次性更强，可读性更强，比如：</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> ...</span><br><span class="line"><span class="keyword">FROM</span> table1</span><br><span class="line">    <span class="keyword">JOIN</span> table2 <span class="keyword">ON</span> table1 和 table2 的连接条件</span><br><span class="line">        <span class="keyword">JOIN</span> table3 <span class="keyword">ON</span> table2 和 table3 的连接条件</span><br></pre></td></tr></table></figure>

<p>  SQL99 采用的这种嵌套结构非常清爽，即使再多的表进行连接也都清晰可见。如果你采用 SQL92，可读性就会大打折扣。</p>
<p>  最后一点就是，SQL99 在 SQL92 的基础上提供了一些特殊语法，比如 NATURAL JOIN 和 JOIN USING。它们在实际中是比较常用的，省略了 ON 后面的等值条件判断，让 SQL 语句更加简洁。</p>
  <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/SQL99.png" class title="SQL99">

<h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><p>  视图，也就是虚拟表</p>
  <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%BA%94%E7%94%A8%E6%9F%A5%E8%AF%A2.jpg" class title="应用查询">

<h3 id="创建视图-CREATE-VIEW"><a href="#创建视图-CREATE-VIEW" class="headerlink" title="创建视图 CREATE VIEW"></a>创建视图 CREATE VIEW</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> view_name <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> column1, column2</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">table</span></span><br><span class="line"><span class="keyword">WHERE</span> condition</span><br></pre></td></tr></table></figure>

<p>  实际上就是我们在 SQL 查询语句的基础上封装了视图 VIEW，这样就会基于 SQL 语句的结果集形成一张虚拟表。其中 view_name 为视图名称，column1、column2 代表列名，condition 代表查询过滤条件。</p>
<h3 id="修改视图-ALTER-VIEW"><a href="#修改视图-ALTER-VIEW" class="headerlink" title="修改视图 ALTER VIEW"></a>修改视图 ALTER VIEW</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> view_name <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> column1, column2</span><br><span class="line"><span class="keyword">FROM</span> <span class="keyword">table</span></span><br><span class="line"><span class="keyword">WHERE</span> condition</span><br></pre></td></tr></table></figure>

<h3 id="删除视图-DROP-VIEW"><a href="#删除视图-DROP-VIEW" class="headerlink" title="删除视图 DROP VIEW"></a>删除视图 DROP VIEW</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> view_name</span><br></pre></td></tr></table></figure>

<h3 id="视图简化SQL操作"><a href="#视图简化SQL操作" class="headerlink" title="视图简化SQL操作"></a>视图简化SQL操作</h3><ul>
<li><p>完成复杂的连接</p>
</li>
<li><p>对数据进行格式化</p>
</li>
<li><p>使用视图计算字段</p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3></li>
<li><p>安全性：虚拟表是基于底层数据表的，我们在使用视图时，一般不会轻易通过视图对底层数据进行修改。</p>
</li>
<li><p>简单清晰：视图是对 SQL 查询的封装，它可以将原本复杂的 SQL 查询简化，在编写好查询之后，我们就可以直接重用它而不必要知道基本的查询细节。</p>
</li>
<li><p>临时表是真实存在的数据表，不过它不用于长期存放数据，只为当前连接存在，关闭连接后，临时表就会自动释放。</p>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E8%A7%86%E5%9B%BE.png" class title="视图">

<h2 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h2><p>SQL 的存储过程是 SQL 中另一个重要应用，和视图一样，都是对 SQL 代码进行封装，可以反复利用。它和视图有着同样的优点，清晰、安全，还可以减少网络传输量。</p>
<p>存储过程的英文是 Stored Procedure。它的思想很简单，就是 SQL 语句的封装。一旦存储过程被创建出来，使用它就像使用函数一样简单，我们直接通过调用存储过程名即可。</p>
<h3 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h3><p>使用 CREATE PROCEDURE 创建一个存储过程，后面是存储过程的名称，以及过程所带的参数，可以包括输入参数和输出参数。最后由 BEGIN 和 END 来定义我们所要执行的语句块。</p>
</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> 存储过程名称 ([参数列表])</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">    需要执行的语句</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  删除已经创建的存储过程，使用的是 DROP PROCEDURE。如果要更新存储过程，我们需要使用 ALTER PROCEDURE。</p>
<p>  例子：<br>  累加运算，计算 1+2+…+n 等于多少</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`add_num`</span>(<span class="keyword">IN</span> n <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">       <span class="keyword">DECLARE</span> i <span class="built_in">INT</span>;</span><br><span class="line">       <span class="keyword">DECLARE</span> <span class="keyword">sum</span> <span class="built_in">INT</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">SET</span> i = <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="number">0</span>;</span><br><span class="line">       WHILE i &lt;= n DO</span><br><span class="line">              <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="keyword">sum</span> + i;</span><br><span class="line">              <span class="keyword">SET</span> i = i +<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">END</span> <span class="keyword">WHILE</span>;</span><br><span class="line">       <span class="keyword">SELECT</span> <span class="keyword">sum</span>;</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  如果你使用 Navicat 这个工具来管理 MySQL 执行存储过程，那么直接执行上面这段代码就可以了。如果用的是 MySQL，你还需要用 DELIMITER 来临时定义新的结束符。因为默认情况下 SQL 采用（；）作为结束符，这样当存储过程中的每一句 SQL 结束之后，采用（；）作为结束符，就相当于告诉 SQL 可以执行这一句了。但是存储过程是一个整体，我们不希望 SQL 逐条执行，而是采用存储过程整段执行的方式，因此我们就需要临时定义新的 DELIMITER，新的结束符可以用（//）或者（$$）。如果你用的是 MySQL，那么上面这段代码，应该写成下面这样：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">DELIMITER //</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`add_num`</span>(<span class="keyword">IN</span> n <span class="built_in">INT</span>)</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">       <span class="keyword">DECLARE</span> i <span class="built_in">INT</span>;</span><br><span class="line">       <span class="keyword">DECLARE</span> <span class="keyword">sum</span> <span class="built_in">INT</span>;</span><br><span class="line">       </span><br><span class="line">       <span class="keyword">SET</span> i = <span class="number">1</span>;</span><br><span class="line">       <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="number">0</span>;</span><br><span class="line">       WHILE i &lt;= n DO</span><br><span class="line">              <span class="keyword">SET</span> <span class="keyword">sum</span> = <span class="keyword">sum</span> + i;</span><br><span class="line">              <span class="keyword">SET</span> i = i +<span class="number">1</span>;</span><br><span class="line">       <span class="keyword">END</span> <span class="keyword">WHILE</span>;</span><br><span class="line">       <span class="keyword">SELECT</span> <span class="keyword">sum</span>;</span><br><span class="line"><span class="keyword">END</span> //</span><br><span class="line">DELIMITER ;</span><br></pre></td></tr></table></figure>

<h3 id="存储过程的三种参数类型"><a href="#存储过程的三种参数类型" class="headerlink" title="存储过程的三种参数类型"></a>存储过程的三种参数类型</h3>  <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B%E7%B1%BB%E5%9E%8B.png" class title="存储过程类型">

<p>  例子：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`get_hero_scores`</span>(</span><br><span class="line">       <span class="keyword">OUT</span> max_max_hp <span class="built_in">FLOAT</span>,</span><br><span class="line">       <span class="keyword">OUT</span> min_max_mp <span class="built_in">FLOAT</span>,</span><br><span class="line">       <span class="keyword">OUT</span> avg_max_attack <span class="built_in">FLOAT</span>,  </span><br><span class="line">       s <span class="built_in">VARCHAR</span>(<span class="number">255</span>)</span><br><span class="line">       )</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">       <span class="keyword">SELECT</span> <span class="keyword">MAX</span>(hp_max), <span class="keyword">MIN</span>(mp_max), <span class="keyword">AVG</span>(attack_max) <span class="keyword">FROM</span> heros <span class="keyword">WHERE</span> role_main = s <span class="keyword">INTO</span> max_max_hp, min_max_mp, avg_max_attack;</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  调用存储过程：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CALL</span> get_hero_scores(@max_max_hp, @min_max_mp, @avg_max_attack, <span class="string">'战士'</span>);</span><br><span class="line"><span class="keyword">SELECT</span> @max_max_hp, @min_max_mp, @avg_max_attack;</span><br></pre></td></tr></table></figure>

<h3 id="流控制语句"><a href="#流控制语句" class="headerlink" title="流控制语句"></a>流控制语句</h3><ol>
<li>BEGIN…END：BEGIN…END 中间包含了多个语句，每个语句都以（;）号为结束符。</li>
<li>DECLARE：DECLARE 用来声明变量，使用的位置在于 BEGIN…END 语句中间，而且需要在其他语句使用之前进行变量的声明。</li>
<li>SET：赋值语句，用于对变量进行赋值。</li>
<li>SELECT…INTO：把从数据表中查询的结果存放到变量中，也就是为变量赋值。</li>
<li>IF…THEN…ENDIF：条件判断语句，我们还可以在 IF…THEN…ENDIF 中使用 ELSE 和 ELSEIF 来进行条件判断。</li>
<li>CASE：CASE 语句用于多条件的分支判断，使用的语法是下面这样的。</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CASE </span><br><span class="line">	WHEN expression1 THEN ...</span><br><span class="line">	WHEN expression2 THEN ...</span><br><span class="line">	...</span><br><span class="line">    ELSE </span><br><span class="line">    <span class="comment">--ELSE 语句可以加，也可以不加。加的话代表的所有条件都不满足时采用的方式。</span></span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>
<ol start="7">
<li><p>LOOP、LEAVE 和 ITERATE：LOOP 是循环语句，使用 LEAVE 可以跳出循环，使用 ITERATE 则可以进入下一次循环。如果你有面向过程的编程语言的使用经验，你可以把 LEAVE 理解为 BREAK，把 ITERATE 理解为 CONTINUE。</p>
</li>
<li><p>REPEAT…UNTIL…END REPEAT：这是一个循环语句，首先会执行一次循环，然后在 UNTIL 中进行表达式的判断，如果满足条件就退出，即 END REPEAT；如果条件不满足，则会就继续执行循环，直到满足退出条件为止。</p>
</li>
<li><p>WHILE…DO…END WHILE：这也是循环语句，和 REPEAT 循环不同的是，这个语句需要先进行条件判断，如果满足条件就进行循环，如果不满足条件就退出循环。</p>
<h3 id="存储过程的争议"><a href="#存储过程的争议" class="headerlink" title="存储过程的争议"></a>存储过程的争议</h3></li>
</ol>
<ul>
<li><p>优点</p>
<ul>
<li>多次使用</li>
<li>可以封装，减少工作量，代码结构清晰</li>
<li>安全性强</li>
<li>减少网络传输</li>
</ul>
</li>
<li><p>缺点</p>
<ul>
<li>可移植性差</li>
<li>调试困难</li>
<li>版本管理困难</li>
<li>不适合高并发场景</li>
</ul>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E5%AD%98%E5%82%A8%E8%BF%87%E7%A8%8B.png" class title="存储过程">

<h2 id="事务处理"><a href="#事务处理" class="headerlink" title="事务处理"></a>事务处理</h2><h3 id="事务的特性"><a href="#事务的特性" class="headerlink" title="事务的特性"></a>事务的特性</h3><p>要么完全执行，要么都不执行。不过要对事务进行更深一步的理解，还要从事务的 4 个特性说起，这 4 个特性用英文字母来表达就是 ACID。</p>
</li>
<li><p>A，也就是原子性（Atomicity）。原子的概念就是不可分割，你可以把它理解为组成物质的基本单位，也是我们进行数据处理操作的基本单位。</p>
</li>
<li><p>C，就是一致性（Consistency）。一致性指的就是数据库在进行事务操作后，会由原来的一致状态，变成另一种一致的状态。也就是说当事务提交后，或者当事务发生回滚后，数据库的完整性约束不能被破坏。</p>
</li>
<li><p>I，就是隔离性（Isolation）。它指的是每个事务都是彼此独立的，不会受到其他事务的执行影响。也就是说一个事务在提交之前，对其他事务都是不可见的。</p>
</li>
<li><p>D，指的是持久性（Durability）。事务提交之后对数据的修改是持久性的，即使在系统出故障的情况下，比如系统崩溃或者存储介质发生故障，数据的修改依然是有效的。因为当事务完成，数据库的日志就会被更新，这时可以通过日志，让系统恢复到最后一次成功的更新状态。</p>
<p>ACID 可以说是事务的四大特性，在这四个特性中，原子性是基础，隔离性是手段，一致性是约束条件，而持久性是我们的目的。</p>
<p>任何写入数据库中的数据都需要满足我们事先定义的约束规则。事务操作会让数据表的状态变成另一种一致的状态，如果事务中的某个操作失败了，系统就会自动撤销当前正在执行的事务，返回到事务操作之前的状态。</p>
<p>事务的另一个特点就是持久性，持久性是通过事务日志来保证的。日志包括了回滚日志和重做日志。当我们通过事务对数据进行修改的时候，首先会将数据库的变化信息记录到重做日志中，然后再对数据库中对应的行进行修改。这样做的好处是，即使数据库系统崩溃，数据库重启后也能找到没有更新到数据库系统中的重做日志，重新执行，从而使事务具有持久性。</p>
<h3 id="事务的控制"><a href="#事务的控制" class="headerlink" title="事务的控制"></a>事务的控制</h3><p>MySQL，可以通过 SHOW ENGINES 命令来查看当前 MySQL 支持的存储引擎都有哪些，以及这些存储引擎是否支持事务。InnoDB 是支持事务的，而 MyISAM 存储引擎不支持事务。</p>
</li>
</ul>
<ol>
<li><p>START TRANSACTION 或者 BEGIN，作用是显式开启一个事务。</p>
</li>
<li><p>COMMIT：提交事务。当提交事务后，对数据库的修改是永久性的。</p>
</li>
<li><p>ROLLBACK 或者 ROLLBACK TO [SAVEPOINT]，意为回滚事务。意思是撤销正在进行的所有没有提交的修改，或者将事务回滚到某个保存点。</p>
</li>
<li><p>SAVEPOINT：在事务中创建保存点，方便后续针对保存点进行回滚。一个事务中可以存在多个保存点。</p>
</li>
<li><p>RELEASE SAVEPOINT：删除某个保存点。</p>
</li>
<li><p>SET TRANSACTION，设置事务的隔离级别。</p>
<p>需要说明的是，使用事务有两种方式，分别为隐式事务和显式事务。隐式事务实际上就是自动提交，Oracle 默认不自动提交，需要手写 COMMIT 命令，而 MySQL 默认自动提交，当然我们可以配置 MySQL 的参数：</p>
<p>mysql&gt; set autocommit =0;  // 关闭自动提交<br>mysql&gt; set autocommit =1;  // 开启自动提交</p>
<p>MySQL 中 completion_type 参数的作用，实际上这个参数有 3 种可能：</p>
</li>
<li><p>completion=0，这是默认情况。也就是说当我们执行 COMMIT 的时候会提交事务，在执行下一个事务时，还需要我们使用 START TRANSACTION 或者 BEGIN 来开启。</p>
</li>
<li><p>completion=1，这种情况下，当我们提交事务后，相当于执行了 COMMIT AND CHAIN，也就是开启一个链式事务，即当我们提交事务之后会开启一个相同隔离级别的事务（隔离级别会在下一节中进行介绍）。</p>
</li>
<li><p>completion=2，这种情况下 COMMIT=COMMIT AND RELEASE，也就是当我们提交后，会自动与服务器断开连接。</p>
<p>当我们设置 autocommit=0 时，不论是否采用 START TRANSACTION 或者 BEGIN 的方式来开启事务，都需要用 COMMIT 进行提交，让事务生效，使用 ROLLBACK 对事务进行回滚。</p>
<p>当我们设置 autocommit=1 时，每条 SQL 语句都会自动进行提交。不过这时，如果你采用 START TRANSACTION 或者 BEGIN 的方式来显式地开启事务，那么这个事务只有在 COMMIT 时才会生效，在 ROLLBACK 时才会回滚。</p>
<h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><p>正是因为有事务的存在，即使在数据库操作失败的情况下，也能保证数据的一致性。同样，多个应用程序访问数据库的时候，事务可以提供隔离，保证事务之间不被干扰。最后，事务一旦提交，结果就会是永久性的，这就意味着，即使系统崩溃了，数据库也可以对数据进行恢复。</p>
<p>事务是数据库区别于文件系统的重要特性之一，当我们有了事务就会让数据库始终保持一致性，同时我们还能通过事务的机制恢复到某个时间点，这样可以保证已提交到数据库的修改不会因为系统崩溃而丢失。</p>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E4%BA%8B%E5%8A%A1%E5%A4%84%E7%90%86.png" class title="事务处理">

<h2 id="事务隔离"><a href="#事务隔离" class="headerlink" title="事务隔离"></a>事务隔离</h2><h3 id="事务并发的可能异常"><a href="#事务并发的可能异常" class="headerlink" title="事务并发的可能异常"></a>事务并发的可能异常</h3><p>SQL-92 标准中已经对 3 种异常情况进行了定义，这些异常情况级别分别为脏读（Dirty Read）、不可重复读（Nnrepeatable Read）和幻读（Phantom Read）。</p>
</li>
<li><p>脏读：读到了其他事务还没有提交的数据。</p>
</li>
<li><p>不可重复读：对某数据进行读取，发现两次读取的结果不同，也就是说没有读到相同的内容。这是因为有其他事务对这个数据同时进行了修改或删除。</p>
</li>
<li><p>幻读：事务 A 根据条件查询得到了 N 条数据，但此时事务 B 更改或者增加了 M 条符合事务 A 查询条件的数据，这样当事务 A 再次进行查询的时候发现会有 N+M 条数据，产生了幻读。</p>
</li>
</ol>
<p>  <strong>不可重复读 VS 幻读的区别：</strong></p>
<ul>
<li><p>不可重复读是同一条记录的内容被修改了，重点在于UPDATE或DELETE</p>
</li>
<li><p>幻读是查询某一个范围的数据行变多了或者少了，重点在于INSERT</p>
<p>SQL-92 标准还定义了 4 种隔离级别来解决这些异常情况。</p>
<p>解决异常数量从少到多的顺序（比如读未提交可能存在 3 种异常，可串行化则不会存在这些异常）决定了隔离级别的高低，这四种隔离级别从低到高分别是：读未提交（READ UNCOMMITTED ）、读已提交（READ COMMITTED）、可重复读（REPEATABLE READ）和可串行化（SERIALIZABLE）。这些隔离级别能解决的异常情况如下表所示：</p>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E8%A7%A3%E5%86%B3%E5%BC%82%E5%B8%B8.png" class title="解决异常">
</li>
<li><p>读未提交，也就是允许读到未提交的数据，这种情况下查询是不会使用锁的，可能会产生脏读、不可重复读、幻读等情况。</p>
</li>
<li><p>读已提交就是只能读到已经提交的内容，可以避免脏读的产生，属于 RDBMS 中常见的默认隔离级别（比如说 Oracle 和 SQL Server），但如果想要避免不可重复读或者幻读，就需要我们在 SQL 查询的时候编写带加锁的 SQL 语句（我会在进阶篇里讲加锁）。</p>
</li>
<li><p>可重复读，保证一个事务在相同查询条件下两次查询得到的数据结果是一致的，可以避免不可重复读和脏读，但无法避免幻读。MySQL 默认的隔离级别就是可重复读。</p>
</li>
<li><p>可串行化，将事务进行串行化，也就是在一个队列中按照顺序执行，可串行化是最高级别的隔离等级，可以解决事务读取中所有可能出现的异常情况，但是它牺牲了系统的并发性。</p>
<h3 id="总结-2"><a href="#总结-2" class="headerlink" title="总结"></a>总结</h3><p>隔离级别的实现满足了下面的两个条件：</p>
</li>
<li><p>正确性：只要能满足某一个隔离级别，一定能解决这个隔离级别对应的异常问题。</p>
</li>
<li><p>与实现无关：实际上 RDBMS 种类很多，这就意味着有多少种 RDBMS，就有多少种锁的实现方式，因此它们实现隔离级别的原理可能不同，然而一个好的标准不应该限制其实现的方式。</p>
<p>隔离级别越低，意味着系统吞吐量（并发程度）越大，但同时也意味着出现异常问题的可能性会更大。在实际使用过程中我们往往需要在性能和正确性上进行权衡和取舍，没有完美的解决方案，只有适合与否。</p>
<img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E4%BA%8B%E5%8A%A1%E9%9A%94%E7%A6%BB.png" class title="事务隔离">

<h2 id="游标"><a href="#游标" class="headerlink" title="游标"></a>游标</h2><p>在数据库中，游标是个重要的概念，它提供了一种灵活的操作方式，可以让我们从数据结果集中每次提取一条数据记录进行操作。游标让 SQL 这种面向集合的语言有了面向过程开发的能力。可以说，游标是面向过程的编程方式，这与面向集合的编程方式有所不同。</p>
<p>在 SQL 中，游标是一种临时的数据库对象，可以指向存储在数据库表中的数据行指针。这里游标充当了指针的作用，我们可以通过操作游标来对数据行进行操作。</p>
<h3 id="如何使用"><a href="#如何使用" class="headerlink" title="如何使用"></a>如何使用</h3><p>游标实际上是一种控制数据集的更加灵活的处理方式。</p>
<p>如果我们想要使用游标，一般需要经历五个步骤。不同 DBMS 中，使用游标的语法可能略有不同。</p>
</li>
</ul>
<ol>
<li>定义游标。</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DECLARE</span> cursor_name <span class="keyword">CURSOR</span> <span class="keyword">FOR</span> select_statement</span><br></pre></td></tr></table></figure>

<p>  要使用 SELECT 语句来获取数据结果集，而此时还没有开始遍历数据，这里 select_statement 代表的是 SELECT 语句。</p>
<ol start="2">
<li>打开游标</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">OPEN cursor_name</span><br></pre></td></tr></table></figure>

<p>  当我们定义好游标之后，如果想要使用游标，必须先打开游标。打开游标的时候 SELECT 语句的查询结果集就会送到游标工作区。</p>
<ol start="3">
<li>从游标中取得数据</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">FETCH cursor_name INTO var_name ...</span><br></pre></td></tr></table></figure>

<p>  这句的作用是使用 cursor_name 这个游标来读取当前行，并且将数据保存到 var_name 这个变量中，游标指针指到下一行。如果游标读取的数据行有多个列名，则在 INTO 关键字后面赋值给多个变量名即可。</p>
<ol start="4">
<li>关闭游标</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">CLOSE cursor_name</span><br></pre></td></tr></table></figure>

<ol start="5">
<li>释放游标</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DEALLOCATE</span> <span class="keyword">PREPARE</span></span><br></pre></td></tr></table></figure>

<p>  我们一定要养成释放游标的习惯，否则游标会一直存在于内存中，直到进程结束后才会自动释放。当你不需要使用游标的时候，释放游标可以减少资源浪费。</p>
<p>  例子：<br>  先创建一个存储过程 calc_hp_max，然后在存储过程中定义游标 cur_hero，使用 FETCH 获取每一行的具体数值，然后赋值给变量 hp，再用变量 hp_sum 做累加求和，最后再输出 hp_sum，代码如下：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">PROCEDURE</span> <span class="string">`calc_hp_max`</span>()</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">        <span class="comment">-- 创建接收游标的变量</span></span><br><span class="line">        <span class="keyword">DECLARE</span> hp <span class="built_in">INT</span>;  </span><br><span class="line">        <span class="comment">-- 创建总数变量 </span></span><br><span class="line">        <span class="keyword">DECLARE</span> hp_sum <span class="built_in">INT</span> <span class="keyword">DEFAULT</span> <span class="number">0</span>;</span><br><span class="line">        <span class="comment">-- 创建结束标志变量  </span></span><br><span class="line">        <span class="keyword">DECLARE</span> done <span class="built_in">INT</span> <span class="keyword">DEFAULT</span> <span class="literal">false</span>;</span><br><span class="line">        <span class="comment">-- 定义游标     </span></span><br><span class="line">        <span class="keyword">DECLARE</span> cur_hero <span class="keyword">CURSOR</span> <span class="keyword">FOR</span> <span class="keyword">SELECT</span> hp_max <span class="keyword">FROM</span> heros;</span><br><span class="line">        <span class="comment">-- 指定游标循环结束时的返回值  </span></span><br><span class="line">        <span class="keyword">DECLARE</span> CONTINUE <span class="keyword">HANDLER</span> <span class="keyword">FOR</span> <span class="keyword">NOT</span> <span class="keyword">FOUND</span> <span class="keyword">SET</span> done = <span class="literal">true</span>;  </span><br><span class="line">        OPEN cur_hero;</span><br><span class="line">        read_loop:LOOP </span><br><span class="line">        FETCH cur_hero INTO hp;</span><br><span class="line">        <span class="keyword">SET</span> hp_sum = hp_sum + hp;</span><br><span class="line">        <span class="keyword">END</span> <span class="keyword">LOOP</span>;</span><br><span class="line">        CLOSE cur_hero;</span><br><span class="line">        <span class="keyword">SELECT</span> hp_sum;</span><br><span class="line">        <span class="keyword">DEALLOCATE</span> <span class="keyword">PREPARE</span> cur_hero;</span><br><span class="line"><span class="keyword">END</span></span><br></pre></td></tr></table></figure>

<p>  当游标溢出时（也就是当游标指向到最后一行数据后继续执行会报的错误），我们可以定义一个 continue 的事件，指定这个事件发生时修改变量 done 的值，以此来判断游标是否已经溢出，即：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">DECLARE</span> CONTINUE <span class="keyword">HANDLER</span> <span class="keyword">FOR</span> <span class="keyword">NOT</span> <span class="keyword">FOUND</span> <span class="keyword">SET</span> done = <span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>  在游标中的循环中，除了使用 LOOP 循环以外，你还可以使用 REPEAT… UNTIL…以及 WHILE 循环。它们同样需要设置 CONTINUE 事件来处理游标溢出的情况。</p>
<p>  所以你能看出，使用游标可以让我们对 SELECT 结果集中的每一行数据进行相同或者不同的操作，从而很精细化地管理结果集中的每一条数据。</p>
<h3 id="总结-3"><a href="#总结-3" class="headerlink" title="总结"></a>总结</h3><p>  游标实际上是面向过程的思维方式，与面向集合的思维方式不同的地方在于，游标更加关注“如何执行”。我们可以通过游标更加精细、灵活地查询和管理想要的数据行。</p>
<p>  有的时候，我们需要找特定数据，用 SQL 查询写起来会比较困难，比如两表或多表之间的嵌套循环查找，如果用 JOIN 会非常消耗资源，效率也可能不高，而用游标则会比较高效。</p>
<p>  虽然在处理某些复杂的数据情况下，使用游标可以更灵活，但同时也会带来一些性能问题，比如在使用游标的过程中，会对数据行进行加锁，这样在业务并发量大的时候，不仅会影响业务之间的效率，还会消耗系统资源，造成内存不足，这是因为游标是在内存中进行的处理。如果有游标的替代方案，我们可以采用替代方案。</p>
  <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/%E6%B8%B8%E6%A0%87.png" class title="游标">

<h2 id="Python-DB-API"><a href="#Python-DB-API" class="headerlink" title="Python DB API"></a>Python DB API</h2><p>  Python 可以支持非常多的数据库管理系统，比如 MySQL、Oracle、SQL Server 和 PostgreSQL 等。为了实现对这些 DBMS 的统一访问，Python 需要遵守一个规范，这就是 DB API 规范。</p>
  <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/pythonAPI.png" class title="pythonAPI">

<p>  使用 Python 对 DBMS 进行操作的时候，需要经过下面的几个步骤：</p>
<ul>
<li><p>引入 API 模块；</p>
</li>
<li><p>与数据库建立连接；</p>
</li>
<li><p>执行 SQL 语句；</p>
</li>
<li><p>关闭数据库连接。</p>
<h3 id="使用-1"><a href="#使用-1" class="headerlink" title="使用"></a>使用</h3><p>例子：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -*- coding: UTF-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> mysql.connector</span><br><span class="line"><span class="comment"># 打开数据库连接</span></span><br><span class="line">db = mysql.connector.connect(</span><br><span class="line">       host=<span class="string">"localhost"</span>,</span><br><span class="line">       user=<span class="string">"root"</span>,</span><br><span class="line">       passwd=<span class="string">"XXX"</span>, <span class="comment"># 写上你的数据库密码</span></span><br><span class="line">       database=<span class="string">'wucai'</span>, </span><br><span class="line">       auth_plugin=<span class="string">'mysql_native_password'</span></span><br><span class="line">)</span><br><span class="line"><span class="comment"># 获取操作游标 </span></span><br><span class="line">cursor = db.cursor()</span><br><span class="line"><span class="comment"># 执行 SQL 语句</span></span><br><span class="line">cursor.execute(<span class="string">"SELECT VERSION()"</span>)</span><br><span class="line"><span class="comment"># 获取一条数据</span></span><br><span class="line">data = cursor.fetchone()</span><br><span class="line">print(<span class="string">"MySQL 版本: %s "</span> % data)</span><br><span class="line"><span class="comment"># 关闭游标 &amp; 数据库连接</span></span><br><span class="line">cursor.close()</span><br><span class="line">db.close()</span><br></pre></td></tr></table></figure>

<p>  以上代码有两个重要部分Connection 和 Cursor</p>
<p>  Connection 就是对数据库的当前连接进行管理，我们可以通过它来进行以下操作：</p>
<ol>
<li><p>通过指定 host、user、passwd 和 port 等参数来创建数据库连接，这些参数分别对应着数据库 IP 地址、用户名、密码和端口号；</p>
</li>
<li><p>使用 db.close() 关闭数据库连接；</p>
</li>
<li><p>使用 db.cursor() 创建游标，操作数据库中的数据；</p>
</li>
<li><p>使用 db.begin() 开启事务；</p>
</li>
<li><p>使用 db.commit() 和 db.rollback()，对事务进行提交以及回滚。</p>
<p>当我们通过cursor = db.cursor()创建游标后，就可以通过面向过程的编程方式对数据库中的数据进行操作：</p>
</li>
<li><p>使用cursor.execute(query_sql)，执行数据库查询；</p>
</li>
<li><p>使用cursor.fetchone()，读取数据集中的一条数据；</p>
</li>
<li><p>使用cursor.fetchall()，取出数据集中的所有行，返回一个元组 tuples 类型；</p>
</li>
<li><p>使用cursor.fetchmany(n)，取出数据集中的多条数据，同样返回一个元组 tuples；</p>
</li>
<li><p>使用cursor.rowcount，返回查询结果集中的行数。如果没有查询到数据或者还没有查询，则结果为 -1，否则会返回查询得到的数据行数；</p>
</li>
<li><p>使用cursor.close()，关闭游标。</p>
<p>增删改查的时候可以使用捕获异常</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> traceback</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">  sql = <span class="string">"INSERT INTO player (team_id, player_name, height) VALUES (%s, %s, %s)"</span></span><br><span class="line">  val = (<span class="number">1003</span>, <span class="string">" 约翰 - 科林斯 "</span>, <span class="number">2.08</span>)</span><br><span class="line">  cursor.execute(sql, val)</span><br><span class="line">  db.commit()</span><br><span class="line">  print(cursor.rowcount, <span class="string">" 记录插入成功。"</span>)</span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">  <span class="comment"># 打印异常信息</span></span><br><span class="line">  traceback.print_exc()</span><br><span class="line">  <span class="comment"># 回滚  </span></span><br><span class="line">  db.rollback()</span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">  <span class="comment"># 关闭数据库连接</span></span><br><span class="line">  db.close()</span><br></pre></td></tr></table></figure>

<h3 id="总结-4"><a href="#总结-4" class="headerlink" title="总结"></a>总结</h3><p>  在使用基于 DB API 规范的协议时，重点需要掌握 Connection 和 Cursor 这两个对象，Connection 就是对数据库的连接进行管理，而 Cursor 是对数据库的游标进行管理，通过它们，我们可以执行具体的 SQL 语句，以及处理复杂的数据。</p>
<p>  用 Python 操作 MySQL，还有很多种姿势，mysql-connector 只是其中一种，实际上还有另外一种方式，就是采用 ORM 框架。ORM 的英文是 Object Relational Mapping，也就是采用对象关系映射的模式，使用这种模式可以将数据库中各种数据表之间的关系映射到程序中的对象。这种模式可以屏蔽底层的数据库的细节，不需要我们与复杂的 SQL 语句打交道，直接采用操作对象的形式操作就可以。</p>
  <img src="/2020/03/08/SQL%E5%9F%BA%E7%A1%80%E5%AD%A6%E4%B9%A0/pythonMySQL.png" class title="pythonMySQL">

<h2 id="ORM框架"><a href="#ORM框架" class="headerlink" title="ORM框架"></a>ORM框架</h2><p>  持久化层在业务逻辑层和数据库层起到了衔接的作用，它可以将内存中的数据模型转化为存储模型，或者将存储模型转化为内存中的数据模型。</p>
  

<p>  在程序的层面操作数据，其实都是把数据放到内存中进行处理，如果需要数据就会通过持久化层，从数据库中取数据；如果需要保存数据，就是将对象数据通过持久化层存储到数据库中。</p>
<p>   ORM 提供了一种持久化模式，可以高效地对数据库进行访问。ORM 的英文是 Object Relation Mapping，中文叫对象关系映射。它是 RDBMS 和业务实体对象之间的一个映射，从图中你也能看到，它可以把底层的 RDBMS 封装成业务实体对象，提供给业务逻辑层使用。程序员往往关注业务逻辑层面，而不是底层数据库该如何访问，以及如何编写 SQL 语句获取数据等等。采用 ORM，就可以从数据库的设计层面转化成面向对象的思维。</p>
<p>   没有一种模式是完美的，采用 ORM 当然也会付出一些代价，比如性能上的一些损失。面对一些复杂的数据查询，ORM 会显得力不从心。虽然可以实现功能，但相比于直接编写 SQL 查询语句来说，ORM 需要编写的代码量和花费的时间会比较多，这种情况下，直接编写 SQL 反而会更简单有效。</p>
<h3 id="python中的三种主流ORM框架"><a href="#python中的三种主流ORM框架" class="headerlink" title="python中的三种主流ORM框架"></a>python中的三种主流ORM框架</h3><ol>
<li><p>Django，它是 Python 的 WEB 应用开发框架，本身走大而全的方式。Django 采用了 MTV 的框架模式，包括了 Model（模型），View（视图）和 Template（模版）。Model 模型只是 Django 的一部分功能，我们可以通过它来实现数据库的增删改查操作。</p>

</li>
<li><p>SQLALchemy，它也是 Python 中常用的 ORM 框架之一。它提供了 SQL 工具包及 ORM 工具，如果你想用支持 ORM 和支持原生 SQL 两种方式的工具，那么 SQLALchemy 是很好的选择。另外 SQLALchemy 的社区更加活跃，这对项目实施会很有帮助。</p>
</li>
<li><p>peewee，这是一个轻量级的 ORM 框架，简单易用。peewee 采用了 Model 类、Field 实例和 Model 实例来与数据库建立映射关系，从而完成面向对象的管理方式。使用起来方便，学习成本也低。</p>
<h3 id="SQLAlchemy-操作-MySQL"><a href="#SQLAlchemy-操作-MySQL" class="headerlink" title="SQLAlchemy 操作 MySQL"></a>SQLAlchemy 操作 MySQL</h3></li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install sqlalchemy</span><br><span class="line">初始化数据库连接</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> create_engine</span><br><span class="line"><span class="comment"># 初始化数据库连接，修改为你的数据库用户名和密码</span></span><br><span class="line">engine = create_engine(<span class="string">'mysql+mysqlconnector://root:password@localhost:3306/wucai'</span>)</span><br><span class="line"><span class="comment"># mysql+mysqlconnector，后面的是用户名:密码@IP地址:端口号/数据库名称</span></span><br></pre></td></tr></table></figure>

<h4 id="创建模型"><a href="#创建模型" class="headerlink" title="创建模型"></a>创建模型</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sqlalchemy <span class="keyword">import</span> Column, String, Integer, Float</span><br><span class="line"><span class="comment"># 定义 Player 对象:</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Player</span><span class="params">(Base)</span>:</span></span><br><span class="line">    <span class="comment"># 表的名字:</span></span><br><span class="line">    __tablename__ = <span class="string">'player'</span></span><br><span class="line"> </span><br><span class="line">    <span class="comment"># 表的结构:</span></span><br><span class="line">    player_id = Column(Integer, primary_key=<span class="literal">True</span>, autoincrement=<span class="literal">True</span>)</span><br><span class="line">    team_id = Column(Integer)</span><br><span class="line">    player_name = Column(String(<span class="number">255</span>))</span><br><span class="line">    height = Column(Float(<span class="number">3</span>,<span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>  <strong>tablename</strong> 指明了模型对应的数据表名称，即 player 数据表。同时我们在 Player 模型中对采用的变量名进行定义，变量名需要和数据表中的字段名称保持一致，否则会找不到数据表中的字段。</p>
<h4 id="增删改查"><a href="#增删改查" class="headerlink" title="增删改查"></a>增删改查</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 创建 DBSession 类型:</span></span><br><span class="line">DBSession = sessionmaker(bind=engine)</span><br><span class="line"><span class="comment"># 创建 session 对象:</span></span><br><span class="line">session = DBSession()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># 创建 Player 对象:</span></span><br><span class="line">new_player = Player(team_id = <span class="number">1003</span>, player_name = <span class="string">" 约翰 - 科林斯 "</span>, height = <span class="number">2.08</span>)</span><br><span class="line"><span class="comment"># 添加到 session:</span></span><br><span class="line">session.add(new_player)</span><br><span class="line"><span class="comment"># 提交即保存到数据库:</span></span><br><span class="line">session.commit()</span><br><span class="line"><span class="comment"># 关闭 session:</span></span><br><span class="line">session.close()</span><br></pre></td></tr></table></figure>

<p>  查询数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 增加 to_dict() 方法到 Base 类中</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">to_dict</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> &#123;c.name: getattr(self, c.name, <span class="literal">None</span>)</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> self.__table__.columns&#125;</span><br><span class="line"><span class="comment"># 将对象可以转化为 dict 类型</span></span><br><span class="line">Base.to_dict = to_dict</span><br><span class="line"><span class="comment"># 查询身高 &gt;=2.08 的球员有哪些</span></span><br><span class="line">rows = session.query(Player).filter(Player.height &gt;= <span class="number">2.08</span>).all()</span><br><span class="line">print([row.to_dict() <span class="keyword">for</span> row <span class="keyword">in</span> rows])</span><br></pre></td></tr></table></figure>

<p>  删除数据</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">row = session.query(Player).filter(Player.player_name==<span class="string">'约翰 - 科林斯'</span>).first()</span><br><span class="line">session.delete(row)</span><br><span class="line">session.commit()</span><br><span class="line">session.close()</span><br></pre></td></tr></table></figure>

<h3 id="总结-5"><a href="#总结-5" class="headerlink" title="总结"></a>总结</h3><p>  如果项目本身不大，那么自己动手写 SQL 语句会比较简单，可以不使用 ORM 工具，而是直接使用 mysql-connector。但是随着项目代码量的增加，为了在业务逻辑层与数据库底层进行松耦合，采用 ORM 框架是更加适合的。</p>
  


<h2 id="基础总结"><a href="#基础总结" class="headerlink" title="基础总结"></a>基础总结</h2><ul>
<li><p>列式数据库是将数据按照列存储到数据库中，这样做的好处是可以大量降低系统的 I/O</p>
<p>行式存储是把一行的数据都串起来进行存储，然后再存储下一行。同样，列式存储是把一列的数据都串起来进行存储，然后再存储下一列。这样做的话，相邻数据的数据类型都是一样的，更容易压缩，压缩之后就自然降低了 I/O。</p>
<p>数据处理可以分为 OLTP（联机事务处理）和 OLAP（联机分析处理）两大类。</p>
<p>OLTP 一般用于处理客户的事务和进行查询，需要随时对数据表中的记录进行增删改查，对实时性要求高。</p>
<p>OLAP 一般用于市场的数据分析，通常数据量大，需要进行复杂的分析操作，可以对大量历史数据进行汇总和分析，对实时性要求不高。</p>
<p>那么对于 OLTP 来说，由于随时需要对数据记录进行增删改查，更适合采用行式存储，因为一行数据的写入会同时修改多个列。传统的 RDBMS 都属于行式存储，比如 Oracle、SQL Server 和 MySQL 等。</p>
<p>对于 OLAP 来说，由于需要对大量历史数据进行汇总和分析，则适合采用列式存储，这样的话汇总数据会非常快，但是对于插入（INSERT）和更新（UPDATE）会比较麻烦，相比于行式存储性能会差不少。</p>
<p>所以说列式存储适合大批量数据查询，可以降低 I/O，但如果对实时性要求高，则更适合行式存储。</p>
</li>
<li><p>在 MySQL InnoDB 存储引擎中，COUNT(*)和COUNT(1)都是对所有结果进行COUNT。如果有 WHERE 子句，则是对所有符合筛选条件的数据行进行统计；如果没有 WHERE 子句，则是对数据表的数据行数进行统计。</p>
<p>一般情况下，三者执行的效率为 COUNT(*)= COUNT(1)&gt; COUNT(字段)。我们尽量使用COUNT(*)，当然如果你要统计的是某个字段的非空数据行数，则另当别论，毕竟比较执行效率的前提是结果一样才可以。<br>如果要统计COUNT(*),尽量在数据表上建立二级索引，系统会自动采用key_len小的二级索引进行扫描，这样当我们使用SELECT COUNT(*)的时候效率就会提升，有时候可以提升几倍甚至更高。</p>
</li>
<li><p>一条完整的 SELECT 语句内部的执行顺序是这样的：</p>
</li>
</ul>
<ol>
<li>FROM 子句组装数据（包括通过 ON 进行连接）；</li>
<li>WHERE 子句进行条件筛选；</li>
<li>GROUP BY 分组 ；</li>
<li>使用聚集函数进行计算；</li>
<li>HAVING 筛选分组；</li>
<li>计算所有的表达式；</li>
<li>SELECT 的字段；</li>
<li>ORDER BY 排序；</li>
<li>LIMIT 筛选。</li>
</ol>
<ul>
<li><p>varchar和nvarchar区别</p>
<ul>
<li><p>相同点：可变长度，字符类型数据</p>
</li>
<li><p>不同点：</p>
<p>  varchar(n)是n个字节，非Unicode字符。（英文字母占1个字节，中文占2个字节）</p>
<p>  而nvarchar(n)是n个字符，Unicode字符。（英文字母或者中文都是占用2个字节）</p>
</li>
</ul>
<p>举个例子，varchar(10)代表10个字节，所以可以是10个英文字母，也可以是5个汉字。<br>而nvarchar(10)代表10个字符，这10个字符可以是10个字母，也可以是10个汉字（英文字母或者中文 都是占用2个字节）</p>
</li>
</ul>
]]></content>
      <tags>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(二) · 核心编程</title>
    <url>/2020/02/26/Spark%E6%A0%B8%E5%BF%83%E7%BC%96%E7%A8%8B/</url>
    <content><![CDATA[<h2 id="RDD常用操作"><a href="#RDD常用操作" class="headerlink" title="RDD常用操作"></a>RDD常用操作</h2><h3 id="RDD-Operation"><a href="#RDD-Operation" class="headerlink" title="RDD Operation"></a>RDD Operation</h3><ol>
<li><p>transformations: which create a new dataset from an existing one</p>
<p> lazy()<br> map/filter/group by/distinct/…</p>
</li>
<li><p>actions: which return a value to the driver program after running a computation on the dataset</p>
<p> count/reduce/collect/….</p>
</li>
</ol>
<p>transformations are lazy. The transformations are only computed when an action requires a result to be returned to the driver program. </p>
<p>action triggers the computation, returns values to driver or writes data to external storage.</p>
<h4 id="transformations"><a href="#transformations" class="headerlink" title="transformations"></a>transformations</h4><ol>
<li><p>map:</p>
<p> map(func) </p>
<p> 将func函数作用到数据集的每一个元素上，生成一个新的分布式数据集返回</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_map</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    rdd1 = sc.parallelize(data)</span><br><span class="line">    rdd2 = rdd1.map(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line">    print(rdd2.collect())</span><br></pre></td></tr></table></figure>


<ol start="2">
<li><p>filter：</p>
<p> filter(func)</p>
<p> 选出所有func返回值为true的元素，生成一个新的分布式数据集返回</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_filter</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">    rdd1 = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd1.map(<span class="keyword">lambda</span> x: x*<span class="number">2</span>)</span><br><span class="line">    filterRdd = mapRdd.filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>)</span><br><span class="line">    print(filterRdd.collect())</span><br><span class="line"></span><br><span class="line">    print(sc.parallelize(data).map(</span><br><span class="line">        <span class="keyword">lambda</span> x: x*<span class="number">2</span>).filter(<span class="keyword">lambda</span> x: x &gt; <span class="number">5</span>).collect())</span><br></pre></td></tr></table></figure>

<ol start="3">
<li><p>flatMap：</p>
<p> faltMap(func)</p>
<p> 输入的item能够被map到0或者多个items输出，返回值是一个Sequence</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_flatMap</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    print(sc.parallelize(data).flatMap(</span><br><span class="line">        <span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)).collect())</span><br></pre></td></tr></table></figure>

<ol start="4">
<li><p>groupByKey：</p>
<p> groupByKey(func)</p>
<p> 把相同的key的数据分发到一起</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_groupBy</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)</span><br><span class="line">                            ).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    groupByRdd = mapRdd.groupByKey()</span><br><span class="line">    print(groupByRdd.collect())</span><br><span class="line">    print(groupByRdd.map(<span class="keyword">lambda</span> x: &#123;x[<span class="number">0</span>]: list(x[<span class="number">1</span>])&#125;).collect())</span><br></pre></td></tr></table></figure>

<ol start="5">
<li><p>reduceByKey：</p>
<p> reduceByKey(func)</p>
<p> 把相同的key的数据分发到一起，并进行相应的计算</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_reduceByKey</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)</span><br><span class="line">                            ).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    reduceByKeyRdd = mapRdd.reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    print(reduceByKeyRdd.collect())</span><br></pre></td></tr></table></figure>

<ol start="6">
<li><p>sortByKey：</p>
<p> 顺序排列，按照key的顺序，缺省True为升序，False为降序</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_sortByKey</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="string">'hello spark'</span>, <span class="string">'hello world'</span>, <span class="string">'hello world'</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    mapRdd = rdd.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>)</span><br><span class="line">                            ).map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))</span><br><span class="line">    reduceByKeyRdd = mapRdd.reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line">    print(reduceByKeyRdd.collect())</span><br><span class="line">    print(reduceByKeyRdd.map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>])).sortByKey(<span class="literal">False</span>).map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>])).collect())</span><br></pre></td></tr></table></figure>

<ol start="7">
<li><p>union：</p>
<p> 数据集并集</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_union</span><span class="params">()</span>:</span></span><br><span class="line">    a = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    b = sc.parallelize([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    print(a.union(b).collect())</span><br></pre></td></tr></table></figure>

<ol start="8">
<li><p>distinct：</p>
<p> 取唯一</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_distinct</span><span class="params">()</span>:</span></span><br><span class="line">    a = sc.parallelize([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    b = sc.parallelize([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line">    print(a.union(b).collect())</span><br><span class="line">    print(a.union(b).distinct().collect())</span><br></pre></td></tr></table></figure>

<ol start="9">
<li><p>join：</p>
<p> 连接：左连接/右连接/全连接</p>
</li>
</ol>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_join</span><span class="params">()</span>:</span></span><br><span class="line">    a = sc.parallelize([(<span class="string">'A'</span>, <span class="string">'a1'</span>), (<span class="string">'C'</span>, <span class="string">'c1'</span>), (<span class="string">'D'</span>, <span class="string">'d1'</span>), (<span class="string">'F'</span>, <span class="string">'f1'</span>), (<span class="string">'F'</span>, <span class="string">'f2'</span>)])</span><br><span class="line">    b = sc.parallelize([(<span class="string">'A'</span>, <span class="string">'a2'</span>), (<span class="string">'C'</span>, <span class="string">'c2'</span>), (<span class="string">'C'</span>, <span class="string">'c3'</span>), (<span class="string">'E'</span>, <span class="string">'e1'</span>)])</span><br><span class="line">    print(a.join(b).collect())</span><br><span class="line">    print(a.leftOuterJoin(b).collect())</span><br><span class="line">    print(a.rightOuterJoin(b).collect())</span><br><span class="line">    print(a.fullOuterJoin(b).collect())</span><br></pre></td></tr></table></figure>

<h4 id="actions"><a href="#actions" class="headerlink" title="actions"></a>actions</h4><p>collect/count/take/max/min/sum/reduce/foreach/…</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_action</span><span class="params">()</span>:</span></span><br><span class="line">    data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>]</span><br><span class="line">    rdd = sc.parallelize(data)</span><br><span class="line">    print(rdd.collect())</span><br><span class="line">    print(rdd.count())</span><br><span class="line">    print(rdd.take(<span class="number">3</span>))</span><br><span class="line">    print(rdd.max())</span><br><span class="line">    print(rdd.min())</span><br><span class="line">    print(rdd.sum())</span><br><span class="line">    print(rdd.reduce(<span class="keyword">lambda</span> x, y: x+y))</span><br><span class="line">    rdd.foreach(<span class="keyword">lambda</span> x: print(x))</span><br></pre></td></tr></table></figure>

<h3 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h3><h4 id="词频统计"><a href="#词频统计" class="headerlink" title="词频统计"></a>词频统计</h4><ol>
<li><p>input：</p>
<p> 文件 文件夹 后缀名</p>
</li>
<li><p>步骤分析：</p>
<p> 文本内容转成一个个单词 ：flatMap</p>
<p> 单词 ==&gt; (单词， 1) ：map</p>
<p> 统计相加 ：reduceByKey</p>
</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line">findspark.init()</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">3</span>:</span><br><span class="line">        print(<span class="string">'Usage: wordcount &lt;input&gt;&lt;output&gt;'</span>, file=sys.stderr)</span><br><span class="line">        sys.exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    sc = SparkContext(conf = conf)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">printResult</span><span class="params">()</span>:</span></span><br><span class="line">        counts = sc.textFile(sys.argv[<span class="number">1</span>])\</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>))\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))\</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)</span><br><span class="line"></span><br><span class="line">        output = counts.collect()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(word, count) <span class="keyword">in</span> output:</span><br><span class="line">            print(<span class="string">'%s : %i'</span>%(word, count))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">saveFile</span><span class="params">()</span>:</span></span><br><span class="line">        counts = sc.textFile(sys.argv[<span class="number">1</span>])\</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>))\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x:(x, <span class="number">1</span>))\</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b:a+b)\</span><br><span class="line">            .saveAsTextFile(sys.argv[<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">        output = counts.collect()</span><br><span class="line">        print(output)</span><br><span class="line"></span><br><span class="line">    printResult()</span><br><span class="line">    saveFile()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br></pre></td></tr></table></figure>

<h4 id="TopN"><a href="#TopN" class="headerlink" title="TopN"></a>TopN</h4><ol>
<li><p>input：</p>
<p> 文件 文件夹 后缀名</p>
</li>
<li><p>求某个维度的topn</p>
</li>
<li><p>步骤分析：</p>
<p> 文本内容的每一行根据需求提取所需要的字段：map</p>
<p> 单词 ==&gt; (单词, 1)：map</p>
<p> 所有相同的相加：reduceByKey</p>
<p> 取最多出现的次数降序：sortByKey</p>
</li>
</ol>
<h4 id="平均数"><a href="#平均数" class="headerlink" title="平均数"></a>平均数</h4><ol>
<li><p>input:</p>
<p> id age</p>
</li>
<li><p>步骤分析：</p>
<p> 读取数据取出年龄：map</p>
<p> 计算总和：reduce</p>
<p> 计算个数：count</p>
<p> 求平均</p>
</li>
</ol>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line">findspark.init()</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> len(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        print(<span class="string">'Usage: wordcount &lt;input&gt;&lt;output&gt;'</span>, file=sys.stderr)</span><br><span class="line">        sys.exit(<span class="number">-1</span>)</span><br><span class="line"></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    sc = SparkContext(conf = conf)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">topn</span><span class="params">()</span>:</span></span><br><span class="line">        counts = sc.textFile(sys.argv[<span class="number">1</span>])\</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">' '</span>))\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>))\</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b: a+b)\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>]))\</span><br><span class="line">            .sortByKey(<span class="literal">False</span>)\</span><br><span class="line">            .map(<span class="keyword">lambda</span> x:(x[<span class="number">1</span>], x[<span class="number">0</span>])).take(<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span>(word, count) <span class="keyword">in</span> counts:</span><br><span class="line">            print(<span class="string">'%s : %i'</span>%(word, count))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">average</span><span class="params">()</span>:</span></span><br><span class="line">        agedata = sc.textFile(sys.argv[<span class="number">1</span>]).map(<span class="keyword">lambda</span> x:x.split(<span class="string">' '</span>)[<span class="number">1</span>])</span><br><span class="line">        reducedata = agedata.map(<span class="keyword">lambda</span> age:int(age)).reduce(<span class="keyword">lambda</span> a,b:a+b)</span><br><span class="line">        countdata = agedata.count()</span><br><span class="line">        avgdata = reducedata/countdata</span><br><span class="line"></span><br><span class="line">        print(agedata.collect())</span><br><span class="line">        print(reducedata)</span><br><span class="line">        print(countdata)</span><br><span class="line">        print(avgdata)</span><br><span class="line"></span><br><span class="line">    average()</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br></pre></td></tr></table></figure>]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark(一) · 初识</title>
    <url>/2020/02/25/Spark%E5%88%9D%E8%AF%86/</url>
    <content><![CDATA[<h2 id="环境搭建"><a href="#环境搭建" class="headerlink" title="环境搭建"></a>环境搭建</h2><p>需要的环境</p>
<ul>
<li>Linux</li>
<li>Python : 3.6</li>
<li>Hadoop : 5.7以上</li>
<li>Spark : 2.3.0以上</li>
<li>ElasticSearch : 6.3.0</li>
<li>Kibana : 6.3.0</li>
<li>JDK : 1.8</li>
<li>Azkaban : 3.x</li>
</ul>
<p>掌握技巧</p>
<ul>
<li>tar解压命令</li>
<li>环境的配置</li>
</ul>
<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><h4 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h4><p>源码： <a href="https://github.com/apache/spark" target="_blank" rel="noopener">https://github.com/apache/spark</a></p>
<p>1) RDD是一个抽象类<br>2) 带泛型，可以支持多种类型</p>
<p>RDD: Resilient Distributed Dataset 弹性 分布式 数据集</p>
<p>Represents an<br>immutable 不可变<br>partitioned collection of elements 分区<br>that can be operated on in parallel. 并行计算</p>
<p>单机存储/计算 ==&gt; 分布式存储/计算</p>
<p>1) 数据的存储: 切割    HDFS的Block<br>2) 数据的计算: 切割(分布式并行计算)    MapReduce/Spark<br>3) 存储+计算: HDFS/S3+MapReduce/Spark</p>
<h4 id="RDD的特性"><a href="#RDD的特性" class="headerlink" title="RDD的特性"></a>RDD的特性</h4><p>Internally, each RDD is characterized by five main properties:</p>
<ul>
<li>A list of partitions</li>
<li>系列的分区/分片</li>
<li>A function for computing each split</li>
<li>操作是对每个分区的</li>
<li>A list of dependencies on other RDDs</li>
<li>RDD存在依赖关系(核心,非常重要)</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>分区策略</li>
<li>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</li>
<li>数据在哪优先把作业调度到数据所在的节点进行计算</li>
</ul>
<p><strong>五大特性</strong></p>
<ul>
<li>def compute(split: Partition, context: TaskContext): Iterator[T] 特性二</li>
<li>def getPartitions: Array[Partition] 特性一</li>
<li>def getDependencies: Seq[Dependency[_]] = deps 特性三</li>
<li>def getPreferredLocations(split: Partition): Seq[String] = Nil 特性五</li>
<li>val partitioner: Option[Partitioner] = None 特性四</li>
</ul>
<p>有两种方式创建RDD </p>
<ul>
<li><p>把一个集合转成RDD Parallelized Collections</p>
</li>
<li><p>把外部数据集Hadoop的兼容转换成RDD External Datasets</p>
</li>
</ul>
<h3 id="SparkContext-amp-SparkConf"><a href="#SparkContext-amp-SparkConf" class="headerlink" title="SparkContext&amp;SparkConf"></a>SparkContext&amp;SparkConf</h3><p>第一要务:创建SparkContext</p>
<ul>
<li><p>连接到Spark集群:local,standalone,yarn,mesos</p>
</li>
<li><p>通过SparkContext来创建RDD，广播变量到集群</p>
</li>
</ul>
<p>在创建SparkContext之前还需要创建SparkConf(优先级高于系统)</p>
<h3 id="PySpark脚本"><a href="#PySpark脚本" class="headerlink" title="PySpark脚本"></a>PySpark脚本</h3><h3 id="Spark应用程序及开发"><a href="#Spark应用程序及开发" class="headerlink" title="Spark应用程序及开发"></a>Spark应用程序及开发</h3><p>1) IDE: pycharm</p>
<p>2) 设置基本参数: python interceptor  PYTHONPATH SPARK_HOME 2zip包</p>
<p>3) 开发</p>
<p>4) 使用local进行本地测试</p>
<p>提交pyspark应用程序</p>
<pre><code>spark-submit --master local[2] --name sparktest /home/***/*.py</code></pre>]]></content>
      <tags>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>初识Redis</title>
    <url>/2020/02/24/Redis%E5%88%9D%E8%AF%86/</url>
    <content><![CDATA[<p>Redis是一个开源的、基于内存的数据结构存储器，可以用作数据库、缓存和消息中间件。</p>
<h3 id="我们可以从缓存开始熟悉"><a href="#我们可以从缓存开始熟悉" class="headerlink" title="我们可以从缓存开始熟悉"></a>我们可以从缓存开始熟悉</h3><p>实现一个缓存</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="comment">// get value from cache</span></span><br><span class="line"><span class="keyword">String</span> value = <span class="built_in">map</span>.<span class="built_in">get</span>(<span class="string">"someKey"</span>);</span><br><span class="line"><span class="keyword">if</span>(null == value) &#123;</span><br><span class="line"> <span class="comment">// get value from DataBase</span></span><br><span class="line"> value = queryValueFromDB(<span class="string">"someKey"</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>HashMap、TreeMap这些都线程不安全，可以用HashTable或者ConcurrentHashMap。</p>
<p>不管你用什么样的Map，它的背后都是key-value的Hash表结构，目的就是为了实现O(1)复杂度的查找算法，Redis也是这样实现的，另一个常用的缓存框架Memcached也是。</p>
<p>Hash表的数据结构是怎样的呢？<br><img src="https://pic1.zhimg.com/80/v2-84e2b36df3700de8417fa052b3ac19b8_hd.jpg" alt="hash"></p>
<p>简单说，Hash表就是一个数组，而这个数组的元素，是一个链表。</p>
<p>为什么元素是链表？理论上，如果我们的数组可以做成无限大，那么每来一个key，我们都可以把它放到一个新的位置。但是这样很明显不可行，数组越大，占用的内存就越大。</p>
<p>所以我们需要限制数组的大小，假设是16，那么计算出key的hash值后，对16取模，得出一个0~15的数，然后放到数组对应的位置上去。</p>
<p>好，现在key1放到index为2的位置，突然又来了一个key9，刚好他也要放到index为2的位置，那咋办，总不能把人家key1给踢掉吧？所以key1的信息必须存储在一个链表结构里面，这样key9来了之后，只需要把key1所在的链表节点的next，指向key9的链表节点即可。</p>
<p>很明显，链表越长，Hash表的查询、插入、删除等操作的性能都会下降，极端情况下，如果全部元素都放到了一个链表里头，复杂度就会降为O(n)，也就和顺序查找算法无异了。（正因如此，Java8里头的HashMap在元素增长到一定程度时会从链表转成一颗红黑树，来减缓查找性能的下降）</p>
<p>怎么解决？rehash。关于rehash，这里就不细讲了，大家可以先了解一下Java HashMap的resize函数，然后再通过这篇文章：<a href="https://medium.com/@kousiknath/a-little-internal-on-redis-key-value-storage-implementation-fdf96bac7453" target="_blank" rel="noopener">A little internal on redis key value storage implementation</a> 去了解Redis的rehash算法，你会惊讶的发现Redis里头居然是两个HashTable。</p>
<h3 id="C-S架构"><a href="#C-S架构" class="headerlink" title="C/S架构"></a>C/S架构</h3><p>作为Redis用户，我们要怎样把数据放到上面提到的Hash表里呢？</p>
<p>我们可以通过Redis的命令行，当然也可以通过各种语言的Redis API，在代码里面对Hash表进行操作，所以我们可以将Redis看作是一个C/S架构，客户端是各种操作，Hash表是服务端。</p>
<p>显然，Client和Server可以是在一台机器上的，也可以不在：</p>
<p><img src="https://pic2.zhimg.com/80/v2-d67c538b3a759800eb8102b5eeefee01_hd.jpg" alt="client"></p>
<p><a href="http://try.redis.io/" target="_blank" rel="noopener">try redis</a>这个网站可以用来熟悉Reids的操作</p>
<p>值得一提的是，Redis的Server是单线程服务器，基于Event-Loop模式来处理Client的请求，这一点和NodeJS很相似。使用单线程的好处包括：</p>
<ul>
<li><p>不必考虑线程安全问题。很多操作都不必加锁，既简化了开发，又提高了性能；</p>
</li>
<li><p>减少线程切换损耗的时间。线程一多，CPU在线程之间切来切去是非常耗时的，单线程服务器则没有了这个烦恼；</p>
</li>
</ul>
<p>当然，单线程服务器最大的问题自然是无法充分利用多处理器。</p>
<h3 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h3><p>搭起这样一个框架，一台客户端，一台Redis缓存服务器</p>
<p>随着系统中使用Redis的客户端越来越多，会带来两个问题：</p>
<ul>
<li>Redis内存不足：随着使用Redis的客户端越来越多，Redis上的缓存数据也越来越大，而一台机器的内存毕竟是有限的，放不了那么多数据；</li>
<li>Redis吞吐量低：客户端变多了，可Redis还是只有一台，而且我们已经知道，Redis是单线程的！一台机器的带宽和处理器都是有限的，Redis自然会忙不过来，吞吐量已经不足以支撑我们越来越庞大的系统。</li>
</ul>
<p>可以通过集群的方式解决问题</p>
<p><img src="https://pic1.zhimg.com/80/v2-3ea442fd9cfba7ae0569f40e764dd8f0_hd.jpg" alt="集群"></p>
<p>客户端的请求会通过负载均衡算法（通常是一致性Hash），分散到各个Redis服务器上。</p>
<p>通过集群，我们实现了两个特性：</p>
<ul>
<li>扩大缓存容量；</li>
<li>提升吞吐量；</li>
</ul>
<p>解决了上面提到的两个问题。</p>
<h3 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h3><p>现在我们已经把Redis升级到了集群，真可谓效果杠杠的，可运行了一段时间后，运维又过来反馈了两个问题：</p>
<ul>
<li>数据可用性差：如果其中一台Redis挂了，那么上面全部的缓存数据都会丢失，导致原来可以从缓存中获取的请求，都去访问数据库了，数据库压力陡增。</li>
<li>数据查询缓慢：监测发现，每天有一段时间，Redis 1的访问量非常高，而且大多数请求都是去查一个相同的缓存数据，导致Redis 1非常忙碌，吞吐量不足以支撑这个高的查询负载。</li>
</ul>
<p>问题分析完，要想解决可用性问题，我们第一个想到的，就是数据库里头经常用到的Master-Slave模式，于是，我们给每一台Redis都加上了一台Slave：</p>
<p><img src="https://pic4.zhimg.com/80/v2-b971a5e0d88583cdb8c5c550b5e5b2ab_hd.jpg" alt="slave"></p>
<p>通过Master-Slave模式，我们又实现了两个特性：</p>
<ul>
<li>数据高可用：Master负责接收客户端的写入请求，将数据写到Master后，同步给Slave，实现数据备份。一旦Master挂了，可以将Slave提拔为Master；</li>
<li>提高查询效率：一旦Master发现自己忙不过来了，可以把一些查询请求，转发给Slave去处理，也就是Master负责读写或者只负责写，Slave负责读；</li>
</ul>
<p>为了让Master-Slave模式发挥更大的威力，我们当然可以放更多的Slave，就像这样：</p>
<p><img src="https://pic4.zhimg.com/80/v2-76238e772c8bb5feaa5bb20e4207cfcf_hd.jpg" alt="更多slave"></p>
<p>可这样又引发了另一个问题，那就是Master进行数据备份的工作量变大了，Slava每增加一个，Master就要多备份一次，于是又有了Master/slave chains的架构：</p>
<p><img src="https://pic1.zhimg.com/80/v2-eb813169598035287738730a5f53c2cc_hd.jpg" alt="chains"></p>
<p>这样最顶层的Master的备份压力就没那么大了，它只需要备份两次，然后让那它底下的那两台Slave再去和他们的Slave备份。</p>
<p>事实上，Redis内部要处理的问题还有很多：</p>
<ul>
<li>数据结构。文章一开头提到了，Redis不仅仅是数据存储器，而是数据结构存储器。那是因为Redis支持客户端直接往里面塞各种类型的数据结构，比如String、List、Set、SortedSet、Map等等。你或许会问，这很了不起吗？我自己在Java里写一个HashTable不也可以放各种数据结构？呵呵，要知道你的HashTable只能放Java对象，人家那可是支持多语言的，不管你的客户端是Java还是Python还是别的，都可以往Redis塞数据结构。这一点也是Redis和Memcached相比，非常不同的一点。当然Redis要支持数据结构存储，是以牺牲更多内存为代价的，正所谓有利必有弊。关于Redis里头的数据结构，大家可以参考：<a href="https://redis.io/topics/data-types-intro" target="_blank" rel="noopener">Redis Data Types</a></li>
<li>剔除策略。缓存数据总不能无限增长吧，总得剔除掉一些数据，好让新的缓存数据放进来吧？这就需要LRU算法了，大家可以参考：<a href="https://redis.io/topics/lru-cache" target="_blank" rel="noopener">Redis Lru Cache</a></li>
<li>负载均衡。用到了集群，就免不了需要用到负载均衡，用什么负载均衡算法？在哪里使用负载均衡？这点大家可以参考：<a href="https://redis.io/topics/partitioning" target="_blank" rel="noopener">Redis Partitioning</a></li>
<li>Presharding。如果一开始只有三台Redis服务器，后来发现需要加多一台才能满足业务需要，要怎么办？Redis提供了一种策略，叫：<a href="https://redis.io/topics/partitioning#presharding" target="_blank" rel="noopener">Presharding</a></li>
<li>数据持久化。如果我的机器突然全部断电了，我的缓存数据还能恢复吗？Redis说，相信我，可以的，不然我怎么用作数据库？去看看这个：<a href="https://redis.io/topics/persistence" target="_blank" rel="noopener">Redis Persistence</a>]</li>
<li>数据同步。这篇文章里提到了主从复制，那么Redis是怎么进行主从复制的呢？根据CAP理论，既然我们已经选择了集群，也就是P，分区容忍性，那么剩下那两个，Consistency和Availability只能选择一个了，那么Redis到底是支持最终一致性还是强一致性呢？可以参考：<a href="https://redis.io/topics/replication" target="_blank" rel="noopener">Redis Replication</a></li>
</ul>
<blockquote>
<p>参考知乎<a href="https://zhuanlan.zhihu.com/p/37055648" target="_blank" rel="noopener">Redis简明教程</a></p>
</blockquote>
]]></content>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark安装</title>
    <url>/2020/02/10/Spark%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="下载Spark"><a href="#下载Spark" class="headerlink" title="下载Spark"></a>下载Spark</h2><p>根据地址选择下载版本</p>
<p><a href="http://spark.apache.org/downloads.html" target="_blank" rel="noopener">http://spark.apache.org/downloads.html</a></p>
<img src="/2020/02/10/Spark%E5%AE%89%E8%A3%85/sparkdownload.png" class title="spark下载版本">

<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ tar -zvxf spark-3.0.0-bin-hadoop2.7.tgz</span><br><span class="line"><span class="comment"># 配置环境变量</span></span><br><span class="line">$ sudo gedit ~/.bashrc</span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/jdk1.8.0_102</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/spark-3.0.0-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> PYSPARK_DRIVER_PYTHON=/home/dc/anaconda3/bin/python</span><br><span class="line"><span class="built_in">export</span> PYSPARK_PYTHON=/home/dc/anaconda3/bin/python</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$SPARK_HOME</span>/bin</span><br><span class="line"></span><br><span class="line">$ <span class="built_in">source</span> ~/.bashrc</span><br></pre></td></tr></table></figure>

<h2 id="安装pyspark"><a href="#安装pyspark" class="headerlink" title="安装pyspark"></a>安装pyspark</h2><h3 id="本地安装"><a href="#本地安装" class="headerlink" title="本地安装"></a>本地安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/ubuntu/spark-2.4.0-bin-hadoop2.7/python</span><br><span class="line">$ python3 setup.py install</span><br></pre></td></tr></table></figure>

<h3 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ pip3 install pyspark</span><br></pre></td></tr></table></figure>

<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkConf, SparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line"></span><br><span class="line">    conf = SparkConf()</span><br><span class="line">    sc = SparkContext(conf = conf)</span><br><span class="line">    </span><br><span class="line">    data = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>]</span><br><span class="line">    data = sc.parallelize(data)</span><br><span class="line">    rdd = data.map(<span class="keyword">lambda</span> x: x+<span class="number">1</span>)</span><br><span class="line">    print(rdd.collect())</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br></pre></td></tr></table></figure>

<p>输出：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[2, 3, 4, 5, 6, 7, 8]</span><br></pre></td></tr></table></figure>

<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="PYTHONPATH问题"><a href="#PYTHONPATH问题" class="headerlink" title="PYTHONPATH问题"></a>PYTHONPATH问题</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">py4j.protocol.Py4JError: org.apache.spark.api.python.PythonUtils.getEncryptionEnabled does not exist in the JVM</span><br></pre></td></tr></table></figure>

<p>解决方法</p>
<pre><code>sudo gedit .bashrc</code></pre><p>Please add $SPARK_HOME/python/build to PYTHONPATH:</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> SPARK_HOME=/opt/spark-3.0.0-bin-hadoop2.7</span><br><span class="line"><span class="built_in">export</span> PYTHONPATH=<span class="variable">$SPARK_HOME</span>/python:<span class="variable">$SPARK_HOME</span>/python/build:<span class="variable">$PYTHONPATH</span></span><br></pre></td></tr></table></figure>

<pre><code>source .bashrc</code></pre><p>或者在 import pyspark 之前：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> findspark</span><br><span class="line">findspark.init()</span><br></pre></td></tr></table></figure>

<h3 id="java-net-ConnectException"><a href="#java-net-ConnectException" class="headerlink" title="java.net.ConnectException"></a>java.net.ConnectException</h3><p>用spark提交任务时报错java.net.ConnectException，出现这个问题可以从以下几个方面排查：</p>
<ol>
<li><p>防火墙问题</p>
<p> 防火墙问题可以禁用防火墙或者开放相应端口</p>
</li>
<li><p>端口占用问题</p>
<p> 端口占用问题的话可以netstat -nltp查看，结束占用端口的应用</p>
</li>
<li><p>namenode未正常启动</p>
<p> namenode问题的话重新启动namenode</p>
</li>
</ol>
]]></content>
      <tags>
        <tag>教程</tag>
        <tag>Spark</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop安装</title>
    <url>/2020/02/09/Hadoop%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="前提"><a href="#前提" class="headerlink" title="前提"></a>前提</h2><p>安装过java，确认可用</p>
<pre><code>java -version</code></pre><h2 id="创建haddop用户组和用户"><a href="#创建haddop用户组和用户" class="headerlink" title="创建haddop用户组和用户"></a>创建haddop用户组和用户</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo addgroup hadoop</span><br><span class="line">sudo adduser --ingroup hadoop hduser</span><br><span class="line">sudo adduser hduser sudo </span><br><span class="line">su hduser</span><br></pre></td></tr></table></figure>

<h2 id="安装和配置ssh"><a href="#安装和配置ssh" class="headerlink" title="安装和配置ssh"></a>安装和配置ssh</h2><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo apt-get install openssh-server</span><br></pre></td></tr></table></figure>

<p>切换到hduser用户，执行如下命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ssh-keygen -t rsa -P <span class="string">''</span></span><br><span class="line">cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure>

<h2 id="下载hadoop"><a href="#下载hadoop" class="headerlink" title="下载hadoop"></a>下载hadoop</h2><p><a href="http://mirror.bit.edu.cn/apache/hadoop/common" target="_blank" rel="noopener">http://mirror.bit.edu.cn/apache/hadoop/common</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sudo tar -zxvf hadoop-2.7.7.tar.gz -C /opt</span><br><span class="line"><span class="built_in">cd</span> /opt</span><br><span class="line">sudo mv hadoop-2.7.7 hadoop</span><br><span class="line">sudo chown -R hduser:hadoop hadoop</span><br></pre></td></tr></table></figure>

<h2 id="配置hadoop环境"><a href="#配置hadoop环境" class="headerlink" title="配置hadoop环境"></a>配置hadoop环境</h2><p>编辑以下文件</p>
<ul>
<li><p>.bashrc</p>
<p>  sudo gedit ~/.bashrc</p>
</li>
</ul>
<p>将下面的内容复制到.bashrc中</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#Hadoop variables</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> HADOOP_INSTALL=/opt/hadoop</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_INSTALL</span>/bin</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$PATH</span>:<span class="variable">$HADOOP_INSTALL</span>/sbin</span><br><span class="line"><span class="built_in">export</span> HADOOP_MAPRED_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_HDFS_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br><span class="line"><span class="built_in">export</span> YARN_HOME=<span class="variable">$HADOOP_INSTALL</span></span><br></pre></td></tr></table></figure>

<ul>
<li>hadoop-env.sh</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /opt/hadoop/etc/hadoop &amp;&amp; sudo gedit hadoop-env.sh</span><br></pre></td></tr></table></figure>

<p>将下面的三行加入到hadoop-env.sh中，注释原来的 “export JAVA_HOME”那行</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-amd64</span><br><span class="line"><span class="built_in">export</span> HADOOP_COMMON_LIB_NATIVE_DIR=<span class="string">"/opt/hadoop/lib/native/"</span></span><br><span class="line"><span class="built_in">export</span> HADOOP_OPTS=<span class="string">"<span class="variable">$HADOOP_OPTS</span> -Djava.library.path=/opt/hadoop/lib/"</span></span><br></pre></td></tr></table></figure>

<h2 id="配置hadoop"><a href="#配置hadoop" class="headerlink" title="配置hadoop"></a>配置hadoop</h2><pre><code>cd /usr/local/hadoop/etc/hadoop</code></pre><p>编辑以下文件</p>
<ol>
<li><p>core-site.xml</p>
<p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://localhost:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>yarn-site.xml</p>
<p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>mapred-site.xml</p>
<pre><code>mv mapred-site.xml.template mapred-site.xml</code></pre><p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>hdfs-site.xml</p>
<pre><code>mkdir -p ~/mydata/hdfs/namenode &amp;&amp; mkdir -p ~/mydata/hdfs/datanode</code></pre><p> 将下面的内容复制到 <configuration> 标签内</configuration></p>
 <figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hduser/mydata/hdfs/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span> </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/home/hduser/mydata/hdfs/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span> </span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="启动并查看服务"><a href="#启动并查看服务" class="headerlink" title="启动并查看服务"></a>启动并查看服务</h2><ol>
<li><p>格式化 namenode</p>
<p> 第一次启动hadoop服务之前，必须执行格式化namenode</p>
<pre><code>hdfs namenode -format</code></pre></li>
<li><p>启动服务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">start-dfs.sh &amp;&amp; start-yarn.sh</span><br><span class="line"><span class="comment"># 或者</span></span><br><span class="line">start-all.sh</span><br></pre></td></tr></table></figure>
</li>
<li><p>jps查看服务</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jps</span><br><span class="line"><span class="comment"># 最好用sudo</span></span><br><span class="line">sudo jps</span><br></pre></td></tr></table></figure>

<p> 顺利的话可以看的：</p>
 <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">23313 DataNode</span><br><span class="line">10421 Master</span><br><span class="line">23749 ResourceManager</span><br><span class="line">23559 SecondaryNameNode</span><br><span class="line">23112 NameNode</span><br><span class="line">24104 NodeManager</span><br><span class="line">10605 Worker</span><br><span class="line">6942 Jps</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h2 id="遇到的问题"><a href="#遇到的问题" class="headerlink" title="遇到的问题"></a>遇到的问题</h2><h3 id="namenode不启动"><a href="#namenode不启动" class="headerlink" title="namenode不启动"></a>namenode不启动</h3><ol>
<li><p>先运行 </p>
<pre><code>stop-all.sh</code></pre></li>
<li><p>格式化namdenode</p>
<p> 不过在这之前要先删除原目录，即core-site.xml下配置的<name>hadoop.tmp.dir</name>所指向的目录，删除后切记要重新建立配置的空目录，然后运行</p>
<pre><code>hadoop namenode -format</code></pre><p> 请务必检查文件夹位置是否正确</p>
</li>
<li><p>运行 </p>
<pre><code>start-all.sh</code></pre></li>
</ol>
<h3 id="datanode不启动"><a href="#datanode不启动" class="headerlink" title="datanode不启动"></a>datanode不启动</h3><p>当我们使用hadoop namenode -format格式化namenode时，会在namenode数据文件夹（这个文件夹为自己配置文件中dfs.name.dir的路径）中保存一个current/VERSION文件，记录clusterID，datanode中保存的current/VERSION文件中的clustreID的值是上一次格式化保存的clusterID，这样，datanode和namenode之间的ID不一致。</p>
<ol>
<li><p>如果dfs文件夹中没有重要的数据，那么删除dfs文件夹，再重新运行下列指令： （删除节点下的dfs文件夹，为自己配置文件中dfs.name.dir的路径）</p>
</li>
<li><p>如果dfs文件中有重要的数据，那么在dfs/name目录下找到一个current/VERSION文件，记录clusterID并复制。然后dfs/data目录下找到一个current/VERSION文件，将其中clustreID的值替换成刚刚复制的clusterID的值即可；</p>
</li>
</ol>
<p><strong>注意</strong></p>
<p>每次运行结束Hadoop后，都应该关闭Hadoop</p>
<pre><code>stop-dfs.sh</code></pre><p>下次想重新运行Hadoop，不用再格式化namenode,直接启动Hadoop即可</p>
<pre><code>start-dfs.sh</code></pre>]]></content>
      <tags>
        <tag>Hadoop</tag>
        <tag>教程</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Java安装</title>
    <url>/2020/02/08/Java%E5%AE%89%E8%A3%85/</url>
    <content><![CDATA[<h2 id="安装jdk"><a href="#安装jdk" class="headerlink" title="安装jdk"></a>安装jdk</h2><h3 id="orcal-Java-Jdk"><a href="#orcal-Java-Jdk" class="headerlink" title="orcal Java Jdk"></a>orcal Java Jdk</h3><h4 id="官网下载"><a href="#官网下载" class="headerlink" title="官网下载"></a>官网下载</h4><ol>
<li><p>官网下载orcal Java Jdk</p>
<p> 下载链接  </p>
<p> <a href="http://www.oracle.com/technetwork/java/javase/downloads/index.html" target="_blank" rel="noopener">http://www.oracle.com/technetwork/java/javase/downloads/index.html</a></p>
<p> 可以根据自己的系统进行下载</p>
</li>
<li><p>进行解压</p>
<pre><code>sudo tar -zxvf jdk-linux-x64.tar.gz </code></pre><p> 解压到当前目录下，解压后可以把解压文件移动到自己想要放的目录下，使用mv命令 sudo mv jdk1.8.0_171 /usr/lib/jvm</p>
</li>
<li><p>进行配置</p>
<p> 使用全局设置方法，它是所有用户的共用的环境变量</p>
<p> 命令如下：</p>
<pre><code>$sudo gedit ~/.bashrc</code></pre><p> 然后把如下命令复制到最底部</p>
<pre><code>export JAVA_HOME=/usr/lib/jvm/jdk1.8.0_25  
export JRE_HOME=${JAVA_HOME}/jre  
export CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/lib  
export PATH=${JAVA_HOME}/bin:$PATH
export JAVA_HOME=后面要填写自己解压后的jdk的路径</code></pre></li>
<li><p>生效~/.bashrc文件</p>
<p> 命令如下：</p>
<pre><code>$sudo source ~/.bashrc</code></pre></li>
<li><p>测试是否安装成功</p>
<p> 查看版本号是否改变</p>
<pre><code>java -version </code></pre></li>
</ol>
<h4 id="在线安装"><a href="#在线安装" class="headerlink" title="在线安装"></a>在线安装</h4><ol>
<li><p>安装依赖包：</p>
<pre><code>sudo apt-get install python-software-properties</code></pre></li>
<li><p>添加仓库源：</p>
<pre><code>sudo add-apt-repository ppa:webupd8team/java</code></pre></li>
<li><p>更新软件包列表：</p>
<pre><code>sudo apt-get update</code></pre></li>
<li><p>安装java JDK：</p>
<pre><code>sudo apt-get install oracle-java8-installer</code></pre></li>
</ol>
<h3 id="openjdk"><a href="#openjdk" class="headerlink" title="openjdk"></a>openjdk</h3><ol>
<li><p>更新软件包列表：</p>
<pre><code>sudo apt-get update</code></pre></li>
<li><p>安装openjdk-8-jdk：</p>
<pre><code>sudo apt-get install openjdk-8-jdk</code></pre></li>
<li><p>查看java版本，看看是否安装成功：</p>
<pre><code>java -version</code></pre></li>
</ol>
]]></content>
      <tags>
        <tag>教程</tag>
        <tag>Java</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/02/01/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br><span class="line">$ hexo n <span class="string">"New Post"</span> <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br><span class="line">$ hexo s <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br><span class="line">$ hexo g <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br><span class="line">$ hexo d <span class="comment"># 等同</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
<h3 id="Multi-config"><a href="#Multi-config" class="headerlink" title="Multi config"></a>Multi config</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo avgr --config .\coding_config.yml</span><br></pre></td></tr></table></figure>

<p>根据不同的config文件生成不同的网页</p>
]]></content>
      <tags>
        <tag>hexo</tag>
      </tags>
  </entry>
</search>
